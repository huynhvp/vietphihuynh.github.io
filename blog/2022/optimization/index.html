<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>What I've learned from finding ways to accelerate the inference of a Transformer model. | Viet-Phi Huynh</title> <meta name="author" content="Viet-Phi Huynh"/> <meta name="description" content="Viet-Phi Huynh personal webpage. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://huynhvp.github.io/blog/2022/optimization/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Viet-Phi </span>Huynh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">What I've learned from finding ways to accelerate the inference of a Transformer model.</h1> <p class="post-meta">November 10, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/dev"> <i class="fas fa-hashtag fa-sm"></i> dev</a>     ·   <a href="/blog/category/optimization"> <i class="fas fa-tag fa-sm"></i> Optimization,</a>   <a href="/blog/category/onnx"> <i class="fas fa-tag fa-sm"></i> ONNX,</a>   <a href="/blog/category/onnx-runtime"> <i class="fas fa-tag fa-sm"></i> ONNX_Runtime,</a>   <a href="/blog/category/huggingface-optimum"> <i class="fas fa-tag fa-sm"></i> Huggingface_Optimum,</a>   <a href="/blog/category/transformer"> <i class="fas fa-tag fa-sm"></i> Transformer.</a>   </p> </header> <article class="post-content"> <p><b>Table of Contents</b></p> <ul id="markdown-toc"> <li><a href="#introduction" id="markdown-toc-introduction"><b>Introduction</b></a></li> <li> <a href="#a-solution" id="markdown-toc-a-solution"><b>A solution</b></a> <ul> <li><a href="#-onnx-open-neural-network-exchange-converter" id="markdown-toc--onnx-open-neural-network-exchange-converter"><img src="/assets/img/optimization/question_1.png" alt="" style="width: 3.7%"> <b>ONNX (Open Neural Network eXchange) Converter</b></a></li> <li><a href="#-adapter-onnx-runtime" id="markdown-toc--adapter-onnx-runtime"><img src="/assets/img/optimization/question_2.png" alt="" style="width: 3.5%"> <b>Adapter: ONNX Runtime</b></a></li> <li><a href="#inference-strategies-based-on-onnx-model-format" id="markdown-toc-inference-strategies-based-on-onnx-model-format"><b>Inference strategies based on ONNX model format</b></a></li> <li><a href="#huggingfaces-optimum-" id="markdown-toc-huggingfaces-optimum-"><b>Huggingface’s Optimum</b> <img src="/assets/img/optimization/optimum.png" alt="" style="width: 3.5%"></a></li> </ul> </li> <li><a href="#conclusion" id="markdown-toc-conclusion"><b>Conclusion</b></a></li> </ul> <h3 id="introduction"><b>Introduction</b></h3> <p>When it comes to deploying a machine learning/deep learning model in production environments, there are many factors that need to be worked out. In this post, I would like to outline 2 aspects:</p> <ul> <li> <p><b>(1) The compatibility between the development environment you use to train your model and the production environment.</b></p> <p>As nowadays’s AI ecosystem is considerably fragmented from software-level to hardware-level. A lot of ML/DL training frameworks are at hand for us to build our model such as PyTorch, Tensorflow or scikit-learn. We can get our jobs done on Windows or Linux with Intel GPUs and Python runtime, but later on, we want to deploy the product on cloud or edge devices with another runtime (e.g. C++) and another sort of GPUs (e.g. NVIDIA). Facing this cross-platform deployment challenge, a classic strategy is to build a specific model for a specific platform, as illustrated in the figure below. This means that developer is required to pick developing tools corresponding to targeted environments which may not be the ones they love to use. Also, they need to ensure that there is no accuracy gap between different model versions.</p> <p><img src="/assets/img/optimization/deployment_1.png" alt="" style="width: 100%; display:block; margin-left:auto; margin-right:auto"></p> </li> <li> <p><b>(2) Optimizing model inference for an efficient user experience.</b></p> <p>In production, together with accuracy, scalability and high performance become crucial concerns. To deal with that, there could be two strategies:</p> <ul> <li> <b>Using smaller models or distilled models that still yield the accuracy you need:</b> DL models, especially, Transformer-based LM models, become more and more powerful, at the cost of number of model parameters and environmental responsibility (carbon footprint). However, larger model does not necessarily mean better. The data size, the training/fine-tuning recipe also account for the performance of your model (<a href="https://www.deeplearning.ai/the-batch/finding-the-best-data-to-parameter-ratio-for-nlp-models/" target="_blank" rel="noopener noreferrer">https://www.deeplearning.ai/the-batch/finding-the-best-data-to-parameter-ratio-for-nlp-models/</a>).</li> </ul> <p><img src="/assets/img/optimization/model_size.jpg" alt="" style="width: 100%; display:block; margin-left:auto; margin-right:auto"> <em>(Source: <a href="https://huggingface.co/blog/large-language-models" target="_blank" rel="noopener noreferrer">https://huggingface.co/blog/large-language-models</a>)</em></p> <ul> <li> <b>Optimizing your model:</b> basically, the optimization involves the improvement of running time (latency) and memory throughput and it is done not only at software level (algorithm, training framework, OS, programming language) but also at hardware level (GPU, hardware accelerator). As the AI ecosystem is fragmented, we can have many possible combinations of {OS, framework, runtime, hardware} with different pros/cons for developing a model. This makes the optimization a challenging task.</li> </ul> </li> </ul> <h3 id="a-solution"><b>A solution</b></h3> <p>Up to this point, you might envision how tough the path from conception to production in ML is. To address two deployment issues mentioned above, a research direction has been identified for AI ecosystem in which the development-production workflow becomes modularizable and the frameworks become interoperable. The idea is to standardize the bridge between development environment and production environment, alternatively stated, the bridge between software (OS, framework) and hardware (CPU, GPU). By this way, different frameworks can be combined with different hardwares without modification. Let’s dive a bit deeper into this standardization process, as demonstrated in the figure below:</p> <p><img src="/assets/img/optimization/deployment_2.png" alt="" style="width: 100%; display:block; margin-left:auto; margin-right:auto"></p> <p>It requires two plugins: (i) a <b>converter</b> to transform the model you are developing in your environment into an universal one that can be loaded, optimized and executed by (ii) an <b>adapter</b> installed on different target platforms.</p> <h5 id="-onnx-open-neural-network-exchange-converter"> <img src="/assets/img/optimization/question_1.png" alt="" style="width: 3.7%"> <b>ONNX (Open Neural Network eXchange) Converter</b> </h5> <p>According to <a href="https://onnx.ai/" target="_blank" rel="noopener noreferrer">https://onnx.ai/</a>, <a href="https://onnx.ai/" target="_blank" rel="noopener noreferrer">ONNX</a> (Open Neural Network eXchange) is an open serialized format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers. It is backed by many leading AI companies:</p> <p><img src="/assets/img/optimization/onnx.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"><em>(Source: <a href="https://onnx.ai/" target="_blank" rel="noopener noreferrer">https://onnx.ai/</a>)</em></p> <h5 id="-adapter-onnx-runtime"> <img src="/assets/img/optimization/question_2.png" alt="" style="width: 3.5%"> <b>Adapter: ONNX Runtime</b> </h5> <p><a href="https://onnxruntime.ai/" target="_blank" rel="noopener noreferrer">ONNX Runtime</a> is a performant inference engine that can read, optimize the ONNX model format and leverage hardware accelerators to perform inference from ONNX format. It is compatible with various technology stack (frameworks, operating systems and hardware platforms).</p> <p><img src="/assets/img/optimization/onnx_runtime.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"><em>(Source: <a href="https://onnxruntime.ai/" target="_blank" rel="noopener noreferrer">https://onnxruntime.ai</a>)</em></p> <p>For example, assuming your production environment supports <code class="language-plaintext highlighter-rouge">Linux OS x64</code>, <code class="language-plaintext highlighter-rouge">Python runtime</code> and <code class="language-plaintext highlighter-rouge">NVIDIA GPU</code> as in figure below, then you can install ONNX Runtime via <code class="language-plaintext highlighter-rouge">pip install onnxruntime-gpu</code> to be ready for deploying any model created from any technology stack on your dev machine as long as it can be exported to ONNX format.</p> <p><img src="/assets/img/optimization/onnx_runtime_choice.png" alt="" style="width: 80%; display:block; margin-left:auto; margin-right:auto"></p> <p>With <b>ONNX</b> and <b>ONNX Runtime</b>, we are able to address two of the issues of putting a ML model in production (compatibility/interoperability and inference performance) introduced earlier in the post. For ML developers, they are liberated from the constraint of framework compatibility and have more freedom to choose their preferred tech stacks. For hardware manufacturers, they can rely on a standard and universal software specification (e.g. ONNX model format) to ease their process of hardware optimization for AI.</p> <p>Now, let’s go a bit more details into possible inference strategies with ONNX format.</p> <h5 id="inference-strategies-based-on-onnx-model-format"><b>Inference strategies based on ONNX model format</b></h5> <p><img src="/assets/img/optimization/onnx_strategies.png" alt="" style="width: 100%; display:block; margin-left:auto; margin-right:auto"></p> <ul> <li> <p><strong><span style="color:red">(1)</span></strong> After your model is converted into ONNX format, it can be used immediately on targeted device using ONNX Runtime.</p> </li> <li> <p><strong><span style="color:#1589FF">(2)</span></strong> After your model is converted into ONNX format, ONNX Runtime can further optimize it before performing inference on targeted device. The optimization techniques can be: graph optimization (node pruning, node fusion…) and quantization (e.g. convert FP32 to INT8).</p> </li> <li> <p><strong><span style="color:green">(3)</span></strong> It is also possible to convert ONNX model to another format that is optimized for a specific hardware. For example, in the figure above, as <strong><span style="color:orange">Cloud</span></strong> is equipped with NVIDIA GPU, we can convert ONNX model to <a href="https://github.com/NVIDIA/TensorRT" target="_blank" rel="noopener noreferrer">TensorRT</a> model which is developed dedicatedly for NVIDIA GPUs. ONNX Runtime is no longer in use in this case of course.</p> </li> </ul> <h5 id="huggingfaces-optimum-"> <b>Huggingface’s Optimum</b> <img src="/assets/img/optimization/optimum.png" alt="" style="width: 3.5%"> </h5> <p>If you are working with Transformers and you want to easily accelerate your model performance leveraging your existing hardware powers, you may think of <a href="https://huggingface.co/docs/optimum/index" target="_blank" rel="noopener noreferrer">Optimum</a>. It gathers a set of optimization toolkits (e.g. ONNX Runtime presented above, Intel Neural Compressor, OpenVINO) each of which is designed specifically for each specific hardware. It also provides a high-level API to facilitate the utilization and the optimization in a few lines of code. Typically, an optimization pipeline with Optimum consists of 3 steps:</p> <ul> <li> <p>Convert the trained model into the standard format such as ONNX or OpenVINO.</p> </li> <li> <p>Optimize the converted model (e.g. graph optimization, quantization) using available Runtimes such as ONNX Runtime or OpenVINO Runtime.</p> </li> <li> <p>Run the inference on optimized model.</p> </li> </ul> <p>Now, let’s get our hand a bit dirty by trying to accelerate a Transformer model with ONNX and ONNX Runtime using Optimum. In detail, we optimize the inference of a BERT-based model for text classification task (<a href="https://huggingface.co/tasks/text-classification#sentiment-analysis" target="_blank" rel="noopener noreferrer">Sentiment Analysis</a>).</p> <p><b>Example:</b></p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Input: I am learning how to accelerate the inference of a language model using Huggingface's Optimum. I am thinking if it can bring some performance gain. I hope it does."

Output: [{'label': 'POSITIVE', 'score': 0.7894014716148376}]
</span></code></pre></div></div> <p><b>My machine is equiped with 6GB GPU NVIDIA Quadro RTX 3000</b>. Hence, I’m going to leverage 2 hardware accelerators for NVIDIA via <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu" target="_blank" rel="noopener noreferrer">ONNX Runtime</a> : generic accelerator (<code class="language-plaintext highlighter-rouge">CUDAExecutionProvider</code>) and <a href="https://developer.nvidia.com/tensorrt" target="_blank" rel="noopener noreferrer">TensorRT inference engine</a> (<code class="language-plaintext highlighter-rouge">TensorrtExecutionProvider</code>)</p> <p><b>For comparison</b>, there are two baselines where the inference is done with native pytorch on either CPU (<code class="language-plaintext highlighter-rouge">cpu_torch_model</code>) or GPU (<code class="language-plaintext highlighter-rouge">gpu_torch_model</code>).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">from</span> <span class="n">optimum.onnxruntime</span> <span class="kn">import</span> <span class="n">ORTModelForSequenceClassification</span><span class="p">,</span> <span class="n">ORTOptimizer</span>
<span class="kn">from</span> <span class="n">optimum.onnxruntime.configuration</span> <span class="kn">import</span> <span class="n">OptimizationConfig</span>
<span class="kn">from</span> <span class="n">time</span> <span class="kn">import</span> <span class="n">perf_counter</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="sh">"</span><span class="s">distilbert-base-uncased-finetuned-sst-2-english</span><span class="sh">"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="c1"># baseline: native pytorch CPU
</span><span class="n">cpu_torch_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">cpu_torch_pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">sentiment-analysis</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">cpu_torch_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># baseline: native pytorch GPU
</span><span class="n">gpu_torch_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda:0</span><span class="sh">'</span><span class="p">))</span>
<span class="n">gpu_torch_pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">sentiment-analysis</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">gpu_torch_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <p>To optimize the native pytorch model using ONNX Runtime, we first convert it into ONNX format <code class="language-plaintext highlighter-rouge">onnx_model</code> and apply graph optimization on converted model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># convert native pytorch model to ONNX format and graph-optimize it.
</span><span class="n">onnx_model</span> <span class="o">=</span> <span class="n">ORTModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">from_transformers</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">ORTOptimizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)</span>
<span class="n">optimization_config</span> <span class="o">=</span> <span class="nc">OptimizationConfig</span><span class="p">(</span>
    <span class="n">optimization_level</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">optimize_with_onnxruntime_only</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">optimize_for_gpu</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">optimizer</span><span class="p">.</span><span class="nf">optimize</span><span class="p">(</span><span class="n">save_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">./optimized</span><span class="sh">"</span><span class="p">,</span> <span class="n">optimization_config</span><span class="o">=</span><span class="n">optimization_config</span><span class="p">)</span>
</code></pre></div></div> <p>Next, we define 3 types of inferences for optimized ONNX model:</p> <ul> <li>Inference on CPU:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># optimized ONNX CPU
</span><span class="n">optimized_cpu_onnx_model</span> <span class="o">=</span> <span class="n">ORTModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">./optimized/</span><span class="sh">"</span><span class="p">)</span>
<span class="n">optimized_cpu_onnx_pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">sentiment-analysis</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">optimized_cpu_onnx_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Inference on GPU with NVIDIA generic accelerators (CUDA):</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># optimized ONNX CUDA
</span><span class="n">optimized_cuda_onnx_model</span> <span class="o">=</span> <span class="n">ORTModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">./optimized/</span><span class="sh">"</span><span class="p">,</span> <span class="n">provider</span><span class="o">=</span><span class="sh">"</span><span class="s">CUDAExecutionProvider</span><span class="sh">"</span><span class="p">)</span>
<span class="n">optimized_cuda_onnx_pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">sentiment-analysis</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">optimized_cuda_onnx_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Inference on GPU with NVIDIA TensorRT engine:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># optimized ONNX Tensorrt
## ONNX Runtime graph optimization need to be disabled for the model to be consumed and optimized by TensorRT
</span><span class="n">optimized_tensorrt_onnx_model</span> <span class="o">=</span> <span class="n">ORTModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">from_transformers</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">session_options</span><span class="o">=</span><span class="n">session_options</span><span class="p">,</span> <span class="n">provider</span><span class="o">=</span><span class="sh">"</span><span class="s">TensorrtExecutionProvider</span><span class="sh">"</span><span class="p">)</span>
<span class="n">optimized_tensorrt_onnx_pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">sentiment-analysis</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">optimized_tensorrt_onnx_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div></div> <p>We measure the latency of inference settings described above with a simple benchmark as following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">intput</span> <span class="o">=</span> <span class="sh">"</span><span class="s">I am learning how to accelerate the inference of a language model using Huggingface</span><span class="sh">'</span><span class="s">s Optimum.  </span><span class="se">\
</span><span class="s">             I am wondering if it can bring some performance gain. I hope it does.</span><span class="sh">"</span>

<span class="c1"># ref: https://www.philschmid.de/optimizing-transformers-with-optimum
</span><span class="k">def</span> <span class="nf">measure_latency</span><span class="p">(</span><span class="n">pipe</span><span class="p">):</span>
    <span class="n">latencies</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># warm up
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
        <span class="n">_</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">intput</span><span class="p">)</span>
    <span class="c1"># Timed run
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="nf">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span>  <span class="nf">pipe</span><span class="p">(</span><span class="n">intput</span><span class="p">)</span>
        <span class="n">latency</span> <span class="o">=</span> <span class="nf">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="n">latencies</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">latency</span><span class="p">)</span>
    <span class="c1"># Compute averaged execution time
</span>    <span class="n">avg_time_ms</span> <span class="o">=</span> <span class="mi">1000</span> <span class="o">*</span> <span class="nf">sum</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span>
    <span class="k">return</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Average latency (ms) : </span><span class="si">{</span><span class="n">avg_time_ms</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span>
</code></pre></div></div> <p><b>The results</b> are shown below:</p> <p><img src="/assets/img/optimization/latency.png" alt="" style="width: 70%; display:block; margin-left:auto; margin-right:auto"></p> <p>Clearly, with ONNX format and ONNX Runtime, the performance has been significantly boosted even on CPU mode:</p> <ul> <li> <p>ONNX Runtime runs inference on CPU accelerates 1.56x w.r.t native pyTorch running on CPU.</p> </li> <li> <p>ONNX Runtime leverages better the GPU hardware than native pyTorch by 2.4x faster on NVIDIA GPU.</p> </li> </ul> <h3 id="conclusion"><b>Conclusion</b></h3> <p>This post walked you through the introduction of two challenges for deploying an AI model (Transformer model in particular) in production: <b>(i)</b> the interoperability between different frameworks and <b>(ii)</b> the performance issue. These two challenges can be addressed by standardizing model formats and employing a powerful cross-platform inference engine for running the inference on this format, such as {ONNX format, ONNX Runtime} or {OpenVINO, OpenVINO Runtime}.</p> <p>If we relax the interoperability aspect and focus on boosting the model performance, then, apart from the method based on converting the original model into standard format as described in this post, <a href="https://github.com/ELS-RD/kernl" target="_blank" rel="noopener noreferrer">kernl</a>, recently released, intervenes directly in GPU kernels that allows to accelerate and optimize your model right on Pytorch with a single line of code, without converting to any standard format. Its benchmark (below) shows impressive improvements over other optimization techniques. Furthermore, they pay more attention to generative models (e.g. Seq2Seq model) which prove to be more difficult to optimize. However, if you don’t have the new generation of NVIDIA GPU (Ampere), you are not able to use this tool yet. (<a href="https://github.com/ELS-RD/kernl/issues/133" target="_blank" rel="noopener noreferrer">follow up here</a>)</p> <p><img src="/assets/img/optimization/kernl.png" alt="" style="width: 90%; display:block; margin-left:auto; margin-right:auto"> <em>(Source: <a href="https://github.com/ELS-RD/kernl" target="_blank" rel="noopener noreferrer">https://github.com/ELS-RD/kernl</a>)</em></p> <p><b>References</b>:</p> <ul> <li>https://www.philschmid.de/optimizing-transformers-with-optimum</li> <li>https://odsc.medium.com/interoperable-ai-high-performance-inferencing-of-ml-and-dnn-models-using-open-source-tools-6218f5709071</li> <li>https://medium.com/geekculture/onnx-in-a-nutshell-4b584cbae7f5</li> <li>https://huggingface.co/blog/convert-transformers-to-onnx</li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Viet-Phi Huynh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>