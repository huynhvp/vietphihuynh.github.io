<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>PoTM - Emergent World Representations - Exploring a Sequence Model Trained on a Synthetic Task | Viet-Phi Huynh</title> <meta name="author" content="Viet-Phi Huynh"/> <meta name="description" content="Viet-Phi Huynh personal webpage. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://huynhvp.github.io/blog/2023/representation-probe/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Viet-Phi </span>Huynh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">PoTM - Emergent World Representations - Exploring a Sequence Model Trained on a Synthetic Task</h1> <p class="post-meta">May 1, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/research"> <i class="fas fa-hashtag fa-sm"></i> research</a>     ·   <a href="/blog/category/paper-of-the-month"> <i class="fas fa-tag fa-sm"></i> paper_of_the_month,</a>   <a href="/blog/category/language-model"> <i class="fas fa-tag fa-sm"></i> language_model,</a>   <a href="/blog/category/explainable-ai"> <i class="fas fa-tag fa-sm"></i> explainable_ai</a>   </p> </header> <article class="post-content"> <hr> <p>I recently came across an interesting paper that got accepted at ICLR 2023: <a href="https://arxiv.org/abs/2210.13382" target="_blank" rel="noopener noreferrer">Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task (Li et al.)</a>. It provides valuable insights toward the understanding of black-box language model. Specifically, by training a LM to play a chess-like game, Othello, without feeding it any knowledge of the game rules, they discover that LM does learn meaningful latent representations that help it uncover the game and make legal disc moves on the board.</p> <hr> <p><b>Table of Contents</b></p> <ul id="markdown-toc"> <li><a href="#the-othello-game-" id="markdown-toc-the-othello-game-"><b>The Othello game </b></a></li> <li> <a href="#teach-lm-to-play-othello" id="markdown-toc-teach-lm-to-play-othello"><b>Teach LM to play Othello</b></a> <ul> <li><a href="#observation-1-lm-respects-the-games-rule" id="markdown-toc-observation-1-lm-respects-the-games-rule">Observation 1: LM respects the game’s rule.</a></li> <li><a href="#observation-2-hidden-representations-encoded-in-lms-layers-represent-the-boards-states" id="markdown-toc-observation-2-hidden-representations-encoded-in-lms-layers-represent-the-boards-states">Observation 2: Hidden representations encoded in LM’s layers represent the board’s states</a></li> <li><a href="#observation-3-the-relationship-between-models-internal-representation-or-board-state-thanks-to-observation-2-and-models-prediction-ie-next-legal-move-is-causal" id="markdown-toc-observation-3-the-relationship-between-models-internal-representation-or-board-state-thanks-to-observation-2-and-models-prediction-ie-next-legal-move-is-causal">Observation 3: The relationship between model’s internal representation (or board state, thanks to observation 2) and model’s prediction (i.e next legal move) is causal.</a></li> </ul> </li> </ul> <h3 id="the-othello-game-"><b>The Othello game </b></h3> <p><img src="/assets/img/probe/othello.gif" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"> <em>(Source: https://github.com/SiyanH/othello-game)</em></p> <p>Two players, one holding black discs, one holding white discs, take turns making a move (aka. placing their colored disc) on the 8x8 board. The goal is to cover the board with the majority of their color. When a player makes a move, any opponent’s disc found in between (horizontally, vertically or diagonally) the disc his just placed and any of existing discs of his color will be flipped over to become their own color. A legal move is a move that ensures at least one such flip happens. Otherwise, the game ends.</p> <p>A casual LM is trained to play Othello and its internal representation will be analyzed. Author chooses this game as it is simple enough while still has a sufficiently large solution space (aka. where to move given the board’s current state) to avoid memorization.</p> <h3 id="teach-lm-to-play-othello"><b>Teach LM to play Othello</b></h3> <p>The LM is fed a naive transcript recording interleaving moves of two players without adding the game rules or additional analysis of game state (e.g. [F5, C5, …] where F, C are vertical indices, and 5 is horizontal index of the board)). It has to figure out who play next and identify which tiles on the board are legal to move to.</p> <h4 id="observation-1-lm-respects-the-games-rule">Observation 1: LM respects the game’s rule.</h4> <p>The trained LM respects the game rule, it can predict the next legal move with very low error rate according to the current state of the board. (Note: a legal move is not necessary an optimized move as the model is not trained to win the game). This is not due to the memorization as the test set is ensured not to be seen during the training.</p> <h4 id="observation-2-hidden-representations-encoded-in-lms-layers-represent-the-boards-states">Observation 2: Hidden representations encoded in LM’s layers represent the board’s states</h4> <p>Board state involves whether each tile holds a black disc or a while disc or is empty. Author trains a non-linear MLP classifier \(p_{\theta}(x)\), taking in the internal activations \(x\) of a specific layer of LM, at a given game step, as features and yielding one of three labels: {black, white, empty}. In this way, they seek to investigate whether there is a mapping between LM’s internal representations and board’s states. Indeed, the results show that the classifier achieves high accuracy, implying such mapping exists.</p> <h4 id="observation-3-the-relationship-between-models-internal-representation-or-board-state-thanks-to-observation-2-and-models-prediction-ie-next-legal-move-is-causal">Observation 3: The relationship between model’s internal representation (or board state, thanks to observation 2) and model’s prediction (i.e next legal move) is causal.</h4> <p>In other words, changes in the network’s activations, leading to changes in board’s states according to observation 2 (e.g. a tile is switched from black to white), will causally cause the model to predict a move from a new set of possible legal moves in compliance with new board state.</p> <p>For example, in the figure below, from lower left board to lower right board, network’s activations has been intervened to switch the tile E6 from black to white. Consequently, the set of next possible legal moves has to be changed from {B4, C6, D3, E7, F4, F6} (upper left) to {B3,B4,C6,D3,F4} (upper right). The model predicts correctly this new set.</p> <p><img src="/assets/img/probe/board_state.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"> <em>(Source: copied from the paper)</em></p> <p>Question arises: how to modify the network’s activations \(x\) such that the tile E6 switches from black to white while others keep intact ? (we denote current board state as \(B\), new board state as \(B'\) and \(B'\) differs from \(B\) only at tile E6)</p> <ul> <li> <p>Pre-define a layer index \(l_s\), all activations from the layer \(l_s\) until the final layer, at the last game step, will be modified, as in figure below:</p> <p><img src="/assets/img/probe/interven.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"> <em>(Source: copied from the paper)</em></p> <p>Author argues that the intervention at only one layer \(l_s\) is not effective, as the change made at layer \(l_s\) will be diluted when it reaches the last layer, making the output being not affected by the change.</p> </li> <li> <p>The network’s activations \(x\) is updated in gradient descent manner such that new \(x'\) is mapped to \(B'\) via \(p_{\theta}(x')\) (see Observation 2). This resorts to:</p> </li> </ul> \[x' = x - \alpha \frac{\partial \mathcal{L}_{CE} (p_{\theta}(x), B')}{\partial x}\] </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Viet-Phi Huynh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>