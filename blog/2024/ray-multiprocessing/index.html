<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>I find Ray a powerful framework for parallel computing | Viet-Phi Huynh</title> <meta name="author" content="Viet-Phi Huynh"/> <meta name="description" content="Viet-Phi Huynh personal webpage. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://huynhvp.github.io/blog/2024/ray-multiprocessing/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Viet-Phi </span>Huynh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">I find Ray a powerful framework for parallel computing</h1> <p class="post-meta">January 1, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/dev"> <i class="fas fa-hashtag fa-sm"></i> dev</a>     ·   <a href="/blog/category/multiprocessing"> <i class="fas fa-tag fa-sm"></i> multiprocessing,</a>   <a href="/blog/category/ray"> <i class="fas fa-tag fa-sm"></i> ray</a>   </p> </header> <article class="post-content"> <p>One strategy to speed up or scale a machine learning workflow is parallel/distributed processing. In python, the <a href="https://docs.python.org/3/library/multiprocessing.html" target="_blank" rel="noopener noreferrer">multiprocessing</a> module can serve as a solution for this purpose. However, it falls short (and can even harm overal performance) for parallel functions that requires heavy workloads or costly initialization due to data copying, moving and overhead input serialization/deserialization.</p> <p>Meanwhile, <a href="https://docs.ray.io/en/latest/index.html" target="_blank" rel="noopener noreferrer">Ray</a> is perfectly suited to such scenarios. Let’s work on two toy examples to illustrate that.</p> <blockquote> <p>Two core concepts (among others) of Ray that make it powerful in distrubed programming are:</p> <ul> <li> <a href="https://docs.ray.io/en/latest/ray-core/key-concepts.html#tasks" target="_blank" rel="noopener noreferrer">Task</a>: like an asynchronous function that can be executed in a seperate process or a remote machine.</li> <li> <a href="https://docs.ray.io/en/latest/ray-core/key-concepts.html#actors" target="_blank" rel="noopener noreferrer">Actor</a>: like an asynchronous stateful class that can run in a seperate process or remotely together with its own methods. Particularly, other actors and tasks from different processes can acess and mutate actor’s states.</li> </ul> </blockquote> <hr> <p><b>Table of Contents</b></p> <ul id="markdown-toc"> <li><a href="#1-parallelize-a-bundle-of-matrix-multiplication-functions" id="markdown-toc-1-parallelize-a-bundle-of-matrix-multiplication-functions">1. Parallelize a bundle of matrix multiplication functions</a></li> <li><a href="#2-parallelize-a-bunch-of-named-entity-recognition-ner-models" id="markdown-toc-2-parallelize-a-bunch-of-named-entity-recognition-ner-models">2. Parallelize a bunch of Named Entity Recognition (NER) models</a></li> </ul> <h4 id="1-parallelize-a-bundle-of-matrix-multiplication-functions">1. Parallelize a bundle of matrix multiplication functions</h4> <p>Let’s parallelize a set of functions described by \(f_i = x*y_i\) where \(x\) is a fixed 10240x10240 float matrix (~800MB), representing heavy input, and \(y_i\) denotes a variable 10240x1024 float matrix for each function \(f_i\).</p> <p><b>With Multiprocessing:</b></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">tracemalloc</span>
<span class="kn">from</span> <span class="n">multiprocessing</span> <span class="kn">import</span> <span class="n">Pool</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">psutil</span>

<span class="n">num_cpus</span> <span class="o">=</span> <span class="n">psutil</span><span class="p">.</span><span class="nf">cpu_count</span><span class="p">(</span><span class="n">logical</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># 8
</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_cpus</span> <span class="o">//</span> <span class="mi">2</span>
<span class="n">pool</span> <span class="o">=</span> <span class="nc">Pool</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)</span> <span class="c1"># multiprocessing pool
</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">10240</span><span class="p">,</span> <span class="mi">10240</span><span class="p">)</span>  <span class="c1"># x takes 800MB
</span>
<span class="k">def</span> <span class="nf">task</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>  <span class="c1"># noqa: D103
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span>

<span class="k">def</span> <span class="nf">run_multiple_tasks_in_parallel</span><span class="p">(</span><span class="n">i_trial</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">i_trial</span><span class="p">)</span>
    <span class="n">y_s</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10240</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)]</span>  <span class="c1"># each y takes 10MB
</span>    <span class="n">results</span> <span class="o">=</span> <span class="n">pool</span><span class="p">.</span><span class="nf">starmap</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="nf">zip</span><span class="p">([</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">y_s</span><span class="p">),</span> <span class="n">y_s</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># benchmark
</span>    <span class="n">num_trials</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>

    <span class="c1"># tracemalloc.start()
</span>    <span class="k">for</span> <span class="n">i_trial</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_trials</span><span class="p">):</span>
        <span class="nf">run_multiple_tasks_in_parallel</span><span class="p">(</span><span class="n">i_trial</span><span class="p">)</span>
    <span class="c1"># current_mem, peak_mem = tracemalloc.get_traced_memory()
</span>    <span class="c1"># tracemalloc.stop()
</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">Avg Time: </span><span class="si">{</span><span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span><span class="o">/</span><span class="n">num_trials</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> (s)</span><span class="sh">"</span>
    <span class="p">)</span>  <span class="c1"># while benchmarking time, disable mem_usage to avoid additional calculation.
</span>    <span class="c1"># print(f"Peak memory: {peak_mem/(1024*1024):.2f} (MB)")
</span></code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Avg Time: 7.31 <span class="o">(</span>s<span class="o">)</span>
Peak memory: 1770.71 <span class="o">(</span>MB<span class="o">)</span>
</code></pre></div></div> <p><b>With Ray:</b></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">ray</span>

<span class="n">num_cpus</span> <span class="o">=</span> <span class="n">psutil</span><span class="p">.</span><span class="nf">cpu_count</span><span class="p">(</span><span class="n">logical</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># 8
</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_cpus</span> <span class="o">//</span> <span class="mi">2</span>  <span class="c1"># max parallel tasks
</span><span class="n">ray</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">num_cpus</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>  <span class="c1"># init ray
</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">10240</span><span class="p">,</span> <span class="mi">10240</span><span class="p">)</span>  <span class="c1"># x takes 800MB
</span><span class="n">x_ref</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="nf">put</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># put x in ray's object store and return its reference.
</span>
<span class="nd">@ray.remote</span>  <span class="c1"># convert func to ray's remote task
</span><span class="k">def</span> <span class="nf">task</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>  <span class="c1"># noqa: D103
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span>

<span class="k">def</span> <span class="nf">run_multiple_tasks_in_parallel</span><span class="p">(</span><span class="n">i_run</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">i_run</span><span class="p">)</span>
    <span class="n">y_s</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10240</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)]</span>  <span class="c1"># each y takes 10MB
</span>    <span class="n">ray_task_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">task</span><span class="p">.</span><span class="nf">remote</span><span class="p">(</span><span class="n">x_ref</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">y_s</span><span class="p">]</span>  <span class="c1"># pass reference of x instead of x itself
</span>    <span class="n">results</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">ray_task_list</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>

</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Avg Time: 0.95 <span class="o">(</span>s<span class="o">)</span>
Peak memory: 44.38 <span class="o">(</span>MB<span class="o">)</span>
</code></pre></div></div> <p><strong><span style="color:green"><b>Clearly,</b></span></strong> Ray is much faster and much more memory-efficient than Multiprocessing. This is due to the fact that in Multiprocessing, each process worker has to copy and pass expensive input data (i.e. \(x\)) from main process, which ends up with overhead serialization/deserialization and high memory usage. On the contrary, in Ray, the main process puts the fixed matrix \(x\) in a shared object store and pass the reference of \(x\) to each worker. This reduces memory usage as each worker now uses the same object \(x\), without duplication. Morever, for object of primitive datatypes, such as numpy array, Ray avoids serializing them, allowing process workers to read them directly without deserialization, leading to significant performance gains.</p> <h4 id="2-parallelize-a-bunch-of-named-entity-recognition-ner-models">2. Parallelize a bunch of Named Entity Recognition (NER) models</h4> <p>Considering the scenarios where a server is asked to tag named entities in a batch of text. The server calls on workers, each of which load a NER model and process a text in batch. When a batch is completed, another batch arrives and the workers continue their works.</p> <p><b>With Multiprocessing:</b></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">tracemalloc</span>
<span class="kn">from</span> <span class="n">multiprocessing</span> <span class="kn">import</span> <span class="n">Pool</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">psutil</span>
<span class="kn">import</span> <span class="n">spacy</span>

<span class="n">num_cpus</span> <span class="o">=</span> <span class="n">psutil</span><span class="p">.</span><span class="nf">cpu_count</span><span class="p">(</span><span class="n">logical</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># 8
</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_cpus</span> <span class="o">//</span> <span class="mi">2</span>
<span class="n">pool</span> <span class="o">=</span> <span class="nc">Pool</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)</span> <span class="c1"># multiprocessing pool
</span>
<span class="k">def</span> <span class="nf">task</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">ner</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">en_core_web_sm</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># load NER model
</span>    <span class="n">entities</span> <span class="o">=</span> <span class="nf">ner</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">entities</span>

<span class="k">def</span> <span class="nf">run_multiple_tasks_in_parallel</span><span class="p">(</span><span class="n">i_batch</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">i_batch</span><span class="p">)</span>
    <span class="n">text_s</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">Paris is the capital of France.</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_workers</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">pool</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">text_s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># benchmark
</span>    <span class="n">num_batches</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>

    <span class="c1"># tracemalloc.start()
</span>    <span class="k">for</span> <span class="n">i_batch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span> <span class="c1"># one batch is completed, another arrives.
</span>        <span class="nf">run_multiple_tasks_in_parallel</span><span class="p">(</span><span class="n">i_batch</span><span class="p">)</span>
    <span class="c1"># current_mem, peak_mem = tracemalloc.get_traced_memory()
</span>    <span class="c1"># tracemalloc.stop()
</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">Total Time for processing </span><span class="si">{</span><span class="n">num_batches</span><span class="si">}</span><span class="s"> batches of texts: </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> (s)</span><span class="sh">"</span>
    <span class="p">)</span>  <span class="c1"># while benchmarking time, disable mem_usage to avoid additional calculation.
</span>    <span class="c1"># print(f"Peak memory: {peak_mem/(1024*1024):.2f} (MB)")
</span></code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Total Time <span class="k">for </span>processing 10 batches of texts: 38.17 <span class="o">(</span>s<span class="o">)</span>
Peak memory: 143.63 <span class="o">(</span>MB<span class="o">)</span>
</code></pre></div></div> <p><b>With Ray:</b></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">ray</span>

<span class="n">num_cpus</span> <span class="o">=</span> <span class="n">psutil</span><span class="p">.</span><span class="nf">cpu_count</span><span class="p">(</span><span class="n">logical</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># 8
</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_cpus</span> <span class="o">//</span> <span class="mi">2</span>
<span class="n">ray</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">num_cpus</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>

<span class="nd">@ray.remote</span>
<span class="k">class</span> <span class="nc">NER</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ner</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">en_core_web_sm</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># load NER model
</span>
    <span class="k">def</span> <span class="nf">tag</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>  <span class="c1"># named entity tag function
</span>        <span class="n">entities</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ner</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">entities</span>

<span class="c1"># creat ray workers via actor, the ner models are loaded once at actor's construction time
</span><span class="n">ner_actors</span> <span class="o">=</span> <span class="p">[</span><span class="n">NER</span><span class="p">.</span><span class="nf">remote</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">run_multiple_tasks_in_parallel</span><span class="p">(</span><span class="n">i_batch</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">i_batch</span><span class="p">)</span>
    <span class="n">y_s</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">Paris is the capital of France.</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_workers</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="nf">get</span><span class="p">([</span><span class="n">actor</span><span class="p">.</span><span class="n">tag</span><span class="p">.</span><span class="nf">remote</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">actor</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">ner_actors</span><span class="p">,</span> <span class="n">y_s</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Total Time <span class="k">for </span>processing 10 batches of texts: 8.73 <span class="o">(</span>s<span class="o">)</span>
Peak memory: 144.62 <span class="o">(</span>MB<span class="o">)</span>
</code></pre></div></div> <p><strong><span style="color:green"><b>Worker processes</b></span></strong> in Multiprocessing.Pool are stateless, thus, for every <code class="language-plaintext highlighter-rouge">pool.map</code> call for every batch, the NER models need to be reloaded. Meanwhile, Ray’s actors are stateful, NER models are loaded only once at actor’s construction time (i.e. <code class="language-plaintext highlighter-rouge">__init__</code>function). Future batches are then processed by just calling <code class="language-plaintext highlighter-rouge">tag</code> function. This explains the outperformance of Ray over Multiprocessing. Additionally, in term of memory usage, both frameworks observe similary memory peaks, as the task does not involve any large data objects.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Viet-Phi Huynh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>