<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://huynhvp.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://huynhvp.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-18T21:41:13+00:00</updated><id>https://huynhvp.github.io/feed.xml</id><title type="html">blank</title><subtitle>Viet-Phi Huynh personal webpage. </subtitle><entry><title type="html">Serve a ML model on a single machine with Flask + Gunicorn vs. FastAPI + Uvicorn</title><link href="https://huynhvp.github.io/blog/2024/ray-serve-vs-flask/" rel="alternate" type="text/html" title="Serve a ML model on a single machine with Flask + Gunicorn vs. FastAPI + Uvicorn"/><published>2024-02-01T00:09:00+00:00</published><updated>2024-02-01T00:09:00+00:00</updated><id>https://huynhvp.github.io/blog/2024/ray-serve-vs-flask</id><content type="html" xml:base="https://huynhvp.github.io/blog/2024/ray-serve-vs-flask/"><![CDATA[<p><a href="https://docs.gunicorn.org/en/stable/design.html">Gunicorn</a> is a Python Web Server Gateway Interface (WSGI) HTTP server based on the pre-fork worker model. By default, Gunicorn workers are synchronous, which handle a single request at a time. However, Gunicorn also supports asynchronous workers, such as <code class="language-plaintext highlighter-rouge">gevent</code> or <code class="language-plaintext highlighter-rouge">eventlet</code>, which can manage multiple simutaneous requests. The web application framework, <a href="https://github.com/pallets/flask">Flask</a>, typically paired with Gunicorn, forms a synchronous or asynchronous web service depending on whether sync or async gunicorn workers are used.</p> <p><a href="https://www.uvicorn.org/">Uvicorn</a>, unlike Gunicorn, is inherently a Python Asynchronous Server Gateway Interface (ASGI) HTTP server. The web application framework, <a href="https://github.com/tiangolo/fastapi">FastAPI</a> uses Uvicorn internally to serve an asynchronous web service.</p> <p>Let’s minimally deploy a standalone ML model on a single machine with Flask + Gunicorn and FastAPI + Uvicorn and measure the throughput (i.e. time to process 1000 concurrent requests) of both systems.</p> <hr/> <p><b>Table of Contents</b></p> <ul id="markdown-toc"> <li><a href="#1-cpu-bound-deploy-a-named-entity-recognition-ner-model" id="markdown-toc-1-cpu-bound-deploy-a-named-entity-recognition-ner-model">1. CPU-bound: Deploy a Named Entity Recognition (NER) model</a></li> <li><a href="#2-cpu--and-io--bound-deploy-a-named-entity-recognition-ner-model-preceded-by-io-operations" id="markdown-toc-2-cpu--and-io--bound-deploy-a-named-entity-recognition-ner-model-preceded-by-io-operations">2. CPU- and IO- bound: Deploy a Named Entity Recognition (NER) model, preceded by IO operations.</a></li> </ul> <h4 id="1-cpu-bound-deploy-a-named-entity-recognition-ner-model">1. CPU-bound: Deploy a Named Entity Recognition (NER) model</h4> <p><b>Server</b></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># server.py
</span><span class="kn">import</span> <span class="n">fastapi</span>
<span class="kn">import</span> <span class="n">flask</span>
<span class="kn">import</span> <span class="n">spacy</span>

<span class="c1"># load NER model
</span><span class="n">ner</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">en_core_web_sm</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># flask web server
</span><span class="n">flask_app</span> <span class="o">=</span> <span class="n">flask</span><span class="p">.</span><span class="nc">Flask</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>

<span class="nd">@flask_app.get</span><span class="p">(</span><span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">ner_flask</span><span class="p">():</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">flask</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="n">json</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">entities</span> <span class="o">=</span> <span class="nf">ner</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">ent</span><span class="p">.</span><span class="n">text</span><span class="p">:</span> <span class="n">ent</span><span class="p">.</span><span class="n">label_</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">entities</span><span class="p">.</span><span class="n">ents</span><span class="p">}</span>

<span class="c1"># fastapi web server
</span><span class="n">fastapi_app</span> <span class="o">=</span> <span class="n">fastapi</span><span class="p">.</span><span class="nc">FastAPI</span><span class="p">()</span>

<span class="nd">@fastapi_app.get</span><span class="p">(</span><span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">ner_fastapi</span><span class="p">(</span><span class="n">request</span><span class="p">:</span> <span class="n">fastapi</span><span class="p">.</span><span class="n">Request</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="p">(</span><span class="k">await</span> <span class="n">request</span><span class="p">.</span><span class="nf">json</span><span class="p">()).</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">entities</span> <span class="o">=</span> <span class="nf">ner</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">ent</span><span class="p">.</span><span class="n">text</span><span class="p">:</span> <span class="n">ent</span><span class="p">.</span><span class="n">label_</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">entities</span><span class="p">.</span><span class="n">ents</span><span class="p">}</span>

</code></pre></div></div> <ul> <li>Run the synchronus API with Flask and Gunicorn’s sync workers. <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gunicorn <span class="nt">--bind</span> 127.0.0.1:5000 <span class="nt">--worker-connections</span> 1000  <span class="nt">-w</span> 2  server:flask_app
</code></pre></div> </div> </li> <li>Run the asynchronus API with Flask and Gunicorn’s async workers <code class="language-plaintext highlighter-rouge">gevent</code>. <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gunicorn <span class="nt">--bind</span> 127.0.0.1:5000 <span class="nt">--worker-class</span><span class="o">=</span>gevent <span class="nt">--worker-connections</span> 1000  <span class="nt">-w</span> 2  server:flask_app
</code></pre></div> </div> </li> <li>Run the asynchronus API with FastAPI and Uvicorn <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uvicorn <span class="nt">--host</span> 127.0.0.1 <span class="nt">--port</span> 5000 <span class="nt">--limit-concurrency</span> 1000 <span class="nt">--workers</span> 2  server:fastapi_app 
</code></pre></div> </div> </li> </ul> <p>where:</p> <ul> <li><code class="language-plaintext highlighter-rouge">-w</code> or <code class="language-plaintext highlighter-rouge">--workers</code>: number of workers for handling requests.</li> <li><code class="language-plaintext highlighter-rouge">--worker-connections</code> or <code class="language-plaintext highlighter-rouge">--limit-concurrency</code>: maximum number of concurrent requests a worker can handle.</li> </ul> <p><b>Client</b>: let’s make 1000 simultaneous requests, send them to the server, and measure the processing time.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># client.py
</span><span class="kn">import</span> <span class="n">asyncio</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="kn">from</span> <span class="n">aiohttp</span> <span class="kn">import</span> <span class="n">ClientSession</span>

<span class="n">api_url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://127.0.0.1:5000/</span><span class="sh">"</span>

<span class="n">text</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Paris[a] is the capital and most populous city of France. With an official estimated population of 2,102,650 residents
as of 1 January 2023[2] in an area of more than 105 km2 (41 sq mi),[5] Paris is the fourth-most populated city in the European
Union and the 30th most densely populated city in the world in 2022.[6] Since the 17th century, Paris has been one of the world</span><span class="sh">'</span><span class="s">
major centres of finance, diplomacy, commerce, culture, fashion, and gastronomy. For its leading role in the arts and sciences,
as well as its early and extensive system of street lighting, in the 19th century, it became known as the City of Light.[7]
The City of Paris is the centre of the Île-de-France region, or Paris Region, with an officia estimated population of 12,271,794
inhabitants on 1 January 2023, or about 19% of the population of France.
</span><span class="sh">"""</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">fetch</span><span class="p">(</span><span class="n">session</span><span class="p">:</span> <span class="n">ClientSession</span><span class="p">,</span> <span class="n">i_request</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="c1"># fetch NER result for a request
</span>    <span class="k">async</span> <span class="k">with</span> <span class="n">session</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">api_url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">i_request</span><span class="sh">"</span><span class="p">:</span> <span class="n">i_request</span><span class="p">,</span> <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="n">text</span><span class="p">})</span> <span class="k">as</span> <span class="n">response</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">i_trial</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="c1"># send 1000 simultaneous requests to the server
</span>    <span class="n">num_requests</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="k">async</span> <span class="k">with</span> <span class="nc">ClientSession</span><span class="p">()</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
        <span class="n">tasks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i_request</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_requests</span><span class="p">):</span>
            <span class="n">tasks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">fetch</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">i_trial</span><span class="o">*</span><span class="n">num_requests</span> <span class="o">+</span> <span class="n">i_request</span><span class="p">))</span>
        <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>

<span class="n">num_trials</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i_trial</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_trials</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
    <span class="n">asyncio</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="nf">main</span><span class="p">(</span><span class="n">i_trial</span><span class="p">))</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
    <span class="n">times</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Avg Time: </span><span class="si">{</span><span class="nf">sum</span><span class="p">(</span><span class="n">times</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">times</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div> <p><strong><span style="color:green"><b>Processing time for a batch of 1000 simultanous requests</b></span></strong></p> <table> <thead> <tr> <th>Number of workers</th> <th>w=1</th> <th>w=2</th> <th>w=4</th> </tr> <tr> <th>API</th> <th> </th> <th> </th> <th> </th> </tr> </thead> <tbody> <tr> <td>Flask + sync Gunicorn</td> <td>5.63 (s)</td> <td>3.34 (s)</td> <td>2.31 (s)</td> </tr> <tr> <td>Flask + async Gunicorn</td> <td>6.04 (s)</td> <td>3.29 (s)</td> <td>2.33 (s)</td> </tr> <tr> <td>FastAPI + async Uvicorn</td> <td>6.35 (s)</td> <td>3.5 (s)</td> <td>2.62 (s)</td> </tr> </tbody> </table> <p></p> <p>As the model spends all its time on the CPU to process requests, designing an async web application is not helpful.</p> <h4 id="2-cpu--and-io--bound-deploy-a-named-entity-recognition-ner-model-preceded-by-io-operations">2. CPU- and IO- bound: Deploy a Named Entity Recognition (NER) model, preceded by IO operations.</h4> <p>Just for testing purposes, let’s add a nonsensical <code class="language-plaintext highlighter-rouge">sleep(0.1)</code> to the model, to represent its IO-bound aspect, and mesure again the throughputs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># server.py
</span><span class="nd">@flask_app.get</span><span class="p">(</span><span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">ner_flask</span><span class="p">():</span>
    <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># do some IO operations
</span>    <span class="n">text</span> <span class="o">=</span> <span class="n">flask</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="n">json</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">entities</span> <span class="o">=</span> <span class="nf">ner</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">ent</span><span class="p">.</span><span class="n">text</span><span class="p">:</span> <span class="n">ent</span><span class="p">.</span><span class="n">label_</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">entities</span><span class="p">.</span><span class="n">ents</span><span class="p">}</span>

<span class="nd">@fastapi_app.get</span><span class="p">(</span><span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">ner_fastapi</span><span class="p">(</span><span class="n">request</span><span class="p">:</span> <span class="n">fastapi</span><span class="p">.</span><span class="n">Request</span><span class="p">):</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># do some IO operations
</span>    <span class="n">text</span> <span class="o">=</span> <span class="p">(</span><span class="k">await</span> <span class="n">request</span><span class="p">.</span><span class="nf">json</span><span class="p">()).</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">entities</span> <span class="o">=</span> <span class="nf">ner</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">ent</span><span class="p">.</span><span class="n">text</span><span class="p">:</span> <span class="n">ent</span><span class="p">.</span><span class="n">label_</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">entities</span><span class="p">.</span><span class="n">ents</span><span class="p">}</span>
</code></pre></div></div> <p><strong><span style="color:green"><b>Processing time for a batch of 1000 simultanous requests</b></span></strong></p> <table> <thead> <tr> <th>Priority apples</th> <th>1</th> <th>2</th> <th>4</th> </tr> </thead> <tbody> <tr> <td>Flask + sync Gunicorn</td> <td>129.69 (s)</td> <td>65.51 (s)</td> <td>33.25 (s)</td> </tr> <tr> <td>Flask + async Gunicorn</td> <td>7.62 (s)</td> <td>3.73 (s)</td> <td>3.19 (s)</td> </tr> <tr> <td>FastAPI + async Uvicorn</td> <td>7.80 (s)</td> <td>4.14 (s)</td> <td>2.74 (s)</td> </tr> </tbody> </table> <p></p> <p>Clearly, the async implementation significantly improve the efficiency of model serving.</p> <p>Additionally, in both test cases, it appears that the difference between Flask with async Gunicorn workers and FastAPI is not conclusive.</p>]]></content><author><name></name></author><category term="flask,"/><category term="gunicorn,"/><category term="fastapi,"/><category term="uvicorn"/><category term="dev"/><summary type="html"><![CDATA[Gunicorn is a Python Web Server Gateway Interface (WSGI) HTTP server based on the pre-fork worker model. By default, Gunicorn workers are synchronous, which handle a single request at a time. However, Gunicorn also supports asynchronous workers, such as gevent or eventlet, which can manage multiple simutaneous requests. The web application framework, Flask, typically paired with Gunicorn, forms a synchronous or asynchronous web service depending on whether sync or async gunicorn workers are used.]]></summary></entry><entry><title type="html">I find Ray a powerful framework for parallel computing</title><link href="https://huynhvp.github.io/blog/2024/ray-multiprocessing/" rel="alternate" type="text/html" title="I find Ray a powerful framework for parallel computing"/><published>2024-01-01T00:09:00+00:00</published><updated>2024-01-01T00:09:00+00:00</updated><id>https://huynhvp.github.io/blog/2024/ray-multiprocessing</id><content type="html" xml:base="https://huynhvp.github.io/blog/2024/ray-multiprocessing/"><![CDATA[<p>One strategy to speed up or scale a machine learning workflow is parallel/distributed processing. In python, the <a href="https://docs.python.org/3/library/multiprocessing.html">multiprocessing</a> module can serve as a solution for this purpose. However, it falls short (and can even harm overal performance) for parallel functions that requires heavy workloads or costly initialization due to data copying, moving and overhead input serialization/deserialization.</p> <p>Meanwhile, <a href="https://docs.ray.io/en/latest/index.html">Ray</a> is perfectly suited to such scenarios. Let’s work on two toy examples to illustrate that.</p> <blockquote> <p>Two core concepts (among others) of Ray that make it powerful in distrubed programming are:</p> <ul> <li><a href="https://docs.ray.io/en/latest/ray-core/key-concepts.html#tasks">Task</a>: like an asynchronous function that can be executed in a seperate process or a remote machine.</li> <li><a href="https://docs.ray.io/en/latest/ray-core/key-concepts.html#actors">Actor</a>: like an asynchronous stateful class that can run in a seperate process or remotely together with its own methods. Particularly, other actors and tasks from different processes can acess and mutate actor’s states.</li> </ul> </blockquote> <hr/> <p><b>Table of Contents</b></p> <ul id="markdown-toc"> <li><a href="#1-parallelize-a-bundle-of-matrix-multiplication-functions" id="markdown-toc-1-parallelize-a-bundle-of-matrix-multiplication-functions">1. Parallelize a bundle of matrix multiplication functions</a></li> <li><a href="#2-parallelize-a-bunch-of-named-entity-recognition-ner-models" id="markdown-toc-2-parallelize-a-bunch-of-named-entity-recognition-ner-models">2. Parallelize a bunch of Named Entity Recognition (NER) models</a></li> </ul> <h4 id="1-parallelize-a-bundle-of-matrix-multiplication-functions">1. Parallelize a bundle of matrix multiplication functions</h4> <p>Let’s parallelize a set of functions described by \(f_i = x*y_i\) where \(x\) is a fixed 10240x10240 float matrix (~800MB), representing heavy input, and \(y_i\) denotes a variable 10240x1024 float matrix for each function \(f_i\).</p> <p><b>With Multiprocessing:</b></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">tracemalloc</span>
<span class="kn">from</span> <span class="n">multiprocessing</span> <span class="kn">import</span> <span class="n">Pool</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">psutil</span>

<span class="n">num_cpus</span> <span class="o">=</span> <span class="n">psutil</span><span class="p">.</span><span class="nf">cpu_count</span><span class="p">(</span><span class="n">logical</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># 8
</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_cpus</span> <span class="o">//</span> <span class="mi">2</span>
<span class="n">pool</span> <span class="o">=</span> <span class="nc">Pool</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)</span> <span class="c1"># multiprocessing pool
</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">10240</span><span class="p">,</span> <span class="mi">10240</span><span class="p">)</span>  <span class="c1"># x takes 800MB
</span>
<span class="k">def</span> <span class="nf">task</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>  <span class="c1"># noqa: D103
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span>

<span class="k">def</span> <span class="nf">run_multiple_tasks_in_parallel</span><span class="p">(</span><span class="n">i_trial</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">i_trial</span><span class="p">)</span>
    <span class="n">y_s</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10240</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)]</span>  <span class="c1"># each y takes 10MB
</span>    <span class="n">results</span> <span class="o">=</span> <span class="n">pool</span><span class="p">.</span><span class="nf">starmap</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="nf">zip</span><span class="p">([</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">y_s</span><span class="p">),</span> <span class="n">y_s</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># benchmark
</span>    <span class="n">num_trials</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>

    <span class="c1"># tracemalloc.start()
</span>    <span class="k">for</span> <span class="n">i_trial</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_trials</span><span class="p">):</span>
        <span class="nf">run_multiple_tasks_in_parallel</span><span class="p">(</span><span class="n">i_trial</span><span class="p">)</span>
    <span class="c1"># current_mem, peak_mem = tracemalloc.get_traced_memory()
</span>    <span class="c1"># tracemalloc.stop()
</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">Avg Time: </span><span class="si">{</span><span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span><span class="o">/</span><span class="n">num_trials</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> (s)</span><span class="sh">"</span>
    <span class="p">)</span>  <span class="c1"># while benchmarking time, disable mem_usage to avoid additional calculation.
</span>    <span class="c1"># print(f"Peak memory: {peak_mem/(1024*1024):.2f} (MB)")
</span></code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Avg Time: 7.31 <span class="o">(</span>s<span class="o">)</span>
Peak memory: 1770.71 <span class="o">(</span>MB<span class="o">)</span>
</code></pre></div></div> <p><b>With Ray:</b></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">ray</span>

<span class="n">num_cpus</span> <span class="o">=</span> <span class="n">psutil</span><span class="p">.</span><span class="nf">cpu_count</span><span class="p">(</span><span class="n">logical</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># 8
</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_cpus</span> <span class="o">//</span> <span class="mi">2</span>  <span class="c1"># max parallel tasks
</span><span class="n">ray</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">num_cpus</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>  <span class="c1"># init ray
</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">10240</span><span class="p">,</span> <span class="mi">10240</span><span class="p">)</span>  <span class="c1"># x takes 800MB
</span><span class="n">x_ref</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="nf">put</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># put x in ray's object store and return its reference.
</span>
<span class="nd">@ray.remote</span>  <span class="c1"># convert func to ray's remote task
</span><span class="k">def</span> <span class="nf">task</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>  <span class="c1"># noqa: D103
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span>

<span class="k">def</span> <span class="nf">run_multiple_tasks_in_parallel</span><span class="p">(</span><span class="n">i_run</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">i_run</span><span class="p">)</span>
    <span class="n">y_s</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10240</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)]</span>  <span class="c1"># each y takes 10MB
</span>    <span class="n">ray_task_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">task</span><span class="p">.</span><span class="nf">remote</span><span class="p">(</span><span class="n">x_ref</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">y_s</span><span class="p">]</span>  <span class="c1"># pass reference of x instead of x itself
</span>    <span class="n">results</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">ray_task_list</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>

</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Avg Time: 0.95 <span class="o">(</span>s<span class="o">)</span>
Peak memory: 44.38 <span class="o">(</span>MB<span class="o">)</span>
</code></pre></div></div> <p><strong><span style="color:green"><b>Clearly,</b></span></strong> Ray is much faster and much more memory-efficient than Multiprocessing. This is due to the fact that in Multiprocessing, each process worker has to copy and pass expensive input data (i.e. \(x\)) from main process, which ends up with overhead serialization/deserialization and high memory usage. On the contrary, in Ray, the main process puts the fixed matrix \(x\) in a shared object store and pass the reference of \(x\) to each worker. This reduces memory usage as each worker now uses the same object \(x\), without duplication. Morever, for object of primitive datatypes, such as numpy array, Ray avoids serializing them, allowing process workers to read them directly without deserialization, leading to significant performance gains.</p> <h4 id="2-parallelize-a-bunch-of-named-entity-recognition-ner-models">2. Parallelize a bunch of Named Entity Recognition (NER) models</h4> <p>Considering the scenarios where a server is asked to tag named entities in a batch of text. The server calls on workers, each of which load a NER model and process a text in batch. When a batch is completed, another batch arrives and the workers continue their works.</p> <p><b>With Multiprocessing:</b></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">tracemalloc</span>
<span class="kn">from</span> <span class="n">multiprocessing</span> <span class="kn">import</span> <span class="n">Pool</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">psutil</span>
<span class="kn">import</span> <span class="n">spacy</span>

<span class="n">num_cpus</span> <span class="o">=</span> <span class="n">psutil</span><span class="p">.</span><span class="nf">cpu_count</span><span class="p">(</span><span class="n">logical</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># 8
</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_cpus</span> <span class="o">//</span> <span class="mi">2</span>
<span class="n">pool</span> <span class="o">=</span> <span class="nc">Pool</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)</span> <span class="c1"># multiprocessing pool
</span>
<span class="k">def</span> <span class="nf">task</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">ner</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">en_core_web_sm</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># load NER model
</span>    <span class="n">entities</span> <span class="o">=</span> <span class="nf">ner</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">entities</span>

<span class="k">def</span> <span class="nf">run_multiple_tasks_in_parallel</span><span class="p">(</span><span class="n">i_batch</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">i_batch</span><span class="p">)</span>
    <span class="n">text_s</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">Paris is the capital of France.</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_workers</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">pool</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">text_s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># benchmark
</span>    <span class="n">num_batches</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>

    <span class="c1"># tracemalloc.start()
</span>    <span class="k">for</span> <span class="n">i_batch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span> <span class="c1"># one batch is completed, another arrives.
</span>        <span class="nf">run_multiple_tasks_in_parallel</span><span class="p">(</span><span class="n">i_batch</span><span class="p">)</span>
    <span class="c1"># current_mem, peak_mem = tracemalloc.get_traced_memory()
</span>    <span class="c1"># tracemalloc.stop()
</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">Total Time for processing </span><span class="si">{</span><span class="n">num_batches</span><span class="si">}</span><span class="s"> batches of texts: </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> (s)</span><span class="sh">"</span>
    <span class="p">)</span>  <span class="c1"># while benchmarking time, disable mem_usage to avoid additional calculation.
</span>    <span class="c1"># print(f"Peak memory: {peak_mem/(1024*1024):.2f} (MB)")
</span></code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Total Time <span class="k">for </span>processing 10 batches of texts: 38.17 <span class="o">(</span>s<span class="o">)</span>
Peak memory: 143.63 <span class="o">(</span>MB<span class="o">)</span>
</code></pre></div></div> <p><b>With Ray:</b></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">ray</span>

<span class="n">num_cpus</span> <span class="o">=</span> <span class="n">psutil</span><span class="p">.</span><span class="nf">cpu_count</span><span class="p">(</span><span class="n">logical</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># 8
</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_cpus</span> <span class="o">//</span> <span class="mi">2</span>
<span class="n">ray</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">num_cpus</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>

<span class="nd">@ray.remote</span>
<span class="k">class</span> <span class="nc">NER</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ner</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">en_core_web_sm</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># load NER model
</span>
    <span class="k">def</span> <span class="nf">tag</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>  <span class="c1"># named entity tag function
</span>        <span class="n">entities</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ner</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">entities</span>

<span class="c1"># creat ray workers via actor, the ner models are loaded once at actor's construction time
</span><span class="n">ner_actors</span> <span class="o">=</span> <span class="p">[</span><span class="n">NER</span><span class="p">.</span><span class="nf">remote</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">run_multiple_tasks_in_parallel</span><span class="p">(</span><span class="n">i_batch</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">i_batch</span><span class="p">)</span>
    <span class="n">y_s</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">Paris is the capital of France.</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_workers</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="nf">get</span><span class="p">([</span><span class="n">actor</span><span class="p">.</span><span class="n">tag</span><span class="p">.</span><span class="nf">remote</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">actor</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">ner_actors</span><span class="p">,</span> <span class="n">y_s</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Total Time <span class="k">for </span>processing 10 batches of texts: 8.73 <span class="o">(</span>s<span class="o">)</span>
Peak memory: 144.62 <span class="o">(</span>MB<span class="o">)</span>
</code></pre></div></div> <p><strong><span style="color:green"><b>Worker processes</b></span></strong> in Multiprocessing.Pool are stateless, thus, for every <code class="language-plaintext highlighter-rouge">pool.map</code> call for every batch, the NER models need to be reloaded. Meanwhile, Ray’s actors are stateful, NER models are loaded only once at actor’s construction time (i.e. <code class="language-plaintext highlighter-rouge">__init__</code>function). Future batches are then processed by just calling <code class="language-plaintext highlighter-rouge">tag</code> function. This explains the outperformance of Ray over Multiprocessing. Additionally, in term of memory usage, both frameworks observe similary memory peaks, as the task does not involve any large data objects.</p>]]></content><author><name></name></author><category term="multiprocessing,"/><category term="ray"/><category term="dev"/><summary type="html"><![CDATA[One strategy to speed up or scale a machine learning workflow is parallel/distributed processing. In python, the multiprocessing module can serve as a solution for this purpose. However, it falls short (and can even harm overal performance) for parallel functions that requires heavy workloads or costly initialization due to data copying, moving and overhead input serialization/deserialization.]]></summary></entry><entry><title type="html">Different techniques for optimizing LLM inference</title><link href="https://huynhvp.github.io/blog/2023/llm-inference/" rel="alternate" type="text/html" title="Different techniques for optimizing LLM inference"/><published>2023-10-04T00:09:00+00:00</published><updated>2023-10-04T00:09:00+00:00</updated><id>https://huynhvp.github.io/blog/2023/llm-inference</id><content type="html" xml:base="https://huynhvp.github.io/blog/2023/llm-inference/"><![CDATA[<hr/> <h3 id="model-optimization">Model Optimization</h3> <ul> <li> <p><b><a href="https://huggingface.co/docs/optimum/onnxruntime/concept_guides/onnx">ONNX model format:</a></b> a common file format used to store deep learning models, including LLMs,that is compatible with various frameworks such as PyTorch, TensorFlow. It represents the model as a computational graph that can be optimized for efficient inference by dedicated Runtime through graph operators like redundant node eliminations (e.g. dropout is not utilized during inference, hence eliminated), node fusion (e.g. Conv layer and batchNorm layer can be merged into one computing node), constant folding (i.g. recognize and contant expressions at compile time insteading of computing them at runtime), etc. In addition, ONNX Runtime supports also model static and dynamic quantization.</p> <p>ONNX Runtime provides 2 accelerator to speed up the inference (with several limitations) on NVIDIA GPU devices:</p> <ul> <li> <p>CUDA Execution Provider: not possible to run quantized model.</p> </li> <li> <p>TensorRT Execution Provider: only support static quantization, offer its own graph optimization techniques rather than inheriting from ONNX Runtime.</p> </li> </ul> </li> <li> <p><b>Quantization</b>:</p> <ul> <li> <p><a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a>: support 8-bit and 4-bit zero-point and absmax quantization. As 8-bit and 4-bit represent a very limited range, quantizing 32-bit or 16-bit big values (i.e. outliers) can cause a degradation in the inference. bitsandbytes addresses this issue with mixed-precision decomposition. The idea is: in matrix multiplication of (hidden feature, weights), a feature column \(i\) that contain outliers is multiplied with the weight row \(i\) in the original precision (FP32, FP16 or BF16), otherwise, the features and weights are quantized, multiplied and dequantized.</p> </li> <li> <p><a href="https://huggingface.co/blog/gptq-integration">GPTQ:</a> different from bitsandbytes, in GPTQ, only weights are quantized into 8,4 or 3 bits, activations are retained in float16 and during the inference, the actual computation is performed in float16 rather than in smaller bits. The key idea of GPTQ is to find a quantized version \(\hat{W}\) of model weights \(W\) by minimizing the MSE loss \(\| WX - \hat{W}X \|\) on a reference calibration dataset.</p> </li> </ul> </li> </ul> <h3 id="os-optimization">OS Optimization</h3> <ul> <li> <p><b><a href="https://www.anyscale.com/blog/continuous-batching-llm-inference">Batching:</a></b> in order to better utilize the hardware (i.e. GPU memory) during the inference processing multiple requests or samples at once in a batch (i.e. batching) is a good pratice.</p> <ul> <li> <p>Static batching: it has to wait for incoming requests that fulfill a predefined batch size before processing. Also, GPU resources reserved for a batch is blocked until the last request in the batch is finished, leading to inefficient GPU utilization when requests have have significantly different execution times.</p> </li> <li> <p>Continous batching: to tackle the issue of static batching, continous batching allows to drop finished requests from the batch, and replace them with new requests without need to wait for the completion of every other requests in the batch.</p> </li> </ul> </li> <li> <p><b><a href="https://blog.vllm.ai/2023/06/20/vllm.html">Paged Attention:</a></b> manages more effectively attention keys and values (KV cache). Instead of keeping all attention keys/values of a sequence in a contiguous GPU memory block, PagedAttention splits the KV cache into blocks, each block containing the keys and values for a number of tokens can be stored flexible in non-contiguous memory blocks. When computing the attention for the whole sequence, involved KV blocks are retrived via a block mapping table. Moreover, by using an universal block table, multiple requests that share the same input prompt can share prompt’s KV(s) rather than computing them individually for each request, resulting in efficient memory utilization.</p> </li> <li> <p><b><a href="https://blog.vllm.ai/2023/06/20/vllm.html">Flash Attention:</a></b> accelerates the attention computation and reduces its memory footprint without zero accuracy loss, especially for long context, with two techniques: (i) avoid computing huge \(QK^T\) matrix at once by breaking down \(K\) into chunks \(K_i\), perform each \(QK_i^T\), then combine results; (ii) \(Q\) and \(K_i\) are moved from GPU’s memory to SRAM in order to compute attention in SRAM, thereby reducing memory reads/writes.</p> </li> </ul>]]></content><author><name></name></author><category term="NLP,"/><category term="LLM"/><category term="Inference"/><category term="dev"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Cheat Sheet of NLP Practitioner</title><link href="https://huynhvp.github.io/blog/2023/nlp-cheatsheet/" rel="alternate" type="text/html" title="Cheat Sheet of NLP Practitioner"/><published>2023-09-08T00:09:00+00:00</published><updated>2023-09-08T00:09:00+00:00</updated><id>https://huynhvp.github.io/blog/2023/nlp-cheatsheet</id><content type="html" xml:base="https://huynhvp.github.io/blog/2023/nlp-cheatsheet/"><![CDATA[<hr/> <p>I am actively maintaining this blog post, gathering NLP papers around large language models, information extraction, structured data-related downstream applications, {retrieval, tool}-augmented language models, prompting techniques and beyond.</p> <hr/> <p><b>Table of Contents</b></p> <ul id="markdown-toc"> <li><a href="#1-knowledge-retrieval---augmented-language-model-" id="markdown-toc-1-knowledge-retrieval---augmented-language-model-"><b>1. Knowledge Retrieval - Augmented Language Model </b></a> <ul> <li><a href="#retrieval-meets-long-context-large-language-models-xu-arxiv-2023" id="markdown-toc-retrieval-meets-long-context-large-language-models-xu-arxiv-2023">Retrieval Meets Long Context Large Language Models (Xu, arxiv 2023)</a></li> <li><a href="#meta-training-with-demonstration-retrieval-for-efficient-few-shot-learning-mueller-finding-acl-2023" id="markdown-toc-meta-training-with-demonstration-retrieval-for-efficient-few-shot-learning-mueller-finding-acl-2023">Meta-training with Demonstration Retrieval for Efficient Few-shot Learning (Mueller, Finding ACL 2023)</a></li> <li><a href="#glimmer-generalized-late-interaction-memory-reranker-de-jong-arxiv-2023" id="markdown-toc-glimmer-generalized-late-interaction-memory-reranker-de-jong-arxiv-2023">GLIMMER: generalized late-interaction memory reranker (de Jong, arxiv 2023)</a></li> <li><a href="#pre-computed-memory-or-on-the-fly-encoding-a-hybrid-approach-to-retrieval-augmentation-makes-the-most-of-your-compute-de-jong-icml-2023" id="markdown-toc-pre-computed-memory-or-on-the-fly-encoding-a-hybrid-approach-to-retrieval-augmentation-makes-the-most-of-your-compute-de-jong-icml-2023">Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute (de Jong, ICML 2023)</a></li> <li><a href="#how-does-generative-retrieval-scale-to-millions-of-passages-pradeep-et-al-genirsigir-2023" id="markdown-toc-how-does-generative-retrieval-scale-to-millions-of-passages-pradeep-et-al-genirsigir-2023">How Does Generative Retrieval Scale to Millions of Passages? (Pradeep∗ et al., GenIR@SIGIR 2023)</a></li> <li><a href="#recitation-augmented-language-models-sun-et-al-iclr-2023" id="markdown-toc-recitation-augmented-language-models-sun-et-al-iclr-2023">Recitation-Augmented Language Models (Sun et al., ICLR 2023)</a></li> <li><a href="#replug-retrieval-augmented-black-box-language-models-shi-et-al-arxiv-2023" id="markdown-toc-replug-retrieval-augmented-black-box-language-models-shi-et-al-arxiv-2023">REPLUG: Retrieval-Augmented Black-Box Language Models (Shi et al., arxiv 2023)</a></li> <li><a href="#rethinking-with-retrieval-faithful-large-language-model-inference-he-et-al-arxiv-2023" id="markdown-toc-rethinking-with-retrieval-faithful-large-language-model-inference-he-et-al-arxiv-2023">Rethinking with Retrieval: Faithful Large Language Model Inference (He et al., arxiv 2023)</a></li> <li><a href="#interleaving-retrieval-with-chain-of-thought-reasoning-for-knowledge-intensive-multi-step-questions-trivedi-et-al-acl-2023" id="markdown-toc-interleaving-retrieval-with-chain-of-thought-reasoning-for-knowledge-intensive-multi-step-questions-trivedi-et-al-acl-2023">Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions (Trivedi et al., ACL 2023)</a></li> <li><a href="#transformer-memory-as-a-differentiable-search-index-tay-et-al-neurips-2022" id="markdown-toc-transformer-memory-as-a-differentiable-search-index-tay-et-al-neurips-2022">Transformer Memory as a Differentiable Search Index (Tay et al., Neurips 2022)</a></li> <li><a href="#autoregressive-search-engines-generating-substrings-as-document-identifiers-bevilacqua-et-al-neurips-2022" id="markdown-toc-autoregressive-search-engines-generating-substrings-as-document-identifiers-bevilacqua-et-al-neurips-2022">Autoregressive Search Engines: Generating Substrings as Document Identifiers (Bevilacqua et al., Neurips 2022)</a></li> <li><a href="#atlas-few-shot-learning-with-retrieval-augmented-language-models-izacard-et-al-arxiv-2022" id="markdown-toc-atlas-few-shot-learning-with-retrieval-augmented-language-models-izacard-et-al-arxiv-2022">Atlas: Few-shot Learning with Retrieval Augmented Language Models (Izacard et al., arxiv 2022)</a></li> <li><a href="#eider-empowering-document-level-relation-extraction-with-efficient-evidence-extraction-and-inference-stage-fusion-xie-et-al-acl-findings-2022" id="markdown-toc-eider-empowering-document-level-relation-extraction-with-efficient-evidence-extraction-and-inference-stage-fusion-xie-et-al-acl-findings-2022">EIDER: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion (Xie et al., ACL Findings 2022)</a></li> <li><a href="#dont-prompt-search-mining-based-zero-shot-learning-with-language-models-van-de-kar-et-al-emnlp-2022" id="markdown-toc-dont-prompt-search-mining-based-zero-shot-learning-with-language-models-van-de-kar-et-al-emnlp-2022">Don’t Prompt, Search! Mining-based Zero-Shot Learning with Language Models (van de Kar et al., EMNLP 2022)</a></li> <li><a href="#skill-structured-knowledge-infusion-for-large-language-models-moiseev-et-al-naacl-2022" id="markdown-toc-skill-structured-knowledge-infusion-for-large-language-models-moiseev-et-al-naacl-2022">SKILL: Structured Knowledge Infusion for Large Language Models (Moiseev et al., NAACL 2022)</a></li> <li><a href="#leveraging-passage-retrieval-with-generative-models-for-open-domain-question-answering-izacard-et-al-eacl-2021" id="markdown-toc-leveraging-passage-retrieval-with-generative-models-for-open-domain-question-answering-izacard-et-al-eacl-2021">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering (Izacard et al., EACL 2021)</a></li> <li><a href="#realm-retrieval-augmented-language-model-pre-training-guu-et-al-icml-2020" id="markdown-toc-realm-retrieval-augmented-language-model-pre-training-guu-et-al-icml-2020">REALM: Retrieval-Augmented Language Model Pre-Training (Guu et al., ICML 2020)</a></li> <li><a href="#generalization-through-memorization-nearest-neighbor-language-models-khandelwal-et-al-iclr-2020" id="markdown-toc-generalization-through-memorization-nearest-neighbor-language-models-khandelwal-et-al-iclr-2020">Generalization through Memorization: Nearest Neighbor Language Models (Khandelwal et al., ICLR 2020):</a></li> </ul> </li> <li><a href="#2-information-extraction" id="markdown-toc-2-information-extraction"><b>2. Information Extraction</b></a> <ul> <li><a href="#how-to-unleash-the-power-of-large-language-models-for-few-shot-relation-extraction-xu-et-al-sustainlpacl-2023" id="markdown-toc-how-to-unleash-the-power-of-large-language-models-for-few-shot-relation-extraction-xu-et-al-sustainlpacl-2023">How to Unleash the Power of Large Language Models for Few-shot Relation Extraction? (Xu et al., SustaiNLP@ACL 2023)</a></li> <li><a href="#gpt-re-in-context-learning-for-relation-extraction-using-large-language-models-wan-et-al-arxiv-2023" id="markdown-toc-gpt-re-in-context-learning-for-relation-extraction-using-large-language-models-wan-et-al-arxiv-2023">GPT-RE: In-context Learning for Relation Extraction using Large Language Models (Wan et al., arxiv 2023)</a></li> <li><a href="#universal-information-extraction-as-unified-semantic-matching-lou-et-al-aaai-2023" id="markdown-toc-universal-information-extraction-as-unified-semantic-matching-lou-et-al-aaai-2023">Universal Information Extraction as Unified Semantic Matching (Lou et al., AAAI 2023)</a></li> <li><a href="#structgpt-a-general-framework-for-large-language-model-to-reason-over-structured-data-jiang-et-al-arxiv-2023" id="markdown-toc-structgpt-a-general-framework-for-large-language-model-to-reason-over-structured-data-jiang-et-al-arxiv-2023">StructGPT: A General Framework for Large Language Model to Reason over Structured Data (Jiang et al., arxiv 2023)</a></li> <li><a href="#gpt4graph-can-large-language-models-understand-graph-structured-data-an-empirical-evaluation-and-benchmarking-guo-et-al-arxiv-2023" id="markdown-toc-gpt4graph-can-large-language-models-understand-graph-structured-data-an-empirical-evaluation-and-benchmarking-guo-et-al-arxiv-2023">GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking (Guo et al., arxiv 2023)</a></li> <li><a href="#neurostructural-decoding-neural-text-generation-with-structural-constraints-bastan-et-al-acl-2023" id="markdown-toc-neurostructural-decoding-neural-text-generation-with-structural-constraints-bastan-et-al-acl-2023">NEUROSTRUCTURAL DECODING: Neural Text Generation with Structural Constraints (Bastan et al., ACL 2023)</a></li> <li><a href="#retrieval-enhanced-generative-model-for-large-scale-knowledge-graph-completion-yu-et-al-sigir-2023" id="markdown-toc-retrieval-enhanced-generative-model-for-large-scale-knowledge-graph-completion-yu-et-al-sigir-2023">Retrieval-Enhanced Generative Model for Large-Scale Knowledge Graph Completion (Yu et al., SIGIR 2023)</a></li> <li><a href="#knowledge-base-completion-for-long-tail-entities-chen-et-al-arxiv-2023" id="markdown-toc-knowledge-base-completion-for-long-tail-entities-chen-et-al-arxiv-2023">Knowledge Base Completion for Long-Tail Entities (Chen et al., arxiv 2023)</a></li> <li><a href="#instructuie-multi-task-instruction-tuning-for-unified-information-extraction-wang-et-al-arxiv-2023" id="markdown-toc-instructuie-multi-task-instruction-tuning-for-unified-information-extraction-wang-et-al-arxiv-2023">InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction (Wang et al., arxiv 2023)</a></li> <li><a href="#unifying-molecular-and-textual-representations-via-multi-task-language-modelling-christofidellis-et-al-icml-2023" id="markdown-toc-unifying-molecular-and-textual-representations-via-multi-task-language-modelling-christofidellis-et-al-icml-2023">Unifying Molecular and Textual Representations via Multi-task Language Modelling (Christofidellis et al., ICML 2023)</a></li> <li><a href="#triggering-multi-hop-reasoning-for-question-answering-in-language-models-using-soft-prompts-and-random-walks-misra-et-al-findings-acl-2023" id="markdown-toc-triggering-multi-hop-reasoning-for-question-answering-in-language-models-using-soft-prompts-and-random-walks-misra-et-al-findings-acl-2023">Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks (Misra et al., Findings ACL 2023)</a></li> <li><a href="#flexible-grammar-based-constrained-decoding-for-language-models-geng-et-al-arxiv-2023" id="markdown-toc-flexible-grammar-based-constrained-decoding-for-language-models-geng-et-al-arxiv-2023">Flexible Grammar-Based Constrained Decoding for Language Models (Geng et al., arxiv 2023)</a></li> <li><a href="#methods-for-measuring-updating-and-visualizing-factual-beliefs-in-language-models-hase-et-al-eacl-2023" id="markdown-toc-methods-for-measuring-updating-and-visualizing-factual-beliefs-in-language-models-hase-et-al-eacl-2023">Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models (Hase et al., EACL 2023)</a></li> <li><a href="#can-lms-learn-new-entities-from-descriptions-challenges-in-propagating-injected-knowledge-onoe-et-al-acl-2023" id="markdown-toc-can-lms-learn-new-entities-from-descriptions-challenges-in-propagating-injected-knowledge-onoe-et-al-acl-2023">Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge (Onoe et al., ACL 2023)</a></li> <li><a href="#demonstratesearchpredict-composing-retrieval-and-language-models-for-knowledge-intensive-nlp-khattab-et-al-arxiv-2023" id="markdown-toc-demonstratesearchpredict-composing-retrieval-and-language-models-for-knowledge-intensive-nlp-khattab-et-al-arxiv-2023">DEMONSTRATE–SEARCH–PREDICT: Composing retrieval and language models for knowledge-intensive NLP (Khattab et al., arxiv 2023)</a></li> <li><a href="#codeie-large-code-generation-models-are-better-few-shot-information-extractors-li-et-al-acl-2023" id="markdown-toc-codeie-large-code-generation-models-are-better-few-shot-information-extractors-li-et-al-acl-2023">CODEIE: Large Code Generation Models are Better Few-Shot Information Extractors (Li et al., ACL 2023)</a></li> <li><a href="#evaluating-language-models-for-knowledge-base-completion-veseli-et-al-eswc-2023" id="markdown-toc-evaluating-language-models-for-knowledge-base-completion-veseli-et-al-eswc-2023">Evaluating Language Models for Knowledge Base Completion (Veseli et al., ESWC 2023)</a></li> <li><a href="#exploiting-asymmetry-for-synthetic-training-data-generation-synthie-and-the-case-of-information-extraction-josifoski-et-al-arxiv-2023" id="markdown-toc-exploiting-asymmetry-for-synthetic-training-data-generation-synthie-and-the-case-of-information-extraction-josifoski-et-al-arxiv-2023">Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction (Josifoski et al., arxiv 2023)</a></li> <li><a href="#large-language-model-is-not-a-good-few-shot-information-extractor-but-a-good-reranker-for-hard-samples-ma-et-al-arxiv-2023" id="markdown-toc-large-language-model-is-not-a-good-few-shot-information-extractor-but-a-good-reranker-for-hard-samples-ma-et-al-arxiv-2023">Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples! (Ma et al., arxiv 2023)</a></li> <li><a href="#understanding-fine-tuning-for-factual-knowledge-extraction-from-language-models-kazemi-et-al-submitted-to-jmlr" id="markdown-toc-understanding-fine-tuning-for-factual-knowledge-extraction-from-language-models-kazemi-et-al-submitted-to-jmlr">Understanding Fine-tuning for Factual Knowledge Extraction from Language Models (Kazemi et al., submitted to JMLR)</a></li> <li><a href="#crawling-the-internal-knowledge-base-of-language-models-cohen-et-al-tbd" id="markdown-toc-crawling-the-internal-knowledge-base-of-language-models-cohen-et-al-tbd">Crawling The Internal Knowledge-Base of Language Models (Cohen et al., TBD)</a></li> <li><a href="#greaselm-graph-reasoning-enhanced-language-models-for-question-answering-zhang-et-al-iclr-2022" id="markdown-toc-greaselm-graph-reasoning-enhanced-language-models-for-question-answering-zhang-et-al-iclr-2022">GREASELM: Graph Reasoning Enhanced Language Models for Question Answering (Zhang et al., ICLR 2022)</a></li> <li><a href="#entity-cloze-by-date-what-lms-know-about-unseen-entities-onoe-et-al-finding-naacl-2022" id="markdown-toc-entity-cloze-by-date-what-lms-know-about-unseen-entities-onoe-et-al-finding-naacl-2022">Entity Cloze By Date: What LMs Know About Unseen Entities (Onoe et al., Finding NAACL 2022)</a></li> <li><a href="#large-language-models-struggle-to-learn-long-tail-knowledge-kandpal-et-al-arxiv-2022" id="markdown-toc-large-language-models-struggle-to-learn-long-tail-knowledge-kandpal-et-al-arxiv-2022">Large Language Models Struggle to Learn Long-Tail Knowledge (Kandpal et al., arxiv 2022)</a></li> <li><a href="#unified-structure-generation-for-universal-information-extraction-lu-et-al-acl-2022" id="markdown-toc-unified-structure-generation-for-universal-information-extraction-lu-et-al-acl-2022">Unified Structure Generation for Universal Information Extraction (Lu et al., ACL 2022)</a></li> <li><a href="#genie-generative-information-extraction-josifoski-et-al-naacl-2022" id="markdown-toc-genie-generative-information-extraction-josifoski-et-al-naacl-2022">GenIE: Generative Information Extraction (Josifoski et al., NAACL 2022)</a></li> <li><a href="#eider-empowering-document-level-relation-extraction-with-efficient-evidence-extraction-and-inference-stage-fusion-xie-et-al-acl-findings-2022-1" id="markdown-toc-eider-empowering-document-level-relation-extraction-with-efficient-evidence-extraction-and-inference-stage-fusion-xie-et-al-acl-findings-2022-1">EIDER: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion (Xie et al., ACL Findings 2022)</a></li> <li><a href="#knowprompt-knowledge-aware-prompt-tuning-with-synergistic-optimization-for-relation-extraction-chen-et-al-the-webconf-2022" id="markdown-toc-knowprompt-knowledge-aware-prompt-tuning-with-synergistic-optimization-for-relation-extraction-chen-et-al-the-webconf-2022">KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction (Chen et al., The WebConf 2022)</a></li> <li><a href="#rewire-then-probe-a-contrastive-recipe-for-probing-biomedical-knowledge-of-pre-trained-language-models-meng-et-al-acl-2022" id="markdown-toc-rewire-then-probe-a-contrastive-recipe-for-probing-biomedical-knowledge-of-pre-trained-language-models-meng-et-al-acl-2022">Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models (Meng et al., ACL 2022)</a></li> <li><a href="#do-pre-trained-models-benefit-knowledge-graph-completion-a-reliable-evaluation-and-a-reasonable-approach-lv-et-al-acl-findings-2022" id="markdown-toc-do-pre-trained-models-benefit-knowledge-graph-completion-a-reliable-evaluation-and-a-reasonable-approach-lv-et-al-acl-findings-2022">Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach (Lv et al., ACL-Findings 2022)</a></li> <li><a href="#simkgc-simple-contrastive-knowledge-graph-completion-with-pre-trained-language-models-wang-et-al-acl-2022" id="markdown-toc-simkgc-simple-contrastive-knowledge-graph-completion-with-pre-trained-language-models-wang-et-al-acl-2022">SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models (Wang et al. ACL 2022)</a></li> <li><a href="#task-specific-pre-training-and-prompt-decomposition-for-knowledge-graph-population-with-language-models-li-et-al-lm-kbciswc-2022-challenge" id="markdown-toc-task-specific-pre-training-and-prompt-decomposition-for-knowledge-graph-population-with-language-models-li-et-al-lm-kbciswc-2022-challenge">Task-specific Pre-training and Prompt Decomposition for Knowledge Graph Population with Language Models (Li et al., LM-KBC@ISWC 2022 Challenge)</a></li> <li><a href="#genre-autoregressive-entity-retrieval-de-cao-et-al-iclr-2021" id="markdown-toc-genre-autoregressive-entity-retrieval-de-cao-et-al-iclr-2021">GENRE: Autoregressive Entity Retrieval (De Cao et al., ICLR 2021).</a></li> <li><a href="#structured-prediction-as-translation-between-augmented-natural-languages-paolini-et-al-iclr-2021" id="markdown-toc-structured-prediction-as-translation-between-augmented-natural-languages-paolini-et-al-iclr-2021">Structured Prediction as Translation Between Augmented Natural Languages (Paolini et al., ICLR 2021)</a></li> <li><a href="#how-can-we-know-what-language-models-know-jiang-et-al-tacl-2020" id="markdown-toc-how-can-we-know-what-language-models-know-jiang-et-al-tacl-2020">How Can We Know What Language Models Know? (Jiang et al., TACL 2020)</a></li> </ul> </li> <li><a href="#3-prompting-methods-" id="markdown-toc-3-prompting-methods-"><b>3. Prompting Methods </b></a> <ul> <li><a href="#large-language-models-as-optimizers-yang-arxiv-2023" id="markdown-toc-large-language-models-as-optimizers-yang-arxiv-2023">Large Language Models as Optimizers (Yang, arxiv 2023)</a></li> <li><a href="#how-far-can-camels-go-exploring-the-state-of-instruction-tuning-on-open-resources-wang-et-al-arxiv-2023--the-flan-collection-designing-data-and-methods-for-effective-instruction-tuning-longpre-et-al-icml-2023" id="markdown-toc-how-far-can-camels-go-exploring-the-state-of-instruction-tuning-on-open-resources-wang-et-al-arxiv-2023--the-flan-collection-designing-data-and-methods-for-effective-instruction-tuning-longpre-et-al-icml-2023">How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources (Wang et al., arxiv 2023) + The Flan Collection: Designing Data and Methods for Effective Instruction Tuning (Longpre et al., ICML 2023)</a></li> <li><a href="#least-to-most-prompting-enables-complex-reasoning-in-large-language-models-zhou-et-al-iclr-2023" id="markdown-toc-least-to-most-prompting-enables-complex-reasoning-in-large-language-models-zhou-et-al-iclr-2023">Least-to-Most Prompting Enables Complex Reasoning in Large Language Models (Zhou et al., ICLR 2023)</a></li> <li><a href="#multitask-prompt-tuning-enables-parameter-efficient-transfer-learning-wang-et-al-iclr-2023" id="markdown-toc-multitask-prompt-tuning-enables-parameter-efficient-transfer-learning-wang-et-al-iclr-2023">Multitask Prompt Tuning enables Parameter-Efficient Transfer Learning (Wang et al., ICLR 2023)</a></li> <li><a href="#grammar-prompting-for-domain-specific-language-generation-with-large-language-models-wang-et-al-arxiv-2023" id="markdown-toc-grammar-prompting-for-domain-specific-language-generation-with-large-language-models-wang-et-al-arxiv-2023">Grammar Prompting for Domain-Specific Language Generation with Large Language Models (Wang et al., arxiv 2023)</a></li> <li><a href="#symbol-tuning-improves-in-context-learning-in-language-models-wei-et-al-arxiv-2023" id="markdown-toc-symbol-tuning-improves-in-context-learning-in-language-models-wei-et-al-arxiv-2023">Symbol Tuning Improves In-Context Learning In Language Models (Wei et al., arxiv 2023)</a></li> <li><a href="#selective-annotation-makes-language-models-better-few-shot-learners-su-et-al-iclr-2023" id="markdown-toc-selective-annotation-makes-language-models-better-few-shot-learners-su-et-al-iclr-2023">Selective Annotation Makes Language Models Better Few-Shot Learners (Su et al., ICLR 2023)</a></li> <li><a href="#learning-to-reason-and-memorize-with-self-notes-lanchantin-et-al-arxiv-2023" id="markdown-toc-learning-to-reason-and-memorize-with-self-notes-lanchantin-et-al-arxiv-2023">Learning to Reason and Memorize with Self-Notes (Lanchantin et al., arxiv 2023)</a></li> <li><a href="#self-consistency-improves-chain-of-thought-reasoning-in-language-models-wang-et-al-iclr-2023" id="markdown-toc-self-consistency-improves-chain-of-thought-reasoning-in-language-models-wang-et-al-iclr-2023">Self-Consistency improves Chain Of Thought Reasoning in Language Models (Wang et al., ICLR 2023)</a></li> <li><a href="#maieutic-prompting-logically-consistent-reasoning-with-recursive-explanations-jung-et-al-emnlp-2022" id="markdown-toc-maieutic-prompting-logically-consistent-reasoning-with-recursive-explanations-jung-et-al-emnlp-2022">Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations (Jung et al., EMNLP 2022):</a></li> <li><a href="#metaicl-learning-to-learn-in-context-min-et-al-naacl-2022" id="markdown-toc-metaicl-learning-to-learn-in-context-min-et-al-naacl-2022">MetaICL: Learning to Learn In Context (Min et al., NAACL 2022)</a></li> <li><a href="#self-instruct-aligning-lm-with-self-generated-instructions-wang-et-al-arxiv-2022" id="markdown-toc-self-instruct-aligning-lm-with-self-generated-instructions-wang-et-al-arxiv-2022">Self-Instruct: Aligning LM with Self Generated Instructions (Wang et al., arxiv 2022)</a></li> <li><a href="#finetuned-language-models-are-zero-shot-learners-wei-et-al-iclr-2022" id="markdown-toc-finetuned-language-models-are-zero-shot-learners-wei-et-al-iclr-2022">Finetuned Language Models are Zero-Shot Learners (Wei et al., ICLR 2022)</a></li> <li><a href="#multitask-prompted-training-enables-zero-shot-task-generalization-sanh-et-al-iclr-2022" id="markdown-toc-multitask-prompted-training-enables-zero-shot-task-generalization-sanh-et-al-iclr-2022">Multitask Prompted Training Enables Zero-Shot Task Generalization (Sanh et al., ICLR 2022)</a></li> <li><a href="#chain-of-thought-prompting-elicits-reasoning-in-large-language-models-wei-et-al-neurips-2022" id="markdown-toc-chain-of-thought-prompting-elicits-reasoning-in-large-language-models-wei-et-al-neurips-2022">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei et al., Neurips 2022)</a></li> <li><a href="#do-prompt-based-models-really-understand-the-meaning-of-their-prompts-webson-et-al-naacl-2022" id="markdown-toc-do-prompt-based-models-really-understand-the-meaning-of-their-prompts-webson-et-al-naacl-2022">Do Prompt-Based Models Really Understand the Meaning of Their Prompts? (Webson et al., NAACL 2022)</a></li> <li><a href="#prefix-tuning-optimizing-continuous-prompts-for-generation-li-et-al-acl-2021" id="markdown-toc-prefix-tuning-optimizing-continuous-prompts-for-generation-li-et-al-acl-2021">Prefix-Tuning: Optimizing Continuous Prompts for Generation (Li et al., ACL 2021)</a></li> <li><a href="#the-power-of-scale-for-parameter-efficient-prompt-tuning-lester-et-al-emnlp-2021" id="markdown-toc-the-power-of-scale-for-parameter-efficient-prompt-tuning-lester-et-al-emnlp-2021">The Power of Scale for Parameter-Efficient Prompt Tuning (Lester et al., EMNLP 2021)</a></li> </ul> </li> <li><a href="#4-tools-augmented-language-model-" id="markdown-toc-4-tools-augmented-language-model-"><b>4. Tools-Augmented Language Model </b></a> <ul> <li><a href="#toolformer-language-models-can-teach-themselves-to-use-tools-schick-et-al-arxiv-2023" id="markdown-toc-toolformer-language-models-can-teach-themselves-to-use-tools-schick-et-al-arxiv-2023">Toolformer: Language Models Can Teach Themselves to Use Tools (Schick et al., arxiv 2023)</a></li> <li><a href="#react-synergizing-reasoning-and-acting-in-language-models-yao-et-al-iclr-2023" id="markdown-toc-react-synergizing-reasoning-and-acting-in-language-models-yao-et-al-iclr-2023">React: Synergizing Reasoning and Acting in Language Models (Yao et al., ICLR 2023)</a></li> <li><a href="#binding-language-models-in-symbolic-languages-cheng-et-al-iclr-2023" id="markdown-toc-binding-language-models-in-symbolic-languages-cheng-et-al-iclr-2023">Binding Language Models in Symbolic Languages (Cheng et al., ICLR 2023)</a></li> </ul> </li> <li><a href="#5-misc-" id="markdown-toc-5-misc-"><b>5. Misc </b></a> <ul> <li><a href="#task-specific-skill-localization-in-fine-tuned-language-models-panigrahi-et-al-icml-2023" id="markdown-toc-task-specific-skill-localization-in-fine-tuned-language-models-panigrahi-et-al-icml-2023">Task-Specific Skill Localization in Fine-tuned Language Models (Panigrahi et al., ICML 2023)</a></li> <li><a href="#same-pre-training-loss-better-downstream-implicit-bias-matters-for-language-models-liu-et-al-icml-2023" id="markdown-toc-same-pre-training-loss-better-downstream-implicit-bias-matters-for-language-models-liu-et-al-icml-2023">Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models (Liu et al., ICML 2023)</a></li> <li><a href="#doremi-optimizing-data-mixtures-speeds-up-language-model-pretraining-xie-et-al-arxiv-2023" id="markdown-toc-doremi-optimizing-data-mixtures-speeds-up-language-model-pretraining-xie-et-al-arxiv-2023">DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining (Xie et al., arxiv 2023)</a></li> <li><a href="#data-selection-for-language-models-via-importance-resampling-xie-et-al-arxiv-2023" id="markdown-toc-data-selection-for-language-models-via-importance-resampling-xie-et-al-arxiv-2023">Data Selection for Language Models via Importance Resampling (Xie et al., arxiv 2023)</a></li> <li><a href="#language-models-represent-space-and-time-gurnee-et-al-arxiv-2023" id="markdown-toc-language-models-represent-space-and-time-gurnee-et-al-arxiv-2023">Language Models represent Space and Time (Gurnee et al., arxiv 2023)</a></li> <li><a href="#can-foundation-models-wrangle-your-data-narayan-et-al-vldb-2023" id="markdown-toc-can-foundation-models-wrangle-your-data-narayan-et-al-vldb-2023">Can Foundation Models Wrangle Your Data? (Narayan et al., VLDB 2023)</a></li> <li><a href="#ranking-and-tuning-pre-trained-models-a-new-paradigm-for-exploiting-model-hubs-you-et-al-jmlr-2023--logme-practical-assessment-of-pre-trained-models-for-transfer-learning-you-et-al-icml-2021" id="markdown-toc-ranking-and-tuning-pre-trained-models-a-new-paradigm-for-exploiting-model-hubs-you-et-al-jmlr-2023--logme-practical-assessment-of-pre-trained-models-for-transfer-learning-you-et-al-icml-2021">Ranking and Tuning Pre-trained Models: A New Paradigm for Exploiting Model Hubs (You et al., JMLR 2023) + LogME: Practical Assessment of Pre-trained Models for Transfer Learning (You et al., ICML 2021)</a></li> <li><a href="#on-exploring-the-reasoning-capability-of-large-language-models-with-knowledge-graphs-lo-et-al-genirsigir-2023" id="markdown-toc-on-exploring-the-reasoning-capability-of-large-language-models-with-knowledge-graphs-lo-et-al-genirsigir-2023">On Exploring the Reasoning Capability of Large Language Models with Knowledge Graphs (Lo et al., GenIR@SIGIR 2023)</a></li> <li><a href="#towards-robust-and-efficient-continual-language-learning-fisch-et-al-arxiv-2023" id="markdown-toc-towards-robust-and-efficient-continual-language-learning-fisch-et-al-arxiv-2023">Towards Robust and Efficient Continual Language Learning (Fisch et al., arxiv 2023)</a></li> <li><a href="#beyond-scale-the-diversity-coefficient-as-a-data-quality-metric-demonstrates-llms-are-pre-trained-on-formally-diverse-data-lee-et-al-icml-2023" id="markdown-toc-beyond-scale-the-diversity-coefficient-as-a-data-quality-metric-demonstrates-llms-are-pre-trained-on-formally-diverse-data-lee-et-al-icml-2023">Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data (Lee et al., ICML 2023)</a></li> <li><a href="#textbooks-are-all-you-need-gunsasekar-et-al-arxiv-2023" id="markdown-toc-textbooks-are-all-you-need-gunsasekar-et-al-arxiv-2023">Textbooks Are All You Need (Gunsasekar et al., arxiv 2023)</a></li> <li><a href="#evaluating-and-enhancing-structural-understanding-capabilities-of-large-language-models-on-tables-via-input-designs-sui-et-al-arxiv-2023" id="markdown-toc-evaluating-and-enhancing-structural-understanding-capabilities-of-large-language-models-on-tables-via-input-designs-sui-et-al-arxiv-2023">Evaluating and Enhancing Structural Understanding Capabilities of Large Language Models on Tables via Input Designs (Sui et al., arxiv 2023)</a></li> <li><a href="#benchmarking-large-language-model-capabilities-for-conditional-generation-joshua-et-al-acl-2023" id="markdown-toc-benchmarking-large-language-model-capabilities-for-conditional-generation-joshua-et-al-acl-2023">Benchmarking Large Language Model Capabilities for Conditional Generation (Joshua et al., ACL 2023)</a></li> <li><a href="#lost-in-the-middle-how-language-models-use-long-contexts-nelson-et-al-arxiv-2023" id="markdown-toc-lost-in-the-middle-how-language-models-use-long-contexts-nelson-et-al-arxiv-2023">Lost in the Middle: How Language Models Use Long Contexts (Nelson et al., arxiv 2023)</a></li> <li><a href="#faith-and-fate-limits-of-transformers-on-compositionality-dziri-et-al-arxiv-2023" id="markdown-toc-faith-and-fate-limits-of-transformers-on-compositionality-dziri-et-al-arxiv-2023">Faith and Fate: Limits of Transformers on Compositionality (Dziri et al., arxiv 2023)</a></li> <li><a href="#improving-representational-continuity-with-supervised-continued-pretraining-sun-et-al-arxiv-2023--fine-tuning-can-distort-pretrained-features-and-underperform-out-of-distribution-kumar-et-al-iclr-2022" id="markdown-toc-improving-representational-continuity-with-supervised-continued-pretraining-sun-et-al-arxiv-2023--fine-tuning-can-distort-pretrained-features-and-underperform-out-of-distribution-kumar-et-al-iclr-2022">Improving Representational Continuity with Supervised Continued Pretraining (Sun et al., arxiv 2023) + Fine-tuning can distort pretrained features and underperform out-of-distribution (Kumar et al., ICLR 2022)</a></li> <li><a href="#can-language-models-solve-graph-problems-in-natural-language-wang-et-al-arxiv-2023" id="markdown-toc-can-language-models-solve-graph-problems-in-natural-language-wang-et-al-arxiv-2023">Can Language Models Solve Graph Problems in Natural Language? (Wang et al., arxiv 2023)</a></li> <li><a href="#selection-inference-exploiting-large-language-models-for-interpretable-logical-reasoning-creswell-et-al-iclr-2023" id="markdown-toc-selection-inference-exploiting-large-language-models-for-interpretable-logical-reasoning-creswell-et-al-iclr-2023">Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning (Creswell et al., ICLR 2023)</a></li> <li><a href="#trusting-your-evidence-hallucinate-less-with-context-aware-decoding-shi-et-al-arxiv-2023" id="markdown-toc-trusting-your-evidence-hallucinate-less-with-context-aware-decoding-shi-et-al-arxiv-2023">Trusting Your Evidence: Hallucinate Less with Context-aware Decoding (Shi et al., arxiv 2023)</a></li> <li><a href="#codet5-open-code-large-language-models-for-code-understanding-and-generation-wang-et-al-arxiv-2023" id="markdown-toc-codet5-open-code-large-language-models-for-code-understanding-and-generation-wang-et-al-arxiv-2023">CodeT5+: Open Code Large Language Models for Code Understanding and Generation (Wang et al., arxiv 2023)</a></li> <li><a href="#emergent-world-representations-exploring-a-sequence-model-trained-on-a-synthetic-task-li-et-al-iclr-2023" id="markdown-toc-emergent-world-representations-exploring-a-sequence-model-trained-on-a-synthetic-task-li-et-al-iclr-2023">Emergent World Representations: Exploring a Sequence Model Trained On a Synthetic Task (Li et al., ICLR 2023).</a></li> <li><a href="#quantifying-memorization-across-neural-language-models-carlini-et-al-iclr-2023" id="markdown-toc-quantifying-memorization-across-neural-language-models-carlini-et-al-iclr-2023">Quantifying Memorization Across Neural Language Models (Carlini et al., ICLR 2023)</a></li> <li><a href="#i2d2-inductive-knowledge-distillation-with-neurologic-and-self-imitation-bhagavatula-et-al" id="markdown-toc-i2d2-inductive-knowledge-distillation-with-neurologic-and-self-imitation-bhagavatula-et-al">I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation (Bhagavatula et al.):</a></li> <li><a href="#symbolic-knowledge-distillation-from-general-language-models-to-commonsense-models-west-et-al-naacl-2022" id="markdown-toc-symbolic-knowledge-distillation-from-general-language-models-to-commonsense-models-west-et-al-naacl-2022">Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (West et al., NAACL 2022)</a></li> <li><a href="#efficient-training-of-language-models-to-fill-in-the-middle-bavarian-et-al-arxiv-2022" id="markdown-toc-efficient-training-of-language-models-to-fill-in-the-middle-bavarian-et-al-arxiv-2022">Efficient Training of Language Models to Fill in the Middle (Bavarian et al., arxiv 2022).</a></li> <li><a href="#language-models-of-code-are-few-shot-commonsense-learners-madaan-et-al-emnlp-2022" id="markdown-toc-language-models-of-code-are-few-shot-commonsense-learners-madaan-et-al-emnlp-2022">Language Models of Code are Few-Shot Commonsense Learners (Madaan et al., EMNLP 2022).</a></li> <li><a href="#fast-model-editing-at-scale-mitchell-et-al-iclr-2022" id="markdown-toc-fast-model-editing-at-scale-mitchell-et-al-iclr-2022">Fast Model Editing at Scale (Mitchell et al., ICLR 2022).</a></li> <li><a href="#locating-and-editing-factual-associations-in-gpt-meng-et-al-neurips-2022" id="markdown-toc-locating-and-editing-factual-associations-in-gpt-meng-et-al-neurips-2022">Locating and Editing Factual Associations in GPT (Meng et al., Neurips 2022).</a></li> <li><a href="#understanding-dataset-difficulty-with-v-usable-information-ethayarajh-et-al-icml-2022" id="markdown-toc-understanding-dataset-difficulty-with-v-usable-information-ethayarajh-et-al-icml-2022">Understanding Dataset Difficulty with V-Usable Information (Ethayarajh et al., ICML 2022).</a></li> <li><a href="#a-contrastive-framework-for-neural-text-generation-su-et-al-neurips-2022" id="markdown-toc-a-contrastive-framework-for-neural-text-generation-su-et-al-neurips-2022">A Contrastive Framework for Neural Text Generation (Su et al., NeurIPS 2022).</a></li> <li><a href="#the-trade-offs-of-domain-adaptation-for-neural-language-models-grangier-et-al-acl-2022" id="markdown-toc-the-trade-offs-of-domain-adaptation-for-neural-language-models-grangier-et-al-acl-2022">The Trade-offs of Domain Adaptation for Neural Language Models (Grangier et al., ACL 2022)</a></li> <li><a href="#memorization-without-overfitting-analyzing-the-training-dynamics-of-large-language-models-tirumala-et-al-neurips-2022" id="markdown-toc-memorization-without-overfitting-analyzing-the-training-dynamics-of-large-language-models-tirumala-et-al-neurips-2022">Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models (Tirumala et al., Neurips 2022)</a></li> <li><a href="#from-zero-shot-to-few-shot-text-classification-with-setfit-tunstall-et-al-enlsp-at-neurips-2022" id="markdown-toc-from-zero-shot-to-few-shot-text-classification-with-setfit-tunstall-et-al-enlsp-at-neurips-2022">From zero-shot to few-shot Text Classification with SetFit (Tunstall et al., ENLSP at Neurips 2022)</a></li> <li><a href="#improving-language-models-by-retrieving-from-trillions-of-token-borgeaud-et-al-icml-2022" id="markdown-toc-improving-language-models-by-retrieving-from-trillions-of-token-borgeaud-et-al-icml-2022">Improving Language Models by Retrieving from Trillions of Token (Borgeaud et al., ICML 2022)</a></li> <li><a href="#adapt-and-distill-developing-small-fast-and-effective-pretrained-language-models-for-domains-yao-et-al-acl-findings-2021" id="markdown-toc-adapt-and-distill-developing-small-fast-and-effective-pretrained-language-models-for-domains-yao-et-al-acl-findings-2021">Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains (Yao et al., ACL Findings 2021)</a></li> <li><a href="#udalm-unsupervised-domain-adaptation-through-language-modeling-karouzos-et-al-naacl-2021" id="markdown-toc-udalm-unsupervised-domain-adaptation-through-language-modeling-karouzos-et-al-naacl-2021">UDALM: Unsupervised Domain Adaptation through Language Modeling (Karouzos et al., NAACL 2021)</a></li> <li><a href="#mauve-measuring-the-gap-between-neural-text-and-human-text-using-divergence-frontiers-pillutla-et-al-neurips-2021" id="markdown-toc-mauve-measuring-the-gap-between-neural-text-and-human-text-using-divergence-frontiers-pillutla-et-al-neurips-2021">MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers (Pillutla et al., NeurIPS 2021).</a></li> <li><a href="#simcse-simple-contrastive-learning-of-sentence-embeddings-gao-et-al-emnlp-2021" id="markdown-toc-simcse-simple-contrastive-learning-of-sentence-embeddings-gao-et-al-emnlp-2021">SimCSE: Simple Contrastive Learning of Sentence Embeddings (Gao et al., EMNLP 2021).</a></li> <li><a href="#surface-form-competition-why-the-highest-probability-answer-isnt-always-right-holtzman-et-al-emnlp-2021" id="markdown-toc-surface-form-competition-why-the-highest-probability-answer-isnt-always-right-holtzman-et-al-emnlp-2021">Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right (Holtzman et al., EMNLP 2021)</a></li> <li><a href="#prefix-tuning-optimizing-continuous-prompts-for-generation-li-et-al-acl-2021-1" id="markdown-toc-prefix-tuning-optimizing-continuous-prompts-for-generation-li-et-al-acl-2021-1">Prefix-Tuning: Optimizing Continuous Prompts for Generation (Li et al., ACL 2021)</a></li> <li><a href="#the-power-of-scale-for-parameter-efficient-prompt-tuning-lester-et-al-emnlp-2021-1" id="markdown-toc-the-power-of-scale-for-parameter-efficient-prompt-tuning-lester-et-al-emnlp-2021-1">The Power of Scale for Parameter-Efficient Prompt Tuning (Lester et al., EMNLP 2021)</a></li> <li><a href="#biomegatron-larger-biomedical-domain-language-model-shin-et-al-emnlp-2020" id="markdown-toc-biomegatron-larger-biomedical-domain-language-model-shin-et-al-emnlp-2020">BioMegatron: Larger Biomedical Domain Language Model (Shin et al., EMNLP 2020)</a></li> <li><a href="#dont-stop-pretraining-adapt-language-models-to-domains-and-tasks-gururangan-et-al-acl-2020" id="markdown-toc-dont-stop-pretraining-adapt-language-models-to-domains-and-tasks-gururangan-et-al-acl-2020">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks (Gururangan et al., ACL 2020):</a></li> <li><a href="#generalization-through-memorization-nearest-neighbor-language-models-khandelwal-et-al-iclr-2020-1" id="markdown-toc-generalization-through-memorization-nearest-neighbor-language-models-khandelwal-et-al-iclr-2020-1">Generalization through Memorization: Nearest Neighbor Language Models (Khandelwal et al., ICLR 2020):</a></li> <li><a href="#when-does-label-smoothing-help-müller-et-al-neurips-2019" id="markdown-toc-when-does-label-smoothing-help-müller-et-al-neurips-2019">When does label smoothing help?. (Müller et al., NeurIPS 2019).</a></li> <li><a href="#back-translation-improving-neural-machine-translation-models-with-monolingual-data-sennrich-et-al-acl-2016" id="markdown-toc-back-translation-improving-neural-machine-translation-models-with-monolingual-data-sennrich-et-al-acl-2016">Back Translation Improving Neural Machine Translation Models with Monolingual Data (Sennrich et al., ACL 2016)</a></li> </ul> </li> </ul> <h5 id="1-knowledge-retrieval---augmented-language-model-"><b>1. Knowledge Retrieval - Augmented Language Model </b></h5> <p><b>2023</b></p> <ul> <li> <h6 id="retrieval-meets-long-context-large-language-models-xu-arxiv-2023"><a href="https://arxiv.org/pdf/2310.03025.pdf">Retrieval Meets Long Context Large Language Models</a> (Xu, arxiv 2023)</h6> <p>The paper demonstrates the benefit of passage retrieval for both short and long context language model on 7 close book QA datasets. Several observations ares:</p> <ul> <li>Retrieval is especially useful for 4K-context LLM as the context provided for each question in 7 tasks usually exceeds 4K tokens.</li> <li>With the same retrieval passages, 16K-context LLMs exploits the passages more effectively than 4K-context LLMs, leading to better performance. Authors speculate this is because 4K-context suffers from “lost in the middle” issue while retrieval passages are only at the beginning part of 16K-context LLM.</li> <li>Retrieval is useful for both long and short-context LLMs (i.e. outperform the without-retrieval counterpart)</li> </ul> </li> <li> <h6 id="meta-training-with-demonstration-retrieval-for-efficient-few-shot-learning-mueller-finding-acl-2023"><a href="https://arxiv.org/pdf/2307.00119.pdf">Meta-training with Demonstration Retrieval for Efficient Few-shot Learning</a> (Mueller, Finding ACL 2023)</h6> <p>Inspired by <a href="https://huynhvp.github.io/blog/2023/nlp-cheatsheet/#metaicl-learning-to-learn-in-context-min-et-al-naacl-2022">MetaICL</a>, this paper proposes few-shot meta learning <em>with demonstration retrieval</em> that leverages multi-task learning on a large variety of tasks, endowing <b>small language models</b> with better ability to generalize across different tasks and domains. The meta-training is conducted by employing a freezed dense passage retriever (i.e. RAG) to retrieve <em>k</em> demonstrations \(z\) for an input \(x\). Each demonstration \(z\) is then concatenated with input \(x\) and is fed into a BART-large model. The model is trained to predict the output \(y\) marginalizing over <em>k</em> retrieved demonstrations:</p> \[p (y|x) \approx \prod_{i}^{N} \sum_{k}^{K} \; p_{retriever} (z_k | x) \; p_{PLM} (y | x, z_k, y_{1:i-1})\] <p><img src="/assets/img/cheatsheet/meta_retrieval.png" alt="" style="width: 35%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>To adapt BART to various tasks without architectural modification, input and output are standardized according to an unified template:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Encoder: "question: ... \n answer: [MASK] \n context: \n"
  Decoder: "question: ... \n answer: ..."
</code></pre></div> </div> <p>Author argues this template aligns with BART’s pre-training objective (generate both question and answer). The results stress the importance of external knowledge bank to the few-shot performance of meta-learned model.</p> </li> <li> <h6 id="glimmer-generalized-late-interaction-memory-reranker-de-jong-arxiv-2023"><a href="https://arxiv.org/pdf/2306.10231.pdf">GLIMMER: generalized late-interaction memory reranker</a> (de Jong, arxiv 2023)</h6> <p>LUMEN (see <a href="https://huynhvp.github.io/blog/2023/nlp-cheatsheet/#pre-computed-memory-or-on-the-fly-encoding-a-hybrid-approach-to-retrieval-augmentation-makes-the-most-of-your-compute-de-jong-icml-2023">here</a>) is a quality-compute trade-off solution for retrieval-augmented LM. <b>GLIMMER</b> is built on LUME with several improvements:</p> <ul> <li>The memory encoder is fine-tuned, instead of being frozen.</li> <li>Live fine-tuned encoder is divided into two parts: <ul> <li>First <em>N</em> layers (Live-A) is used to re-rank retrieved passages conditioned on the input question. Top-k relevant passages are kept and passed to last <em>M</em> encoder layers.</li> <li>Last <em>M</em> layers (Live-B) updates the representation of each {question input, retrieved passage} and sends them to the decoder, similarly to LUME’s live encoder.</li> </ul> </li> </ul> <p><img src="/assets/img/cheatsheet/glimmer.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>Both memory encoder, live encoders and deocer are fine-tuned end-to-end with multi-task learning to endow components better generalization capability. The training loss is inspired Atlas’s PDist (see <a href="https://huynhvp.github.io/blog/2023/nlp-cheatsheet/#atlas-few-shot-learning-with-retrieval-augmented-language-models-izacard-et-al-arxiv-2022">atlas</a>): promote the passages that lower the generation’s perplexity to be ranked higher.</p> <p>\(\mathcal{L_{pdist}} = KL (p^{rank} \; | \; p^{LM})\) where \(p^{rank} \varpropto exp(score(passage_k, \; question)/\tau)\) and \(p^{LM} \varpropto exp(log \; p_{LM} (answer \; | \; passage_k, \; question)/\tau)\)</p> </li> <li> <h6 id="pre-computed-memory-or-on-the-fly-encoding-a-hybrid-approach-to-retrieval-augmentation-makes-the-most-of-your-compute-de-jong-icml-2023"><a href="https://arxiv.org/pdf/2301.10448.pdf">Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute</a> (de Jong, ICML 2023)</h6> <p><b>LUMEN</b> is a retrieval-augmented LM that neutralizes the pros/cons of Fusion-in-Decoder LM (<em>on-the-fly-encoding</em>) and memory-augmented LM (<em>pre-computed memory</em>):</p> <ul> <li>Fusion-in-Decoder (FiD) encodes the retrieved passages on-the-fly together with the input \(Enc(input, passage_i)\). Hence, it is expensive if number of retrieved passages is large.</li> <li>Memory-augmented pre-computes the embedding of passages, without taking the input into account, \(Enc(passage_i)\). Hence, the representation of each passage is input-agnostic. This method is more efficient than FiD but less powerful.</li> </ul> <p><b>LUMEN</b> trade-off both methods by employing a frozen large encoder to pre-compute the embeddings for the passages and a live (aka. parameters will be fine-tuned) question-encoder to encode the question. Then, another live encoder but with smaller number of parameters will update the representation of a passage conditioned on the input. Finally, the decoder performs cross-attention over {input, retrieved passage} pairs to select the most relevant one, just like in FiD. As the live encoder in LUME is much smaller than FiD, LUME is more efficient accordingly.</p> <p><img src="/assets/img/cheatsheet/lumen.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>Experiments demonstrates the LUME’s performance is very close to FiD while being much cheaper.</p> </li> <li> <h6 id="how-does-generative-retrieval-scale-to-millions-of-passages-pradeep-et-al-genirsigir-2023"><a href="https://arxiv.org/pdf/2305.11841.pdf">How Does Generative Retrieval Scale to Millions of Passages?</a> (Pradeep∗ et al., GenIR@SIGIR 2023)</h6> <p>Differential search index (DSI) has emerged as a novel generative retrieval, deviating from common retrieve-then-rerank paradigm. While working effectively on small corpus ( O(100k) documents ), this paper has pointed that the performance of DSI when scaling to large corpus ( O(1M) documents ) is significantly degraded. Several observations:</p> <ul> <li>Synthetic queries for the fine-tuning of retrieval phase are important, as it helps to reduce the coverage gap: indexing phase sees the whole corpus while this is not the case for retrieval phase.</li> <li>In case of MSMarco, indexing phase does not yield gain.</li> <li>In case of MSMarco, using Naive IDs as Document Identifiers has strongest performance among {Atomic ID, Naive ID, Semantic ID}. However, scaling LM from T5-XL (3B) to T5-XXL (11B) causes performance drop.</li> </ul> <p>Note: the paper consider only MS-Marco as large corpus, which may cause a bias in the evaluation.</p> </li> <li> <h6 id="recitation-augmented-language-models-sun-et-al-iclr-2023"><a href="https://arxiv.org/pdf/2210.01296.pdf">Recitation-Augmented Language Models</a> (Sun et al., ICLR 2023)</h6> <p>Leveraging the memorizing ability of large language models, the paper propose <b>RECITE</b>, a recite-and-answer strategy for close book question answering. Instead of retrieving supports from external corpus (“open book”), the model tries to recite the relevant knowledge stored in the model parameters (“close book”) and then answer the question (in the similar way to chain-of-thought prompting).</p> <p><img src="/assets/img/cheatsheet/recite_1.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>They introduces 3 <b>RECITE</b> settings which are all based on in-context learning:</p> <ul> <li>Recite a single passage for the question, then answer the question using the recitation.</li> <li>Self-consistency ensemble: recite multiple passages instead of one using top-k sampling, each passage leads to an answer, the final answer is decided via majority vote.</li> <li>Multiple-recite-and-answer: recite multiple passages, then concatenate them and output a single answer based on the concatenation.</li> <li>Passage hint-based diversified recitation: solution to hallucinating wrong recitation while ensuring enough diversity of generated recitations, this method proposes to recite the “hint” first which then serves as a guide to recite the associated passage appropriately. In Wikipedia, the hint of a section can be the concatenation of page title + section title.</li> </ul> <p><img src="/assets/img/cheatsheet/recite_2.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>Ablation studies shows <b>RECITE</b> improves when number of recitations increases, and is robust to the prompt’s demonstration.</p> </li> <li> <h6 id="replug-retrieval-augmented-black-box-language-models-shi-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2301.12652.pdf">REPLUG: Retrieval-Augmented Black-Box Language Models</a> (Shi et al., arxiv 2023)</h6> <p><b>REPLUG</b> (Retrieve and Plug) takes another approach in retrieval-augmented LM where the LM is a black-box (hence, unknown parameters and impossible to retrain/finetune) and the retriever is either frozen or trainable. This characteristic makes <b>REPLUG</b> particularly flexible that it can be used with any existing LLM (yes, <b>only large LM (&gt;100B parameters)</b>).</p> <p><img src="/assets/img/cheatsheet/replug_1.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>The architecture of the retriever is almost the same for every retrieval-augmented LMs. It is based on dual-encoder to compute top-k documents for the input query in the embedding space. If the retriever is trainable, we then have <b>REPLUG LSR</b> (REPLUG with LM-Supervised Retrieval). Similarly to <b>Atlas’s Perplexity Distillation</b> training objective, the retriever is trained to predict how much each retrieved document would improve the black-box LM perplexity, conditioned on the query. The LM perplexity scores are normalized (via softmax) and are then distilled into the retriever to encourage the documents yielding the higher LM perplexity.</p> <p>The black-box LM takes in both the input query and every retrieved document, producing a probability distribution. These distributions are combined using an ensemble method to form the final probability distribution.</p> <p><img src="/assets/img/cheatsheet/replug_2.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p><b>REPLUG</b> can benefit rare entities.</p> </li> <li> <h6 id="rethinking-with-retrieval-faithful-large-language-model-inference-he-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2301.00303.pdf">Rethinking with Retrieval: Faithful Large Language Model Inference</a> (He et al., arxiv 2023)</h6> <p>The knowledge stored in the LM’s parameters may inevitable be incomplete, out-of-date or incorrect. The paper proposes <b> rethinking with retrieval (RR)</b>, a simple post-preprocessing method that uses the a diverse set of reasoning steps obtained from the <b> chain-of-thought</b> prompting to retrieve relevant knowledge from external sources, to improve the the explanation, thereby, the prediction of LLMs. This approach require no additional training or finetuning and is not limited by the input length of LLMs.</p> <p><img src="/assets/img/cheatsheet/rr.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>Specifically, for each sentence in each reasoning path, a retriever (e.g. BM25) is employed to retrieve the top-K most relevant paragraph from an external knowledge source (e.g. Wikipedia). Then, each sentence is assigned three scores:</p> <ul> <li>semantic similarity score: calculated by the maximum cosine similarity between the sentence embeddings of retrieved paragraphs and the sentence.</li> <li>entailment score and contradiction score: use a NLI model to calculate those scores assuming the most similar paragraph (according to above semantic similarity) as the premise and the sentence as the hypothesis.</li> </ul> <p>The faithfulness of a reasoning path is computed using the scores of all sentences in the path. To arrive at final prediction, priority is given to the reasoning paths that exhibit higher levels of faithfulness.</p> <p><b> rethinking with retrieval (RR)</b> outperforms <b> chain-of-thought</b> prompting even when using smaller LMs.</p> </li> <li> <h6 id="interleaving-retrieval-with-chain-of-thought-reasoning-for-knowledge-intensive-multi-step-questions-trivedi-et-al-acl-2023"><a href="https://arxiv.org/pdf/2212.10509.pdf">Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions</a> (Trivedi et al., ACL 2023)</h6> <p><b>Interleaving Retrieval with Chain-of-Thought (IRCoT)</b> interleaves a knowledge retriever at each reasoning step obtained from chain-of-thought (CoT) prompting to mutually guide the retrieval by CoT and vice-versa. This strategy allows to retrieve more relevant supports for later reasoning steps in the reasoning path, thereby, enhance the answer for complex multi-step reasoning question.</p> <p><img src="/assets/img/cheatsheet/ircot.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p><b>IRCoT</b> follows \(\textsf{retrieve-and-read}\) mechanism:</p> <ul> <li>\(\textsf{retrieve}\) step: perform interleavingly and iteratively two sub steps until the termination criterion (e.g. the phrase “the answer is” is generated in the reasoning path) is met: <ul> <li>CoT-guided retrieval step (“retrieve”): using the last generated CoT sentence in the reasoning path as a query to retrieve relevant support paragraph from external knowledge source.</li> <li>Retrieval-guided reasoning step (“reasoning”): using the question, the paragraphs collected so far and the CoT sentences generated so far to generate the next CoT sentence in the reasoning path.</li> </ul> </li> <li> <p>\(\textsf{read}\) step: all the support paragraphs collected from the \(\textsf{retrieve}\) step are appended to the CoT prompting as the context, asking LLM to generate the answer. The prompting template can appear like:</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">  Wikipedia Title: &lt;Page Title&gt;</span><span class="w">
</span><span class="gp">  &lt;Paragraph Text&gt;</span><span class="w">
</span><span class="c">  ...
</span><span class="gp">  Wikipedia Title: &lt;Page Title&gt;</span><span class="w">
</span><span class="gp">  &lt;Paragraph Text&gt;</span><span class="w">
</span><span class="go">
</span><span class="gp">  Q: &lt;question&gt;</span><span class="w">
</span><span class="gp">  A: &lt;CoT-Sent-1&gt;</span><span class="w"> </span>... &lt;CoT-Sent-n&gt;  
</code></pre></div> </div> <p><b>IRCoT</b> has shown some remarkable benefits:</p> </li> <li><b>IRCoT</b> retriever outperforms (with higher recall) one-step retriever that relies solely on the question as query.</li> <li><b>IRCoT</b> is also effective for smaller LMs (e.g. T5-Flan-large 0.7B). <b>IRCoT</b> for QA based on Flan-T5-XL (3B) even outperform GPT3 (175B) with no retriever or on-step retriever.</li> <li>Although <b>IRCoT</b> retriever (\(\textsf{retrieve}\) step) can itself produce the answer from its last generated CoT sentence, the \(\textsf{read}\) step where a separate QA reader is employed to consider all collected support paragraphs together is still necessary, since it yields much better accuracy.</li> </ul> </li> </ul> <p><b>2022</b></p> <ul> <li> <h6 id="transformer-memory-as-a-differentiable-search-index-tay-et-al-neurips-2022"><a href="https://arxiv.org/pdf/2202.06991.pdf">Transformer Memory as a Differentiable Search Index</a> (Tay et al., Neurips 2022)</h6> <p>Traditional Information retrieval (IR) system involves <em>retrieve-then-rank</em> mechanism: (i) given a query, \(k\) nearest documents are retrieved from an indexed corpus, (ii) retrieved documents are then sorted. The paper presents the <b>Differentiable Search Index (DSI)</b>, a new paradigm for learning an end-to-end search system where the <em>retrieve</em> phase and the <em>rank</em> phase are performed within a single seq2seq neural model (e.g. T5, BART). It is shown with an appropriate design for {document representation, document identifier representation, document indexing strategy, training strategy}, <b>DSI</b> can obtain significant gain over state-of-the-art baselines (dual encoder, BM25):</p> <ul> <li>Document representation: a document is represented by its first \(L\) tokens.</li> <li>Document identifier representation: a document can be tagged by an unique integer, or unique \(tokenizable\) string, or semantically structured identifiers.</li> <li>Documents are indexed by training a Seq2Seq model to learn the mapping \(\textsf{doc_tokens} \rightarrow \textsf{docid}\). In other word, this task makes the model memorize which document corresponds to which identifier.</li> <li>At the same time, the model is jointly trained (under multi-task learning, similarly to T5 training style) with (\(\textsf{query, docid}\)) samples, so that at inference time, the decoder is able to generate relevant \(\textsf{docid}\) (s) for given input query.</li> </ul> <p><img src="/assets/img/cheatsheet/dsi.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>In summary, the multi-task training of <b>DSI</b> looks like:</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">  Input: "Document indexing: document_tokens" --&gt;</span><span class="w"> </span>T5 <span class="nt">--</span><span class="o">&gt;</span> Output: <span class="s2">"docid"</span>
<span class="gp">  Input: "Document retrieval: query_tokens" --&gt;</span><span class="w"> </span>T5 <span class="nt">--</span><span class="o">&gt;</span> Output: <span class="s2">"docid"</span>
</code></pre></div> </div> </li> <li> <h6 id="autoregressive-search-engines-generating-substrings-as-document-identifiers-bevilacqua-et-al-neurips-2022"><a href="https://arxiv.org/pdf/2204.10628.pdf">Autoregressive Search Engines: Generating Substrings as Document Identifiers</a> (Bevilacqua et al., Neurips 2022)</h6> <p>Autoregressive models has emerged as the de-facto way to address the knowledge-intensive language task (KILT). This paper suggests that this kind of model also has the capability to performance the evidence retrieval with minimal intervention to the model’s architecture. The whole evidence corpus is indexed using an efficient data structure (<a href="https://www.cs.jhu.edu/~langmea/resources/lecture_notes/bwt_and_fm_index.pdf">FM index</a>) in the way that for a given token, we can quickly figure out all possible next tokens in the corpus. The paper introduces SEAL, an autoregressive model that can directly locate the answer as well as the document containing the answer via generation constraint on FM index, for a query. It proposes a clever scoring function combining LM’s score and token’s frequency in the corpus while taking into account the fact that a document can contain multiple supports.</p> <p>Ablation studies reveal:</p> <ul> <li>SEAL can work well even with small size (~400M)</li> <li>Performance increase with a larger beam search, and seems to start decreasing when the beam reaches between 10 and 15.</li> <li>Decoding maximum length is a crucial factor, where longer output sequence is more informative than shorter one.</li> </ul> <p><img src="/assets/img/cheatsheet/seal.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="atlas-few-shot-learning-with-retrieval-augmented-language-models-izacard-et-al-arxiv-2022"><a href="https://arxiv.org/pdf/2208.03299.pdf">Atlas: Few-shot Learning with Retrieval Augmented Language Models</a> (Izacard et al., arxiv 2022)</h6> <p>Medium LMs augmented with retrieval capability can be competitive with (or even outperform) LLMs in few-shot learning while being much more parameter-efficient. <b> Atlas </b> consists of a retriever and a LM that are jointly learnt with a focus on the ability to perform various knowledge intensive tasks with very few training examples.</p> <ul> <li> <p>Retriever: initialized from a BERT-based dual-encoder pre-trained with contrastive loss.</p> </li> <li> <p>LM: all tasks are casted as text-to-text. The model is initialized from the T5-1.1-lm-adapt (trained on unlabeled text only + trained with LM objective) variants.</p> </li> </ul> <p><img src="/assets/img/cheatsheet/atlas.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Before fine-tuning with few-shot examples, the retriever and the LM are jointly pretrained with a set of objectives:</p> <ul> <li> <p>Attention Distillation (<b>ADist</b>): the cross-attention scores between the input documents and the output are distilled into the retriever to encourage the retrieval of documents of higher scores.</p> </li> <li> <p>End-to-end training of Multi-Document Reader and Retriever (<b>EMDR2</b>): minimize the loss (similar to REALM):</p> \[log \sum_{z \in Z}{ p_{LM} (x | q, z ) * p_{retriever} (z | q) }\] <p>where \(q\) is the input query, \(x\) is the output, and \(z\) is retrieved documents, playing as latent variable.</p> </li> <li> <p>Perplexity Distillation (<b>PDist</b>): train the retriever to predict how much each retrieved document would improve the LM perplexity, conditioned on the query. The LM perplexity scores are normalized (via softmax) and are then distilled into the retriever to promote the documents yielding the higher LM perplexity at later stage.</p> </li> <li> <p>Leave-one-out Perplexity Distillation (<b>LOOP</b>): if removing one of the retrieved documents, how much it affects the prediction of the LM.</p> </li> <li> <p>Prefix language modelling: divide the sentence into two parts, taking first part as input and predicting the second part.</p> </li> <li> <p>Masked language modelling: similar to T5.</p> </li> </ul> <p>The experimentation show Perplexity Distillation and Mask language modelling to be more stable than other objectives.</p> <p>As retriever’s parameters are updated every training step, re-calculating the embedding and re-indexing the whole collection of documents is significantly computationally expensive (or even impossible), <b>Atlas</b> propose several efficient index update: (i) re-indexing the collection of document embedding every \(k\) epoch; (ii) instead of re-indexing the whole collection, only perform on top-k documents return; or (iii) freeze the index of documents.</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">  A remarkable feature of retrieval-augmented model is that their knowledge can be kept up-to-date 
  without retraining, by simply maintaining a collection of documents.
</span></code></pre></div> </div> </li> <li> <h6 id="eider-empowering-document-level-relation-extraction-with-efficient-evidence-extraction-and-inference-stage-fusion-xie-et-al-acl-findings-2022"><a href="https://arxiv.org/pdf/2106.08657.pdf">EIDER: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion</a> (Xie et al., ACL Findings 2022)</h6> <p><b>EIDER: Extracted Evidence Empowered Relation Extraction</b></p> <p><img src="/assets/img/cheatsheet/eider.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Typical document-level relation extraction models rely on the whole document to infer the relation of an entity pair in the document. On the one hand, a minimal set of sentences (i.e. evidences) in the documents is enough for human to annotate the relation, taking the whole document as input may add noise and ambiguity to the model. On the other hand, there is no way to extract such minimal set perfectly, leading to missing important information. <b>EIDER</b> alleviates both aspect by introducing:</p> <ul> <li>Joint training of relation extraction end evidence sentence extraction: a base encoder is employed to learn the representation of the relation from the counterparts of the head entity, tail entity and the whole document \(p(r \mid e_h, e_t, c_{h,t})\), as well as to learn the representation of each evidence sentence \(s_n\) given the head and tail entity \(p (s_n \mid e_h, e_t)\). For the training, evidence sentences for each entity pair in a document can be either manually provided, or extracted using simple heuristics (e.g. a sentence containing both head and tail entities is considered as an evidence for this entity pair).</li> <li>Fusion of evidence in Inference: the score of each candidate relation is given by two inferences: one with the prediction from the whole documents, one with the prediction from the set of extracted evidence sentences (a subset of original document). \s</li> </ul> </li> <li> <h6 id="dont-prompt-search-mining-based-zero-shot-learning-with-language-models-van-de-kar-et-al-emnlp-2022"><a href="https://arxiv.org/pdf/2210.14803.pdf">Don’t Prompt, Search! Mining-based Zero-Shot Learning with Language Models</a> (van de Kar et al., EMNLP 2022)</h6> <p>\(\textsf{Generate-filter-finetune}\)<b> approach for zero-shot learning</b></p> <p><img src="/assets/img/cheatsheet/dontprompt_mine.png" alt="" style="width: 35%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>The paper introduces a retrieval augmented zero-shot learning method which is more flexible and interpretable than prompting methods. The present method is reliant on an unlabeled corpus playing as knowledge source (e.g. the corpus used for pretraining), a regex-like mining pattern and a set of verbalizer that represents a downstream task (similar to prompting), such as \(\textsf{(is} \mid \textsf{was) \{VERBALIZER\}*. \{INPUT\} }\) for sentiment analysis where \(\textsf{VERBALIZER} \in\) \(\textsf{\{good, great, awesome, etc\}}\) for positive label and \(\textsf{\{bad, awful, terrible, etc\}}\) for negative label. It consists of 3 steps:</p> <ol> <li>Using the regex-based mining pattern to extract training samples from the unlabeled corpus. For example, given the pattern above, the sentences following <em>“is good”</em> or <em>“was good”</em> are examples of the positive class, and the sentences following <em>“is bad”</em>, <em>“was bad”</em> are examples of the negative class.</li> <li>As mined samples can be noisy, they are filtered by zero-shot prompting. Specifically, samples in which predicated label by zero-shot prompting and the mined label do not match will be removed.</li> <li>The mined dataset is then used to finetune a pretrained LM for the downstream task. Intuitively, the original zero-shot learning is casted as full finetuning with the help of mined dataset.</li> </ol> <p>Experimented on sentiment analysis, topic classification and NLI tasks, mining approach outperforms zero-shot prompting method when using the same verbalizers and comparable patterns. It can partly explain the performance of prompting method using the fact that many task-relevant examples are seen during the training which can be explicitly retrieved through simple regex mining pattern.</p> </li> <li> <h6 id="skill-structured-knowledge-infusion-for-large-language-models-moiseev-et-al-naacl-2022"><a href="https://aclanthology.org/2022.naacl-main.113.pdf">SKILL: Structured Knowledge Infusion for Large Language Models</a> (Moiseev et al., NAACL 2022)</h6> <p>The paper introduces <b>SKILL</b> a simple way to inject knowledge from structured data, such as a KG, into a language model, that can benefit knowledge-retrieval-based downstream tasks. <b>SKILL</b> continue to pretrain LLM directly on structured data (e.g. triples in KG) with salient-term masking without synthesizing them into equivalent natural sentences (e.g. KELM) as they found that the two approaches are competitive with each other.</p> <p><b>SKILL</b> demonstrates better performance than original LMs on Wikidata-related QA benchmarks as it is pre-trained on Wikidata triples. Most of the gains comes from the ability to memorize KG triples during the training. As a consequence, the model can perform very well on 1-hop questions that are supported by single triples, such as “When was Elon Musk born ?” corresponds to the triple <em>&lt;Elon Musk, date of birth, ?&gt;</em>. However, when it comes to answering multi-hop questions (e.g. “Who worked at the companies directed by Elon Musk ?” may correspond to two triples <em>&lt;Elon Musk, owner of, ?x&gt;</em> and <em>&lt;?y, employer, ?x&gt;</em> ) which requires not only the memorizing ability but also the reasoning ability, <b>SKILL</b> performs just slightly better than original LMs. <b> The author points out one limitation of SKILL is that the training relies on a random set of independent triples, lacking of topological structure exploitation of a KG describing how triples are connected. Addressing this issue can improve the multi-hop QA tasks</b>.</p> </li> </ul> <p><b>2021</b></p> <ul> <li> <h6 id="leveraging-passage-retrieval-with-generative-models-for-open-domain-question-answering-izacard-et-al-eacl-2021"><a href="https://arxiv.org/abs/2007.01282">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</a> (Izacard et al., EACL 2021)</h6> <p><b>Retrieved evidence fusion in decoder (<em>Fusion-in-Decoder</em>).</b></p> <p><img src="/assets/img/cheatsheet/fid.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>To address the Open Domain Question Answering, firstly, an independent knowledge retriever is leveraged to retrieve supporting passages for the input question, then, a seq2seq model (T5) takes as input the combination of the input question and supporting passages to produce the answer. Specifically, each retrieved passage concatenated with the input question is independently encoded by the encoder and their representations are merged together before sending to the decoder, in this way, the decoder can attend over the whole set of retrieved potential evidences and rely on them to generate the answer. There are two advantages of this <em>fusion-in-decoder</em> method:</p> <ul> <li>Avoid encoding all retrieved passages and the input question in one place which is very costly due to significant lengths of the passages.</li> <li>Thanks to the first point, we can efficiently increase the number of retrieved support passages, leading to the higher accuracy in the answer.</li> </ul> </li> </ul> <p><b>2020</b></p> <ul> <li> <h6 id="realm-retrieval-augmented-language-model-pre-training-guu-et-al-icml-2020"><a href="https://arxiv.org/pdf/2002.08909.pdf">REALM: Retrieval-Augmented Language Model Pre-Training</a> (Guu et al., ICML 2020)</h6> <p><b>Knowledge Retriever jointly pre-trained with LM.</b></p> <p>BERT-style LM is pre-trained to denoise a corrupted input sentence \(\hat{x}\) by predicting the masked tokens [MASK] in \(\hat{x}: p(x| \hat{x})\). For example, given “The [MASK] is the currency of the United Kingdom” as \(\hat{x}\), then the answer for [MASK] is “pound”. REALM makes the prediction more interpretable by first retrieving possibly helpful documents \(z\) (\(z\) plays as latent variable) from a knowledge source \(Z\) and using them (as evidences) to support the prediction of [MASK], as following:</p> \[p(x \mid \hat{x}) = \sum_{z \in Z}{ p (x | \hat{x}, z ) * p (z | \hat{x}) }\] <p>\(p (x \mid \hat{x}, z )\) helps to inform which documents \(z\) contribute the most to [MASK] tokens. The <b>knowledge retriever</b> \(p_{\theta}(z \mid \hat{x})\) and <b>knowledge-augmented encoder</b> \(p_{\phi}(x \mid \hat{x}, z )\) are modelled separately using two different BERT\(_{\theta}\) and BERT\(_{\phi}\). Document \(z\) is represented by its title and body. \(p_{\theta}(z \mid \hat{x})\) involves the cosine similarity between the sentence embedding produced by BERT\(_{\theta}\) of \(\hat{x}\) and \(z\). During the pre-training, the marginal \(p(x \mid \hat{x})\) requires a summation over all documents \(z\) in \(Z\) which is very costly. Also, as \({\theta}\) changes every training step, hence the embeddings <b>Emb(z)</b> of all documents \(z\) in \(Z\) need to be recalculated every step \(\rightarrow\) sound impossible. To deal with these issues, REALM proposes two training strategies</p> <ul> <li>\(p_{\theta}(z \mid \hat{x})\) is marginalized over only top-K documents \(z\) instead of all. Top-K relevant documents \(z\) w.r.t. input \(\hat{x}\) can be efficiently performed by Maximum Inner Product Search (MIPS) algorithm where the embeddings of \(z\)(s) are pre-computed and pre-indexed.</li> <li>The <b>Emb(z)</b> are freezed for an amount of time and are only re-calculated every several hundred update step.</li> </ul> <p><img src="/assets/img/cheatsheet/realm.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Regarding the training setting, the model is trained using masked-language modelling. They found that masking salient terms instead of masking random span could significantly improve the performance on downstream tasks.</p> </li> <li> <h6 id="generalization-through-memorization-nearest-neighbor-language-models-khandelwal-et-al-iclr-2020"><a href="https://arxiv.org/pdf/1911.00172.pdf">Generalization through Memorization: Nearest Neighbor Language Models</a> (Khandelwal et al., ICLR 2020):</h6> <p>The paper hypothesizes that the representation learning problem may be easier than the prediction problem. For example, two sentences <em>Dickens is the author of</em> and <em>Dickens wrote</em> will essentially have the same distribution over the next word, even if they do not know what that distribution is. Given a sequence of tokens \(x = (w_1,...,w_{t-1})\), \(k\) nearest neighbors \(\mathcal{N}\) of \(x\) is retrieved from a pre-built catalog \(\mathcal{C}\) by comparing the sentence embedding of each sequence in Eclidean space. Each nearest neighbor \(x_i\) of \(x\) has a next token \(y_i\): \((x_i, y_i) \in \mathcal{N}\). The distribution of the next token \(y\) of \(x\) can be estimated via a simple linear regression: \(p_{kNN} (y \mid x) = \sum_{(x_i, y_i) \in \mathcal{N}} softmax (\mathbb{1}_{y=y_i} exp (-d (\textsf{Emb}(x), \textsf{Emb}(x_i))))\).</p> <p>The LM distribution of a token \(y\) \(p_{LM} (y \mid x)\) given \(x\) is then updated by the nearest neighbor distribution \(p_{kNN} (y \mid x)\): \(p (y \mid x) = \lambda p_{kNN} (y \mid x) + (1-\lambda) p_{LM} (y \mid x)\).</p> <p><img src="/assets/img/cheatsheet/nearestlm.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Several advantages of nearest neighbor LM:</p> <ul> <li>No additional training required.</li> <li>Long-tail patterns can be explicitly memorized in the pre-built catalog \(\mathcal{C}\) instead of encoded implicitly in model parameters. New domain can be adapted to LM by creating a new catalog for the target domain dataset.</li> <li>\(k\) nearest neighbor search in the embedding space of word sequences can be efficiently done using FAISS index.</li> </ul> </li> </ul> <h5 id="2-information-extraction"><b>2. Information Extraction</b></h5> <p><b>2023</b></p> <ul> <li> <h6 id="how-to-unleash-the-power-of-large-language-models-for-few-shot-relation-extraction-xu-et-al-sustainlpacl-2023"><a href="https://aclanthology.org/2023.sustainlp-1.13.pdf">How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?</a> (Xu et al., SustaiNLP@ACL 2023)</h6> </li> <li> <h6 id="gpt-re-in-context-learning-for-relation-extraction-using-large-language-models-wan-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.02105.pdf">GPT-RE: In-context Learning for Relation Extraction using Large Language Models</a> (Wan et al., arxiv 2023)</h6> </li> <li> <h6 id="universal-information-extraction-as-unified-semantic-matching-lou-et-al-aaai-2023"><a href="https://arxiv.org/pdf/2301.03282.pdf">Universal Information Extraction as Unified Semantic Matching</a> (Lou et al., AAAI 2023)</h6> </li> <li> <h6 id="structgpt-a-general-framework-for-large-language-model-to-reason-over-structured-data-jiang-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.09645.pdf">StructGPT: A General Framework for Large Language Model to Reason over Structured Data</a> (Jiang et al., arxiv 2023)</h6> </li> <li> <h6 id="gpt4graph-can-large-language-models-understand-graph-structured-data-an-empirical-evaluation-and-benchmarking-guo-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.15066.pdf">GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking</a> (Guo et al., arxiv 2023)</h6> <p>The paper presents a graph understanding benchmark to evaluate the capability (0-shot and 1-shot) of LLM (i.e. InstructGPT3,5) in comprenhending graph data. The benchmarking tasks are classified into 2 categories: structure understanding and semantic understanding, as below:</p> <p><img src="/assets/img/cheatsheet/gpt4graph_2.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Several findings:</p> <ul> <li>Input design (e.g. order of graph and question, format explaination) has a significant impact.</li> <li>Role prompting is beneficial: explicitly ask the model to summarize the graph, then use it as new context, combine with initial prompt for the target task.</li> </ul> <p><img src="/assets/img/cheatsheet/gpt4graph_1.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p><br/></p> </li> <li> <h6 id="neurostructural-decoding-neural-text-generation-with-structural-constraints-bastan-et-al-acl-2023"><a href="https://aclanthology.org/2023.acl-long.528.pdf">NEUROSTRUCTURAL DECODING: Neural Text Generation with Structural Constraints</a> (Bastan et al., ACL 2023)</h6> <p><b>NeuroStructural Decoding</b> is a new beam-search based decoding scheme in generative LLMs that guides the model to follow given structural constraints when generate the output.</p> <p><img src="/assets/img/cheatsheet/neuro_structural.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Structural constraints considered in this work are classified into three types:</p> <ul> <li>Unary Constraint: constraints the role of a single word in generated text such as <em>D = (ball, obj)</em>: the word <em>ball</em> should appear as an object of some verb.</li> <li>Binary Constraint: constraints the relation between two words such as <em>D = (team, subj, run)</em>: the word <em>team</em> should be the subject of the word <em>run</em>.</li> <li>Triplet Constraint: a triplet, such as <em>D = (team, run, field)</em> should appear in the generated text.</li> </ul> <p>At each decoding step, the score of a token is modified as: \(P_{\theta} (y | x) - \lambda \sum_{i=1}^{k} ( 1 - C_i)\) where the second term pernalizes the token that does not satisfies a clause \(i\) consisting of disjunctive constraints (\(C_i = 0\)), if sastified, \(C_i = 1\).</p> <p>To effectively search for relevant tokens in a beam at each decoding step, <b>NeuroStructural Decoding</b> tracks the states of each clause, whereby prune the paths that irreversibly violate a constraint or group paths that share irreversiby satisfied clauses. To evaluate whether partially generated text holds a syntactic constraint, it needs a syntactic parser that is capable of parsing incomplete sentence. To this end, author continues to train a dependency parser on incomplete sentence to improve its performance on such pattern.</p> </li> <li> <h6 id="retrieval-enhanced-generative-model-for-large-scale-knowledge-graph-completion-yu-et-al-sigir-2023"><a href="https://dl.acm.org/doi/pdf/10.1145/3539618.3592052">Retrieval-Enhanced Generative Model for Large-Scale Knowledge Graph Completion</a> (Yu et al., SIGIR 2023)</h6> <p><b>ReSKGC</b> is a retrieval-augmented generative model for KG completion. It consists of two steps:</p> <ul> <li>retrieval: the KG’s triplets and input tripet with to-be-predicted object (s, p, ?) is linearized into text (see figure below). Then, the input is used to retrieve $k$ relevant KG’s linearized triplets using non-parametric retriever BM25.</li> <li>fusion-in-decoder (FiD): a FiD is employed to encode efficiently the concatenation of retrieved passages and the input, whereby generate the missing object in the triplet (s, p, ?). <b>ReSKGC</b> attains the new sota performance on Wikidata5M and WikiKG90Mv2 benchmarks.</li> </ul> <p><img src="/assets/img/cheatsheet/reskgc.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="knowledge-base-completion-for-long-tail-entities-chen-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2306.17472.pdf">Knowledge Base Completion for Long-Tail Entities</a> (Chen et al., arxiv 2023)</h6> <p><b>MALT</b> is a dataset for KB completion that focuses on long-tail entities and is extracted from Wikidata. Long-tail entities are defined as being involved in less than 14 triples in KG. The dataset contains 3 entity types (i.e. business, musicComposition and human) and 8 associated predicates such as foundedBy, placeOfBirth.</p> <p><img src="/assets/img/cheatsheet/malt.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="instructuie-multi-task-instruction-tuning-for-unified-information-extraction-wang-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2304.08085.pdf">InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction</a> (Wang et al., arxiv 2023)</h6> <p><img src="/assets/img/cheatsheet/instructuie.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p><b>InstructUIE</b> gathers 32 public datasets covering three IE tasks: NER, RE, EE and transform each sample in each dataset into text-2-text-with-instruction format (see figure above). They then fine-tune a FLAN-T5 11B on those datasets. <b>InstructUIE</b> demonstrates better performance than <a href="https://huynhvp.github.io/blog/2023/nlp-cheatsheet/#unified-structure-generation-for-universal-information-extraction-lu-et-al-acl-2022">UIE</a> and USM on in-domain test set and than GPT-3-davinci or ChatGPT (for RE task) on out-of-domain test set.</p> </li> <li> <h6 id="unifying-molecular-and-textual-representations-via-multi-task-language-modelling-christofidellis-et-al-icml-2023"><a href="https://arxiv.org/pdf/2301.12586.pdf">Unifying Molecular and Textual Representations via Multi-task Language Modelling</a> (Christofidellis et al., ICML 2023)</h6> </li> <li> <h6 id="triggering-multi-hop-reasoning-for-question-answering-in-language-models-using-soft-prompts-and-random-walks-misra-et-al-findings-acl-2023"><a href="https://arxiv.org/pdf/2306.04009.pdf">Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks</a> (Misra et al., Findings ACL 2023)</h6> <p>LM can perform well with KG-based one-hop Q/A thanks to its ability to memorize injected triples. However, for two-hop Q/A, the model finds difficult to combine separate triples that supports the question to arrive at the correct answer. This paper improves the two-hop Q/A by exposing the model to two-hop predicate paths explicitly. This is done through several tuning based on T5, resulting <b>KNowledge-Integrated T5 (KNIT5)</b>:</p> <ul> <li>Knowledge Integration: given a triple (s,p,o), model is tuned to predict o given s and p.</li> <li>Two-hop Knowledge Integration: given two triples (s1, p1, o1) and (o1, p2, o2), model is prefix tuned to predict o2 given s1, p1, o1, p2.</li> <li>Either of the two prefix tuning methods below is considered <ul> <li><b>Parse-the-Hop</b>: consists of two steps: (i) given input question, model is tuned to parse the question into a two-hop path (s1, p1, o1, p2); (ii) model then predict the answer o2 given (s1, p1, o1, p2).</li> <li><b>MixHop</b>: jointly tune the two steps.</li> </ul> </li> </ul> <p><img src="/assets/img/cheatsheet/multi_hop.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>The above training paradigm shows to improve substantially the 2-hop capabilities of LMs, but mostly in large LMs. (e.g. T5-XXL).</p> </li> <li> <h6 id="flexible-grammar-based-constrained-decoding-for-language-models-geng-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.13971.pdf">Flexible Grammar-Based Constrained Decoding for Language Models</a> (Geng et al., arxiv 2023)</h6> </li> <li> <h6 id="methods-for-measuring-updating-and-visualizing-factual-beliefs-in-language-models-hase-et-al-eacl-2023"><a href="https://aclanthology.org/2023.eacl-main.199.pdf">Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models</a> (Hase et al., EACL 2023)</h6> </li> <li> <h6 id="can-lms-learn-new-entities-from-descriptions-challenges-in-propagating-injected-knowledge-onoe-et-al-acl-2023"><a href="https://arxiv.org/pdf/2305.01651.pdf">Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge</a> (Onoe et al., ACL 2023)</h6> <p>This work investigates whether LM can add a new entity through entity’s description, propagate this information and performance inference on the new entity.</p> <p><img src="/assets/img/cheatsheet/new_entity.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Different from works on injecting facts into LM, where the inference is usually based on the paraphrasing version of injected facts (e.g. upper part of the image above), this work involves higher level of inference complexity, which requires the model learn/propagate new entities from their definitions, and evaluate diverse facts around the new entities. A few examples can be found in the table below:</p> <p><img src="/assets/img/cheatsheet/new_entity_2.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Evaluation metrics for the inference on injected entities:</p> <ul> <li>Update success: accuracy or perplexity (i.e updated model should have lower perplexity on facts related to new entities)</li> <li>Specificity: the entity injection should not impact the existing facts that do not relate to new entities. Or, the perplexity on those facts should not be increased.</li> </ul> <p><b>Findings:</b> (i) full fine-tuning approach can work effectively on controlled benchmark (LM does not predict an answer for a probe, but instead choose an answer from a set of candidates) , but it comes at the cost of increasing the specificity.; (ii) finetuning for longer does not necessarily propagate the entity’s information into the model; (iii) for more realistic benchmark which require higher level of reasoning/inference, none of model editting techniques improve the update success while keeping specificity stable. Furthermore, author found that such techniques only work when there is lexical overlap between the target inference and the definition of injected entity (e.g. answer span contained in the definition).</p> </li> <li> <h6 id="demonstratesearchpredict-composing-retrieval-and-language-models-for-knowledge-intensive-nlp-khattab-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2212.14024.pdf">DEMONSTRATE–SEARCH–PREDICT: Composing retrieval and language models for knowledge-intensive NLP</a> (Khattab et al., arxiv 2023)</h6> </li> <li> <h6 id="codeie-large-code-generation-models-are-better-few-shot-information-extractors-li-et-al-acl-2023"><a href="https://arxiv.org/pdf/2305.05711.pdf">CODEIE: Large Code Generation Models are Better Few-Shot Information Extractors</a> (Li et al., ACL 2023)</h6> <p>Code language model (i.e. model trained on code, among other things) has been discovered that it has better capability to deal with the generation of structured output (e.g. graph, rdf triplet, dictionary…), as code has also structure. Natural-text LM needs to serialize the structured output as plain text, which is very different from what it saw during the pretraining, making the inference difficult.</p> <p><img src="/assets/img/cheatsheet/codeie.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>This paper employs a code-LLM (i.e. OpenAI’s Codex) to perform few-shot information extraction (NER and RE). The prompting templates for two tasks are described in the figure above. The results show that:</p> <ul> <li>Code-style prompt is better than plain-text prompt</li> <li>CodeLM has better few-shot performance (even with plain-text prompt) on information extraction tasks than text-LM.</li> <li>Code-style prompt with CodeLM yields lower structural error rate. In other words, it can generate the output with correct format. <br/><br/></li> </ul> </li> <li> <h6 id="evaluating-language-models-for-knowledge-base-completion-veseli-et-al-eswc-2023"><a href="https://arxiv.org/pdf/2303.11082.pdf">Evaluating Language Models for Knowledge Base Completion</a> (Veseli et al., ESWC 2023)</h6> <p>Previous benchmarks for LM-based Knowledge Base Completion tends to be biased toward popular entities, leading to an overestimate of the completion performance of LM. This paper proposes WD-Know, a new benchmark to address this issue. It relies on Wikidata to extract facts via randomly and equally sampling entities. The new benchmark reveals that the completion accuracy of LM is not equal across relations. While LM achieves high precision and good generalization for language-related and socio-demographic relations (e.g. citizenOf, headquarteredIn), non-socio-demographic relations (e.g. producedBy) may require the fact to be present explicitly (retrieve rather than generalize).</p> </li> <li> <h6 id="exploiting-asymmetry-for-synthetic-training-data-generation-synthie-and-the-case-of-information-extraction-josifoski-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2303.04132.pdf">Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction</a> (Josifoski et al., arxiv 2023)</h6> <p>Author points out that the lack of a large, balanced, high quality training dataset has been a important obstacle for the success of close Information Extraction (cIE). Indeed, previous datasets exposes several problems: (i) skewness: rare relations/subjects/object appear only a few times. Most models perform poorly on these entities; (ii) noisy: target output does not always contain all the facts conveyed in the input. For these reasons, author proposes to generate a synthetic balanced dataset with the help of LLM. Specifically, LLM is asked to generate text describing a knowledge subgraph fed as input.</p> <p><img src="/assets/img/cheatsheet/synthIE.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>They then train a FLAN-T5 on this synthetic dataset, yielding <b>SynthIE</b>. Experiments show <b>SynthIE</b> performs much better than <b>GenIE</b> on test synthetic dataset, but much worse than GenIE on REBEL’s test set. They argue REBEL’s test set is a poor approximation of performance.</p> </li> <li> <h6 id="large-language-model-is-not-a-good-few-shot-information-extractor-but-a-good-reranker-for-hard-samples-ma-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2303.08559.pdf">Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!</a> (Ma et al., arxiv 2023)</h6> <p>Through an exhaustive evaluation on multiple information extraction tasks (NER, RE, ED), the paper argues that LLM is not an effective few-shot information extractor and still lags behind well-finetuned small LMs, given enough training data. The rationales are:</p> <ul> <li>Limited number of demonstrations in in-context learning: challenging when dealing with learning problems that involves many labels.</li> <li>Author speculates that ICL has difficulty with structured prediction.</li> </ul> <p>In addition, author also found that LLM can work well on samples that seem to be hard for small LMs. This motivates them to propose a hybrid model combining both small LM and LLM. Concretely, samples for which small LM yields small scores are passed to LLM to re-evaluate.</p> </li> <li> <h6 id="understanding-fine-tuning-for-factual-knowledge-extraction-from-language-models-kazemi-et-al-submitted-to-jmlr"><a href="https://arxiv.org/pdf/2301.11293.pdf">Understanding Fine-tuning for Factual Knowledge Extraction from Language Models</a> (Kazemi et al., submitted to JMLR)</h6> <p>This study dives more deeply into the application of using language models to construct a knowledge graph. By investigating the behavior of LMs finetuned for factual knowledge extraction, the author argues that the finetuning process results both positive and negative impacts, depending on the frequency mismatch of entity appearance between the train data and the test data. They relates this issue to the well-known Out-of-distribution generalization in machine learning:</p> <ul> <li> <p>Positive impact: if the train and test dataset have similar entity frequency (low mismatch), the fine-tuning yields improvements for knowledge extraction.</p> </li> <li> <p>Negative impact: otherwise (high mismatch), the fine-tuning is no better than zero-shot or few-shot learning due to the appearance of <em>forgetting</em>-related effects: <b>Frequency Shock</b> and <b>Range Shift</b> that may sometimes outweigh positive impact.</p> </li> </ul> <p>Examples of <b>Frequency Shock</b> are shown below: <img src="/assets/img/cheatsheet/frequency_short.png" alt="" style="width: 45%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Even though both “Moscow” and “Baku” are observed an equal number of times (5) during the fine-tuning, “Baku” is less popular then “Moscow” during the pre-training of the LM \(\rightarrow\) the fine-tuned model receives a frequency shock (i.e. “Boku” shift from “unpopular” in pre-training to “as-popular-as” “Moscow” in fine-tuning), making it over-predict “Baku” (rare entity) in the test dataset.</p> <p><b>Range Shift</b>: finetuning makes the model tend to predict entities that are seen as answer during the fine-tuning (cold-start problem)</p> <p>To alleviate the negative impact of finetuning, the paper propose two solutions: (i) ensemble models (fine-tuning + k-shot) as k-show is better than fine-tuning for <em>high mismatch</em> scenario; (ii) mixture training (similar to solution to catastrophic forgetting): jointly fine-tune the model with two objectives: knowledge extraction task and LM objective (e.g. MLM).</p> </li> <li> <h6 id="crawling-the-internal-knowledge-base-of-language-models-cohen-et-al-tbd"><a href="https://arxiv.org/pdf/2301.12810.pdf">Crawling The Internal Knowledge-Base of Language Models</a> (Cohen et al., TBD)</h6> <p>The paper presents <b>LMCRAWL</b>, a pipeline for crawling a subgraph centering around a seed entity, from LM using in-context learning with GPT-3 model.</p> <p>The crawling processing is decomposed into subtasks:</p> <ul> <li>Relation Generation: generate a set of relations given subject entity. They leverage Wikidata to generate in-context examples.</li> <li>Relation Paraphrasing: generate different surface forms for a relation.</li> <li>Subject Paraphrasing: generate different surface forms for an entity.</li> <li>Object Generation: given a subject entity and a relation, generate a list of object entities. Only objects that are generated by at least two variants of the relation (via relation paraphrasing) are accepted.</li> <li>Learning to say “Don’t Know” instead of giving an erroneous fact: they simply include “don’t know” in-context examples to make the model aware of answering “don’t know” if needed.</li> </ul> <p>Examples of in-context learning are shown below: <img src="/assets/img/cheatsheet/LMCRAWL.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> </ul> <p><b>2022</b></p> <ul> <li> <h6 id="greaselm-graph-reasoning-enhanced-language-models-for-question-answering-zhang-et-al-iclr-2022"><a href="https://openreview.net/pdf?id=41e9o6cQPj">GREASELM: Graph Reasoning Enhanced Language Models for Question Answering</a> (Zhang et al., ICLR 2022)</h6> <p>The paper presents <b>GREASELM</b>, a Graph Reasoning Enhanced LM for improving multiple choice QA. Differing from previous works, <b>GREASELM</b> fuses encoded representations of LM (used to encode QA context) and GNN (used to encode the KG that contains entities appearing in QA context) across <b>multiple network layers</b>. The information propagates from LM to GNN, and vice versa via two proxies: <em>interaction token</em> \(w_{int}\) appended to QA context and <em>interaction node</em> \(e_{int}\) appended to entity graph.</p> <p><img src="/assets/img/cheatsheet/greaselm.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p><b>GREASELM</b> consists of three components stacked vertically:</p> <ul> <li>LM representation: an uni-modal encoder of N layers encodes the QA context and the prefix <em>interaction token</em> \(w_{int}\).</li> <li>Graph representation and Cross-modal Fuser of M layers: <ul> <li>An entity linker is employed to extract KG entities from QA context from which a small KG \(\mathcal{G}\) is constructuted.</li> <li>The embeddings of entity nodes and proxy <em>interactive node</em> \(e_{int}\) are calculated by graph attention network.</li> <li>LM leverages the reasoning skills of GNN by fusing, at every layer, the representations of tokens and nodes through two proxies \(w_{int}\) and \(e_{int}\):</li> </ul> \[[\textbf{h}_{int}^{(l)},\textbf{e}_{int}^{(l)}] = MInt([\hat{\textbf{h}}_{int}^{(l)},\hat{\textbf{e}}_{int}^{(l)}])\] <p>where (\(\hat{\textbf{h}}_{int}^{(l)},\hat{\textbf{e}}_{int}^{(l)}\)) and \((\textbf{h}_{int}^{(l)},\textbf{e}_{int}^{(l)})\) are embeddings of (\(w_{int}\), \(e_{int}\)) before and after fusion. \(Mint\) is a two-layer MLP.</p> </li> <li>For multi-choice QA, the score of an answer is computed by another MLP taking in \((\textbf{h}_{int}^{(N+M)},\textbf{e}_{int}^{(M)}, g)\) where \(g\) is attention-weighted embedding of graph entities.</li> </ul> <p><b>GREASELM</b> demonstrates better performance than previous KG-enhanced LM for CommonsenseQA, OpenbookQA, MedQA-USMLE.</p> <p>Ablation shows :</p> <ul> <li><b>GREASELM</b>’s improvement questions that require complex reasoning: negation, hedging term’s presence.</li> <li>Attention visualization makes sense. <img src="/assets/img/cheatsheet/greaselm_abla.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></li> </ul> <p>(source: copied from the paper)</p> </li> <li> <h6 id="entity-cloze-by-date-what-lms-know-about-unseen-entities-onoe-et-al-finding-naacl-2022"><a href="https://www.cs.utexas.edu/~yasumasa/papers/ecbd.pdf">Entity Cloze By Date: What LMs Know About Unseen Entities</a> (Onoe et al., Finding NAACL 2022)</h6> <p>The paper introduces ECBD dataset, containing new entities that are did not exist when the LMs were pretrained, together with cloze sentences in which the entity mentions are found.</p> <p><img src="/assets/img/cheatsheet/ecbd.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>The masked spans in cloze sentences are chosen that likely relates to the new entities. For each cloze sentence (ORIGINAL), three variants are generated:</p> <ul> <li>NO ENT: replaces the entity mention span by mention of another entity that is seen during pre-training.</li> <li>RANDOM DEFINITION: prepend the definition of a random entity to ORIGINAL.</li> <li>DEFINITION: prepend the definition of the gold entity to ORIGINAL.</li> </ul> <p>By measuring the perplexity on 4 categories of cloze sentence, author suggest that injecting additional information (i.e. entity definition) can help the LM guess better (perplexity order: DEFINITION &lt; ORIGINAL~RANDOM DEFINITION &lt; NO ENT) the masked spans related to new entities.</p> </li> <li> <h6 id="large-language-models-struggle-to-learn-long-tail-knowledge-kandpal-et-al-arxiv-2022"><a href="https://arxiv.org/pdf/2211.08411.pdf">Large Language Models Struggle to Learn Long-Tail Knowledge</a> (Kandpal et al., arxiv 2022)</h6> <p>The paper experimentally shows that the performance of LM on entity-centric knowledge-intensive task (e.g. question-answering) depends strongly in the co-occurrence of {question entity, anwser entity} in the training documents. Specifically, questions related to entities of low frequency result significant low accuracy. They argue this is not due to the questions being “harder”, which causes the drop in the performance, as human performs very well for those questions.</p> <p><img src="/assets/img/cheatsheet/longtail.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="unified-structure-generation-for-universal-information-extraction-lu-et-al-acl-2022"><a href="https://arxiv.org/pdf/2203.12277.pdf">Unified Structure Generation for Universal Information Extraction</a> (Lu et al., ACL 2022)</h6> <p><b>Universal Information Extraction (UIE)</b> is a unified text-to-structure framework for Information Extraction tasks. It models various IE tasks (NER, EL, RL, etc) within a single T5-based model, allowing different tasks to be jointly learned, to share and collaborate. To this end, <b>UIE</b> introduces two univeral templates for linearizing the heterogeneous input and the heterogeneous output and pre-training scheme to endow the model with common IE abilities (i.e. mapping text to structure, decoding structure).</p> <p>In more details:</p> <ul> <li>SSI (Structural Schema Instructor) template to represent the heterogeneous input: e.g. \(\textsf{[spot] person [asso] work for [text] Steve became CEO of Apple in 1997}\) where special tokens \(\textsf{[spot]}\) and \(\textsf{[asso]}\) indicate what to extract in \(\textsf{[text]}\) (\(\textsf{[spot]}\): person entity, \(\textsf{[asso]}\): its attribute).</li> <li>SEL (Structured Extraction Language) template to represent the heterogeneous output such as \(\textsf{((entity: (attribute: )))}\): e.g. \(\textsf{((person: Steve (work for: Apple)))}\) for the above input.</li> <li>Pre-training paradigm: <b>UIE</b> is jointly trained with three objectives: (1) text-to-structure with Wikipedia-Wikidata aligned (text, KB triplets) pairs. (2) UIE decoder pretraining to autoregressively predict components (predicate, object) in KB triplets. (3) T5’s training objective: span corruption based MLM.</li> <li> <p><b>Rejection Machanism</b>: adding <b>NULL</b> to the training data to help the model learn to reject misleading generation.</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">  Example

</span><span class="gp">  Encoder: &lt;spot&gt;</span><span class="w"> </span>person ... &lt;spot&gt; facility &lt;asso&gt; ... &lt;text&gt; Steve became CEO of Apple <span class="k">in </span>1997.
<span class="go">  Decoder: ((person: Steve (work for: Apple)) (facility: [NULL]) ...
</span></code></pre></div> </div> </li> </ul> <p><img src="/assets/img/cheatsheet/uie.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="genie-generative-information-extraction-josifoski-et-al-naacl-2022"><a href="https://aclanthology.org/2022.naacl-main.342.pdf">GenIE: Generative Information Extraction</a> (Josifoski et al., NAACL 2022)</h6> <p>Close Information Extraction (cIE) typically aims at extracting an exhaustive set of relational triplets \((subject, relation, object)\) from given text where \(subject/object\) entity and \(relation\) are constrained to come from a predefined knowledge base. Traditional cIE pipeline encompasses multiple independent sub-tasks (NER, NED, RE) which suffers from the error accumulation. <b>GenIE</b> is an end-to-end autoregressive cIE system that casts the triplet extraction as text-2text problem in which the decoder generates entities and relations token-by-token in an autoregressive fashion. They introduce special tokens &lt;sub&gt;, &lt;rel&gt;, &lt;obj&gt;, &lt;end_of_triplet&gt; to linearize the generated output. To assure that generated tokens refer to valid entity and relation, <b>GenIE</b> employs constrained beam search to guide the decoding following prefix tries built on the entity set and the relation set of the knowledge base. This makes the beam search effective for large million of entities.</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">  Example

  Encoder: John Smith acts in the movie Wichia
</span><span class="gp">  Decoder: &lt;sub&gt;</span><span class="w"> </span>Wichia <span class="o">(</span>1995 film<span class="o">)</span> &lt;rel&gt; cast member &lt;obj&gt; John Smith <span class="o">(</span>actor<span class="o">)</span> &lt;end_of_triple&gt; 
<span class="gp">                 &lt;sub&gt;</span><span class="w"> </span>Wichia <span class="o">(</span>1995 film<span class="o">)</span> &lt;rel&gt; instance of &lt;obj&gt; film &lt;end_of_triple&gt;
</code></pre></div> </div> <p><b>GenIE</b> enforces the order of generated triplets in the way that triples for which the subject entity appears earlier in the text will be generated first.</p> <p><b>GenIE</b> can be extended to the generation of literal object \(\rightarrow\) similar to open Information Extraction where the object does not need to be aligned with a KB.</p> </li> <li> <h6 id="eider-empowering-document-level-relation-extraction-with-efficient-evidence-extraction-and-inference-stage-fusion-xie-et-al-acl-findings-2022-1"><a href="https://arxiv.org/pdf/2106.08657.pdf">EIDER: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion</a> (Xie et al., ACL Findings 2022)</h6> <p><b>EIDER: Extracted Evidence Empowered Relation Extraction</b></p> <p><img src="/assets/img/cheatsheet/eider.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Typical document-level relation extraction models rely on the whole document to infer the relation of an entity pair in the document. On the one hand, a minimal set of sentences (i.e. evidences) in the documents is enough for human to annotate the relation, taking the whole document as input may add noise and ambiguity to the model. On the other hand, there is no way to extract such minimal set perfectly, leading to missing important information. <b>EIDER</b> alleviates both aspect by introducing:</p> <ul> <li>Joint training of relation extraction end evidence sentence extraction: a base encoder is employed to learn the representation of the relation from the counterparts of the head entity, tail entity and the whole document \(p(r \mid e_h, e_t, c_{h,t})\), as well as to learn the representation of each evidence sentence \(s_n\) given the head and tail entity \(p (s_n \mid e_h, e_t)\). For the training, evidence sentences for each entity pair in a document can be either manually provided, or extracted using simple heuristics (e.g. a sentence containing both head and tail entities is considered as an evidence for this entity pair).</li> <li>Fusion of evidence in Inference: the score of each candidate relation is given by two inferences: one with the prediction from the whole documents, one with the prediction from the set of extracted evidence sentences (a subset of original document). \s</li> </ul> </li> <li> <h6 id="knowprompt-knowledge-aware-prompt-tuning-with-synergistic-optimization-for-relation-extraction-chen-et-al-the-webconf-2022"><a href="https://arxiv.org/pdf/2104.07650.pdf">KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction</a> (Chen et al., The WebConf 2022)</h6> <p><b>KnowPrompt: prompting with knowledge constraint </b></p> <p><img src="/assets/img/cheatsheet/knowprompt.png" alt="" style="width: 70%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>KnowPrompt relieves the cumbersome prompt engineering by representing the prompt template and prompt verbalizer by learnable virtual words. Specifically, given a prompt: \(\textsf{[CLS] It solds [E1] ALICO [/E1] to [E2] MetLife Inc [/E2] for \$162 billion. [SEP] [sub] ALICO [sub] [MASK] [obj] Metlife Inc [obj]. [SEP] }\) where the first sentence is the context in which foreknow sentinel tokens \(\textsf{[E1], [E2]}\) indicates entities whose relation will be discovered in the second sentence. Three tokens \(\textsf{[sub], [MASK], [obj]}\) are considered as virtual words representing the subject entity type, the relation, and the object entity type respectively. The possible relation \(r\) between \(\textsf{E1}\) and \(\textsf{E2}\) is computed from the probability distribution at \(\textsf{[MASK]}\) token.</p> <p>To guide \(\textsf{[sub], [obj]}\) to represent meaningfully the associated entity \(\textsf{E1, E2}\) as well as to encode the structural constraint between them and the relation, KnowPrompt:</p> <ul> <li>Instead of random initialization, the embeddings of \(\textsf{[sub], [MASK], [obj]}\) are initialized with prior distribution (calculated by frequency statistics) of entity type’s word-embedding and relation’s word embedding.</li> <li>Incorporate structural knowledge constraint: apart from LM loss, inspired by knowledge graph embedding, KnowPrompt interprets \(\textsf{[MASK]}\) as a translation from \(\textsf{[sub]}\) to \(\textsf{[obj]}\) (similar to TransE), leading to the minimization of the Euclidean distance in the embedding space: \(d([sub], [obj]) = \mid \mid [sub] + [MASK] - [obj] \mid \mid_2\)</li> </ul> <p><br/></p> </li> <li> <h6 id="rewire-then-probe-a-contrastive-recipe-for-probing-biomedical-knowledge-of-pre-trained-language-models-meng-et-al-acl-2022"><a href="https://arxiv.org/pdf/2110.08173.pdf">Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models</a> (Meng et al., ACL 2022)</h6> <p><b>Contrastive-Probe for Knowledge probing from LM.</b></p> <p>Knowledge probing approaches based on mask prediction or text generation have two typical drawbacks:</p> <ul> <li> <p>Multi-token span prediction: the mask prediction approaches use the MLM head to fill in a single mask token in a cloze-style query \(\rightarrow\) if an answer entity names that contain multi-token span, the query needs to be padded with the same amount of [MASK] token.</p> </li> <li> <p>The answer may not be a valid identifier of an entity: the mask prediction or text generation approaches rely on the vocabulary to generate the answer in an unconstrained way \(\rightarrow\) the generated texts may not exist in the answer space. Furthermore, different LMs can have different vocabularies, leading to the vocabulary bias.</p> </li> </ul> <p>The paper introduces <b>Contrastive-Probe</b> to address two above issues by avoiding using the LM head for mask prediction or text generation. Similarly to sentence embedding approaches, <b>Contrastive-Probe</b> employs the LM to encode the prompt query \(q\)(e.g. “Elvitegravir may prevent [Mask]”, [Mask] can represent multiple tokens) into the embedding \(e_q\) and encode each answer (e.g. “Epistaxis”) in the complete answer space into another embeddings \(e^i_s, i=1..N\) where \(N\) is the size of answer space. The K-nearest neighbors \(e^k_s, k=1..K\) of \(e_q\) in the embedding space are considered as the answer of \(q\). Self-supervised contrastive learning is used to rewire the pretrained LM to this answer-retrieval task. Specifically, with infoNCE objective loss, the PLM is fine-tuned on {query, answer} pairs in order for the {query, correct answer} pairs (positive samples) stay close to each other and {query, other answer in the same batch} pairs (negative samples) are pulled far apart.</p> <p>Testing on bio domain, <b>Contrastive-Probe</b> achieved several following results:</p> <ul> <li><b>Contrastive-Probe</b> outperforms other probing baselines (mask prediction, text generation) ) regardless of the underlying PLM on MedLAMA benchmark for Bio domain.</li> <li>It is effective at predict long answer (aka. multi-token span)</li> <li>In phase with previous observation, no configuration fits all relations. Different relation require different underlying LM, different depth of tuning layer for the best performance.</li> <li>It is pretty stable in performance where training with different dataset results small deviation and similar trend.</li> </ul> </li> </ul> <p> </p> <ul> <li> <h6 id="do-pre-trained-models-benefit-knowledge-graph-completion-a-reliable-evaluation-and-a-reasonable-approach-lv-et-al-acl-findings-2022"><a href="https://aclanthology.org/2022.findings-acl.282.pdf">Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach</a> (Lv et al., ACL-Findings 2022)</h6> <p>The paper demonstrates that PLM-based KGC models are still left quite behind the SOTA KGC models (e.g. KGE models) because the evaluation benchmark is conducted under the closed-world assumption (CWA) where any knowledge that does not exist in a given KG is said to be incorrect. Indeed, PLM is known to implicitly contain more open knowledge unseen in a KG. By manually verify the veracity of Top-1 prediction of KGC models, they show that PLM-based models outperforms SOTA KGE-based models for the link prediction and the triple classification tasks.</p> <p>Likewise many other models, this work also make use of prompting method to elicit the knowledge from PLM. A hard prompting template is manually designed for a relation to represent the semantics of the associated triples. For example, relation <em>&lt;X, member of sport teams, Y&gt;</em> has the template <em>X plays for Y</em>. To further improve the expressivity of triple prompts, two other kinds of prompts are added into the triple prompt:</p> <ul> <li>Soft prompts (i.e. learnable sentinel tokens [SP]): play as separators to signal the position of template components and entity labels in the triple prompt. For example, the prompt <em>X plays for Y</em> after adding soft prompts becomes <em>[SP1] X [SP2] [SP3] plays for [SP4] [SP5] Y [SP6]</em>. Each relation has its own set of soft prompts \([SP]_i, i=1..6\) and they are all learnable via triple classification objective.</li> <li>Support prompts: entity definition and entity attribute are useful information that can help the KGC. Therefore, they are concatenated to the triple prompt through two templates: “[Entity]: [Entity_Definition]” and “The [Attribute] of [Entity] is [Value]”. As an entity has many attributes, only few attributes are randomly selected. The results reveal that the entity definition provides more gain than entity attributes.</li> </ul> <p>Additionally, their analysis conveys two messages: <b>(i)</b> by counting number of sentences in the training that contain both the head and the tail of a triple, it indicates that PLM-based KGC still outperforms KGE-based KGC on the triples with zero co-occurrence of {head, tail} in the training set \(\rightarrow\) they argue PLMs, apart from seeing many facts in the massive text, have the ability to reason the knowledge. <b>(i)</b> PLM-based KGC models are less sensitive to the size of training dataset where reducing the training data size decreases slightly the prediction accuracy.</p> </li> <li> <h6 id="simkgc-simple-contrastive-knowledge-graph-completion-with-pre-trained-language-models-wang-et-al-acl-2022"><a href="https://arxiv.org/pdf/2203.02167.pdf">SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models</a> (Wang et al. ACL 2022)</h6> <p><b>SimKGC: Promptless method for KGC based on sentence embedding</b></p> <p>To predict an entity \(e_i \in KG \; \mathcal{E}\) for a triple \(&lt;h, r, ?&gt;\), SimKGC employs a PLM-based bi-encoder architecture where two encoders do not share parameters. One encoder computes the relation-aware embedding \(e_{hr}\) for the head entity \(h\) from the concatenation of the descriptions of the head entity and the relation: “[header_description] [SEP] [relation_description]”. Another encoder is leveraged to compute the embedding of the description of the candidate tail entity \(e_t\). Candidate tail entities \(e^i_t\) are ranked according to the cosine similarity between its embedding and the relation-aware embedding of the head entity \(e_{hr}\). The bi-encoder is trained to learn useful representation for head entity and tail entity in the triple using contrastive learning.</p> <p>The paper argues that the reason why previous contrastive learning-based models are lag behind SOTA KGE-based models highly involves the ineffectiveness of training setting for contrastive learning where they use small negative sample size (\(\approx\) 1..5 due to computational complexity) and the margin loss. Indeed, by augmenting the number of negative sample per positive sample (e.g. 256) and changing the margin loss to InfoNCE loss, they obtain much better performance and outperform KGE-based models.</p> <p>For further improvement, in addition to in-batch negative, SimKGC also combine two other strategies for generating negative samples:</p> <ul> <li>Pre-batch Negatives: sample batches at training step \(t-1\), \(t-2\)… can be considered as negative samples for current training batch at step \(t\).</li> <li>Self-Negatives: triple \(&lt;h, r, h&gt;\) (tail entity is predicted as head entity) is seen as a hard negative sample for the triple \(&lt;h, r, ?&gt;\) \(\rightarrow\) this makes the model rely less on the spurious text matching/overlapping to make the prediction.</li> </ul> <p>Lastly, the work also stresses that predicting <em>one-to-many, many-to-one, many-to-many</em> relations is more difficult.</p> </li> <li> <h6 id="task-specific-pre-training-and-prompt-decomposition-for-knowledge-graph-population-with-language-models-li-et-al-lm-kbciswc-2022-challenge"><a href="https://lm-kbc.github.io/static/papers/paper_2.pdf">Task-specific Pre-training and Prompt Decomposition for Knowledge Graph Population with Language Models</a> (Li et al., LM-KBC@ISWC 2022 Challenge)</h6> <p>This work continues to pre-train BERT with task-specific data to make it familiar with the task. How ? triples <em>&lt;sub, rel, obj&gt;</em> are verbalized into a sentence using a prompt template of <em>rel</em>. As the task is object prediction, the object or surround words in the sentence are masked and the LM is asked to predict them. Large dataset is necessary for pre-training, hence, they leverage Wikidata for data augmentation where they generate KG triples that have same relations as provided training relations). However, they discover later that the accuracy does not clearly relate to data size but the property of relation (see below).</p> <ul> <li>Prompt generation: they curate a set of prompts for a relation both in manual and automatic way. In manual way, they explicitly append the type of the subject into the prompt, such as “The musician [SUBJ] plays [OBJ]” for relation “PersonInstrument”. In automatic way, they employ two methods from <a href="https://arxiv.org/pdf/1911.12543.pdf">How Can We Know What Language Models Know?</a>. However, in contrast to <a href="https://arxiv.org/pdf/1911.12543.pdf">How Can We Know What Language Models Know?</a>, this paper shows that an ensemble of automatically-generated prompts is not better than a single manual-curated one.</li> <li>Prompt decomposition: a relation can have diverse domain and diverse range. For example, considering the relation “StateSharesBorderState”, its domain can include “Andalusia”-is a autonomous community or “Hebei” - a province. To better distinguish the type of the subject and probe more relevant knowledge from LM, two prompts are performed: <ul> <li>ask for subject type: e.g. e “[SUBJ], as a place, is a [TYPE]”.</li> <li>inject the subject type into the prompt of the relation: e.g. “[SUBJ] [TYPE] shares border with [MASK] [TYPE]”.</li> </ul> </li> </ul> </li> </ul> <p><b>2021</b></p> <ul> <li> <h6 id="genre-autoregressive-entity-retrieval-de-cao-et-al-iclr-2021"><a href="https://arxiv.org/pdf/2010.00904.pdf">GENRE: Autoregressive Entity Retrieval</a> (De Cao et al., ICLR 2021).</h6> <p>Very interesting entity retriever that casts the entity linking problem as a text-to-text problem and employs a seq2seq model (i.e. BART) to address it.</p> <p>Example:</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">  Encoder: In 1503, Leonardo began painting the Mona Lisa
  Decoder: In 1503, [Leonardo](Leonardo da Vinci) began painting the [Mona Lisa](Mona Lisa)

  where [X](Y) : X is the mention, and Y is the entity label (aka. entity identifier) that represents X.
</span></code></pre></div> </div> <p>Importantly, they perform the inference with constrained beam search to force the decoder to generate the valid entity identifier. Specifically, at a decoding step \(t\), the generation of the next token \(x_t\) is conditioned on previous ones \(x_1,..., x_{t-1}\) such that \(x_1,..., x_{t-1}, x_{t}\) is a valid n-gram of an entity identifier.</p> </li> <li> <h6 id="structured-prediction-as-translation-between-augmented-natural-languages-paolini-et-al-iclr-2021"><a href="https://arxiv.org/pdf/2101.05779.pdf">Structured Prediction as Translation Between Augmented Natural Languages</a> (Paolini et al., ICLR 2021)</h6> <p>Many knowledge extraction tasks such as NER, EL, Relation extraction, etc can be seen as structured prediction tasks where the output space consists of structured objects such as entities, relations.</p> <p><b>Translation between Augmented Natural Languages (TANL)</b> frames multiple prediction tasks as text-2-text problems and employs an unified architecture (e.g. BART, T5, etc) to solve all those tasks without task-specific designs. To this end, they propose informative templates (called <b>augmented language</b>) to encode structured input and decode output text into structured objects.</p> <p><img src="/assets/img/cheatsheet/taln.png" alt="" style="width: 70%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>One remarkable feature of TANL’s encode scheme is its ability to represent nested entities and multiple relations, as illustrated in example below:</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">  Example

  Encoder: Six days after starting acyclovir she exhibited signs of lithium toxicity.
  Decoder: Six days after starting [ acyclovir | drug ] she exhibited signs of [ [ lithium | drug ] toxicity |
                      disease | effect = acyclovir | effect = lithium ].
</span></code></pre></div> </div> <p>To ensure the consistency and the relevance of decoder’s output text, <b>TANL</b> follows several post-processing steps including:</p> <ul> <li>Dynamic Programming to align generate output text with input text. This helps to tackle imperfect generation by the decoder (e.g. generated word is a mispelling version of input word).</li> <li>Verify if predicted tail entity of predicted relation extactly matches an entity in the input.</li> </ul> <p><b>TANL</b> is shown to be beneficial in multi-task learning in which a single model is trained on multiple different datasets for different structured prediction tasks and in low-data regime (few shot finetuning).</p> </li> </ul> <p><b>2020</b></p> <ul> <li> <h6 id="how-can-we-know-what-language-models-know-jiang-et-al-tacl-2020"><a href="https://arxiv.org/pdf/1911.12543.pdf">How Can We Know What Language Models Know?</a> (Jiang et al., TACL 2020)</h6> <p>Knowledge in LM can be probed by asking the LM fill in the blanks of prompts such as “CR7 plays for ___”. This prompt-based method can only measure the lower bound of amount of knowledge contained in LM as there is no single prompt that works best for all instances of a relation (depending on what LM sees during its pre-training). To predict a missing object in a KB triple \(tpl\): <em>&lt;sub, rel, ?&gt;</em>, \(tpl\) is converted into a cloze-style prompt \(t_r\) that semantically expresses the relation <em>rel</em> and let the LM predict the object by filling the blank in \(t_r\). No prompt fits all, they propose two ways to generate a set of prompts for each relation \(r\):</p> <ul> <li><em>Mining-based generation</em>: <b>(i)</b> collecting sentences that contain both subject and object of a given relation \(r\), words between subject and object can be viewed as a representation of \(r\); <b>(ii)</b> if there is no meaningful middle words, sentence is analyzed syntactically, a prompt for \(r\) can be generated from the dependency tree.</li> <li><em>Paraphrasing-based generation</em>: starting from an initial prompt \(p\) for \(r\), \(p\) is paraphrased into other \(p'\) semantically similar. For example, if \(r\) == “<em>hasName</em>” has a prompt \(p\) == “<em>x is named as y</em>” then \(p'\) could be “<em>y is a name of x</em>”. Back-translation is a prevailing method for paraphrasing.</li> </ul> <p><br/> <b>Thoughts</b>:</p> <ul> <li>Blank in cloze-style prompt: how does LM know if ___ is single-token and multi-tokens (this work defaults single token).</li> <li>Domain and Range of a relation are ignored: a relation can appear under many different situations. A prompt is suitable for a situation but could turn out to be strange for other situations.</li> </ul> </li> </ul> <h5 id="3-prompting-methods-"><b>3. Prompting Methods </b></h5> <p><b>2023</b></p> <ul> <li> <h6 id="large-language-models-as-optimizers-yang-arxiv-2023"><a href="https://arxiv.org/pdf/2309.03409.pdf">Large Language Models as Optimizers</a> (Yang, arxiv 2023)</h6> <p>(a) <img src="/assets/img/cheatsheet/opro_1.png" alt="" style="width: 40%"/> (b) <img src="/assets/img/cheatsheet/opro_2.png" alt="" style="width: 40%"/></p> <p>(source: copied from the paper)</p> <p>Prompting engineering is a non-trivial task as many works show that semantically similar instructions may cause significantly different accuracies. <b>OPRO</b> is a gradient-free LLM-based optimizer for searching instructions that maximizes the accuracy of given task within a few training samples. The optimization process consists of two (distinct or identical) models: <em>scorer-LLM</em> and <em>optimizer-LLM</em> and performs iteratively 2 steps:</p> <ul> <li>Given <em>N</em> instructions generated at step <em>t</em>, the <em>scorer-LLM</em> run the inference over the training samples. Top-k instruction yeilding the best accuracies are retrained.</li> <li>At step <em>t+1</em>, top-k instructions, along with its scores are then appended into the context containing a few training samples as demonstrations and a meta-instruction such as “write the new instruction that is different from the old ones and has a score as high as possible<em>. The optimizer-LLM</em> is then prompted to generate <em>N</em> instruction via sampling.</li> </ul> <p>At each step, <em>optimizer-LLM</em> generates <em>N</em> instructions instead of a single instruction to improve the optimization stability as generated instruction can be very sensile to the demonstrations in the context.</p> <p>In addition, sorting <em>N</em> instructions in ascending order and appending <em>top-k</em> ones into the context perfoms better and converges faster.</p> </li> <li> <h6 id="how-far-can-camels-go-exploring-the-state-of-instruction-tuning-on-open-resources-wang-et-al-arxiv-2023--the-flan-collection-designing-data-and-methods-for-effective-instruction-tuning-longpre-et-al-icml-2023"><a href="https://arxiv.org/pdf/2306.04751.pdf">How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</a> (Wang et al., arxiv 2023) + <a href="https://arxiv.org/pdf/2301.13688.pdf">The Flan Collection: Designing Data and Methods for Effective Instruction Tuning</a> (Longpre et al., ICML 2023)</h6> <p><b>Flan Collection</b> and <b>TuLu</b> are two large, holistic collection of different instruction-tuning datasets in few-shot, zero-shot, chain-of-though styles. They have demonstrated that training with such mixed prompt and multi tasks settings help models (i.e. T5, LLaMa) generalize better unseen domains and uncover new skills.</p> <p><img src="/assets/img/cheatsheet/tulu.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Several findings:</p> <ul> <li>There is no best instruction collection for all tasks.</li> <li>Base model used to instruct-tune is important (i.e. LLaMa &gt; OPT, Pthia across sizes).</li> <li>Smaller models may benefit more from instruction tuning.</li> <li>Models fined-tuned on traditional NLP instruction datasets (e.g. CoT, FLAN, SuperNI) perform poorly on open-ended generation.</li> <li>In addition to benchmark-based evaluation, model-based evaluation (e.g. using GPT-4 to score the predictions) is necessary for the evalation of open-ended generation task. However, model-based evaluation should not be the sole metric as bias may occur when GPT-4 based evaluation prefers long and diverse generations.</li> </ul> <p><br/></p> </li> <li> <h6 id="least-to-most-prompting-enables-complex-reasoning-in-large-language-models-zhou-et-al-iclr-2023"><a href="https://openreview.net/forum?id=WZH7099tgfM">Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</a> (Zhou et al., ICLR 2023)</h6> <p><b>Least-to-Most</b> few-shot prompting helps improve the solving capacity of reasoning problems that are harder than provided demonstrations (<em>easy-to-hard generalization</em>) by breaking down the problem into a series of subproblems and sequentially solving subproblems. The difference between <b>Least-to-Most</b> and <b>CoT</b> may be that <b>CoT</b> does not explicitly use command decomposition (e.g. “how long does each trip take ?”) and demonstrations in <b>Least-to-Most</b> are often subproblems of the target problem (i.e. recursive programming).</p> <p><img src="/assets/img/cheatsheet/least-to-most.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Several findings:</p> <ul> <li>Generalize better to the length longer than those in the demonstrations.</li> <li>Better math reasoning for complex problems (i.g. those require many solving steps)</li> <li>Decomposition prompts don’t generalize well across different domains. A specific domain needs a specific decomposition template.</li> </ul> <p><br/></p> </li> <li> <h6 id="multitask-prompt-tuning-enables-parameter-efficient-transfer-learning-wang-et-al-iclr-2023"><a href="https://arxiv.org/pdf/2303.02861.pdf">Multitask Prompt Tuning enables Parameter-Efficient Transfer Learning</a> (Wang et al., ICLR 2023)</h6> <p>In the context of efficient multi-task learning, learning a single prompts for all training tasks, then adaptive fine-tuning it for downstream task may not be optimal as it fails to leverage the commonalities while minizing the interference among training tasks. To enable efficient knowledge sharing across tasks, this paper introduces <b>multitask prompt tuning (MPT)</b>.</p> <p><img src="/assets/img/cheatsheet/multitask_prompt.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Specifically, the prompt \(P_k\) for \(k\)-th task is a composition of two components:</p> \[P_k = P^* \circ (u_k \otimes v_k^T )\] <p>where \(P^*\) is shared among tasks and \(W_k = (u_k \otimes v_k^T )\) is low-rank task-specific prompt for \(k\)-th task.</p> <p>Learning the above prompt decomposition from multiple training tasks may cause the shared prompt \(P^*\) overfit to the large tasks. To mitigate this issue, <b>(MPT)</b> employs three loss functions:</p> <ul> <li>For \(k\)-th source task, teacher prompt \(P_k^{teacher}\) is obtained via conventional prompt tuning (independent of other tasks). Then, \(P_k\) is optimzed to match with \(P_k^{teacher}\):</li> </ul> \[\mathcal{L}_{logits} = \sum_{k-th \; task} \sum_{sample \; (x, y)} KL[ P(y | x; \theta, P_k^{teacher} ) || P(y | x; \theta, P_k ) ]\] <ul> <li>Hidden states of teacher model (\(P_k^{teacher}\)) and student model (\(P_k\)) shoule match.</li> </ul> \[\mathcal{L}_{hidden} = \sum_{k-th \; task} \sum_{sample \; (x_i, y_i)} (H_{k,i} - H_{k,i}^{teacher})^2\] <ul> <li>Total loss:</li> </ul> \[\mathcal{L}_{total} = \mathcal{L}_{PLM} + \mathcal{L}_{hidden} + \mathcal{L}_{logits}\] </li> <li> <h6 id="grammar-prompting-for-domain-specific-language-generation-with-large-language-models-wang-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.19234.pdf">Grammar Prompting for Domain-Specific Language Generation with Large Language Models</a> (Wang et al., arxiv 2023)</h6> <p>It is challenging to perform in-context learning with LLMs for the prediction of highly structured languages (e.g. semantic parsing or domain-specific language (DSL)). Effectively, DSLs are unlikely frequently seen during pretraining of LLM and it is not adequate for the model to uncover the complex task specification/requirement within a few demonstrations.</p> <p>This paper introduces <b>grammar prompting</b>, which augments in-context demonstrations with domain-specific constraint, expressed under <a href="https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form">Backus–Naur Form (BNF) context-free grammar</a>. BNF is a metasyntax notation provding a symbolic way to define the syntax of a language (e.g. see <b>G[y]</b> as a minimal BNF grammar specialized for a calendar DSL of which <b>y</b> is an example).</p> <p><img src="/assets/img/cheatsheet/grammar.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>While an instance of a DSL language like <b>y</b> is rare in the pre-training, author argues that the metalanguage used to describe the DSL language like BNF <b>G[y]</b> is, to some extent, more often. Consequently, like Chain-of-Thought prompting, <b>grammar prompting</b> suggests performing several intermediate reasoning step (i.e. generate automatically <b>G[y]</b>) before arriving at the final prediction (i.e. decoding <b>G[y]</b> to get <b>y</b>).</p> <p>During in-context learning, author discovers that <b>providing the full grammar G</b> to the demonstrations is not effective. They proposes instead a minimal grammar G’ \(\subset\) G which is enough to constraint the generation of the corresponding demonstrations.</p> <p>The generation of <b>G’</b> is constrained by <em>metalanguage</em> (the grammar of G’) and the generation of <b>y</b> is constrained by G’ (the grammar of y). Instead of verifying the validity of generated token at each decoding step, <b>grammar prompting</b> first predicts the whole output (without constraint). If the output is legal, it is returned. Otherwise, an incremental parser is used to extract the longest valid prefix from the output. The prediction is continued from this prefix.</p> <p>Tested on semantic parsing task, <b>grammar prompting</b> outperforms standard prompting without constrained decoding or with decoding constrained on full grammar G. However, it still lags behind the prompting with decoding constrained on the gold G’ (not predicted G’ as in <b>grammar prompting</b>), indicating room for future improvements.</p> </li> <li> <h6 id="symbol-tuning-improves-in-context-learning-in-language-models-wei-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.08298.pdf">Symbol Tuning Improves In-Context Learning In Language Models</a> (Wei et al., arxiv 2023)</h6> <p>The paper relies on the intuitions related to in-context learning (ICL) for classification-type tasks:</p> <ul> <li>The model is not forced to learn to reason from provided demonstrations as it can sometimes understand the task by just reading the instruction and natural language labels.</li> <li>When the model can not rely on the instructions (e.g. empty instruction) or relevant natural language labels (e.g. random label) to figure out the task, it has to reason from and learn the input-label mapping to understand the task, as described in the image below:</li> </ul> <p><img src="/assets/img/cheatsheet/symbol_tuning.png" alt="" style="width: 30%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>Author proposes <b>Symbol Tuning</b>, a simple fine-tuning that forces the model to learn the input-label mapping in the demonstrations, by removing the instruction and replacing the natural language labels by random (semantically-unrelated) ones. By this way, the model could be endowed with better in-context learning.</p> <p>Findings are:</p> <ul> <li>Symbol-tuning shows strong potential to improve the model performance when tasks are not clear, relevant labels are unavailable and require learning from demonstrations.</li> <li>Symbol-tuning may degrade the performance of smaller LM (8B) on tasks where task instructions and relevant labels are available. One solution to this is to mix instruction-tuning and symbol-tuning data during the tuning. The proportion of two components is not important.</li> <li>Symbol-tuning is efficient as it requires fewer steps to achieve stable performance.</li> <li>Symbol-tuned models can override what it has learnt before via flipped labels (e.g. 0 –&gt; True, 1 –&gt; False instead of 1 –&gt; True, 0 –&gt; False as usual). Indeed, symbol tuning forces the model to read the {input, flipped label} pairs in the demonstration, which should make it rely less on prior knowledge that may counter the flipped labels.</li> </ul> <p><br/></p> </li> <li> <h6 id="selective-annotation-makes-language-models-better-few-shot-learners-su-et-al-iclr-2023"><a href="https://arxiv.org/pdf/2209.01975.pdf">Selective Annotation Makes Language Models Better Few-Shot Learners</a> (Su et al., ICLR 2023)</h6> </li> <li> <h6 id="learning-to-reason-and-memorize-with-self-notes-lanchantin-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.00833.pdf">Learning to Reason and Memorize with Self-Notes</a> (Lanchantin et al., arxiv 2023)</h6> <p><img src="/assets/img/cheatsheet/selfnotes.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>Unlike Chain-of-Thought or Scratchpad prompting which generates a reasoning path to arrive at answer for a question after reading the entire context/demonstrations, <b>Self-Notes</b> allows the model to create reasoning tokens (aka. take notes) at any point while reading the context. This has two advantages:</p> <ul> <li>Faciliate the multi-step reasoning where partial reasonsing tokens can be deviated from the context on the fly.</li> <li>Act as working memory for tracking the state of model computation: while traversing the context, the model can explicitly write down the current state as new tokens. If the later reasonings need this state, the model can recall it without thinking again from scratch.</li> </ul> <p><b>Self-Notes</b> employs several special tokens {[start], [end]} to signal when to take a note and when to finish a note. Once the note ends, it is appended to the context and the model continues to process the rest of the context.</p> <p><b>Self-Notes</b> is fine-tuned with supervised dataset (training sample includes context, question, gold self-notes and answer) or unsupervised dataset (there is no gold self notes, from the context, the model learns to generate its own question and insert its answers as self-notes).</p> <p>Trick: Self-Notes manually amplifies the probability of [start] token to favor the production of more notes.</p> </li> <li> <h6 id="self-consistency-improves-chain-of-thought-reasoning-in-language-models-wang-et-al-iclr-2023"><a href="https://arxiv.org/pdf/2203.11171.pdf">Self-Consistency improves Chain Of Thought Reasoning in Language Models</a> (Wang et al., ICLR 2023)</h6> <p><img src="/assets/img/cheatsheet/self_consistency.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>While <b>Chain of Thought (CoT)</b> prompting generates only one reasoning path to arrive at an anwser for a question via greedy decoding, <b>Self-consistency</b> instead proposes to produce a diverse set of reasoning paths via sampling decoding methods (e.g. top-k, top-p or beamsearch), each reasoning path leads to an answer, the best answer is chosen using majority voting for example.</p> <p>The rationale behind <b>Self-consistency</b> is that there could have different ways of thinking to solve a question. As the LM is not perfect reasoner, it may produce an incorret reasoning path or make mistakes in one of the reasoning steps (even though the reasoning path is relevant). Generating multiple diverse reasoning path can increase the likelihood of having a correct reasoning process, ratherthan relying solely on a single path.</p> <p>Some benifits of <b>Self-consistency</b>:</p> <ul> <li> <p>While <b> Single CoT </b> could sometimes hurt the performance, <b>Self-consistency</b> helps to alleviate this issue.</p> </li> <li> <p>Sampling decode outperforms beam search decoding in <b>Self-consistency</b>.</p> </li> <li> <p><b>Self-Consistency</b> can work with prompts that contain minor mistakes.</p> </li> </ul> </li> </ul> <p><b>2022</b></p> <ul> <li> <h6 id="maieutic-prompting-logically-consistent-reasoning-with-recursive-explanations-jung-et-al-emnlp-2022"><a href="https://aclanthology.org/2022.emnlp-main.82.pdf">Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</a> (Jung et al., EMNLP 2022):</h6> <p>LLM can generate inconsistent and unreliable explanation when a question and its negated version get the same answer (e.g. “One is a number that comes before zero ? … True” vs. “One is a number that comes after zero ? … True”). They introduce a novel prompting technique, <b> Maieutic Prompting</b> to improve the consistency in LLM’s generation. Inspired by Socratic style of conversation, the inference process exploits the depth of reasoning by asking recursively if a newly generated explanation is logically consistent with its parent (previous) explanation, as illustrated in the figure below:</p> <p><img src="/assets/img/vldb/maieutic.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/> <em>(source: copied from the paper).</em></p> </li> <li> <h6 id="metaicl-learning-to-learn-in-context-min-et-al-naacl-2022"><a href="https://aclanthology.org/2022.naacl-main.201.pdf">MetaICL: Learning to Learn In Context</a> (Min et al., NAACL 2022)</h6> <p><b>MetaICL</b> is a meta-training framework where the model is fine-tuned with in-context demonstrations on a large set of training tasks. MetaICL improves in-context learning for new unseen task at inference time.</p> <p><img src="/assets/img/cheatsheet/meta_icl.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>MetaICL is meta-trained on a collection of &gt; 30 tasks including text classification, QA, NLI, etc. The input context for each task has 4-32 demonstrations. MetaICL demonstrates significant gains for low-resource tasks or tasks whose data distribution is different from training data (unseen domain). It matches or sometimes outperforms models fine-tuned on target data. Furthermore, fine-tuning a meta-trained model on target data achieves the best performance.</p> <p>Ablation study suggests MetaICL tends to saturate when number of demonstrations reaches 16 and more diverse meta-training tasks results in better performance.</p> </li> <li> <h6 id="self-instruct-aligning-lm-with-self-generated-instructions-wang-et-al-arxiv-2022"><a href="https://arxiv.org/pdf/2212.10560.pdf">Self-Instruct: Aligning LM with Self Generated Instructions</a> (Wang et al., arxiv 2022)</h6> <p>In line with FLAN, TO, <b>Self-Instruct</b> continues to showcase the impressive ability of “instruction-tuned” LM to generalize to new tasks via zero-shot learning. FLAN, TO use instruction data manually created by human which is limited in quantity, diversity and creativity. This may impact the generality of the tuned model. Alternatively, <b>Self-Instruct</b> relies on the model itself (i.e. GPT3) to create automatically new instruction/input/output samples from a seed set of initial instruction/input/output samples through in-context learning. The new instruction data is then used to fine-tune the original model. Some post-preprocessing steps are also applied to filter low-quality data: (i) only retain new instructions that are low-overlap with existing instructions, (ii) discard new instructions that contain some specific keywords (e.g. images, graphs), (iii) discard instances that have different outputs for the same input.</p> <p>The instruction/instance samples generated by <b>Self-Instruct</b> shows good diversity. Most of the instructions are meaningful while instances may contain noise (to a reasonable extent).</p> <p><img src="/assets/img/cheatsheet/self_instruct.png" alt="" style="width: 70%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>P/s: <b>Self-Instruct</b> was used to generate 52K instruction-following samples (<a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca</a>) to fine-tune <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">LLaMA 7B</a> model, resulting ChatGPT-like <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca 7B</a></p> </li> <li> <h6 id="finetuned-language-models-are-zero-shot-learners-wei-et-al-iclr-2022"><a href="https://arxiv.org/pdf/2109.01652.pdf">Finetuned Language Models are Zero-Shot Learners</a> (Wei et al., ICLR 2022)</h6> <p>The paper shows that finetuning language models on a collection of datasets via instructions (aka. <b>Instruction tuning</b>, e.g. “Translate this sentence to French:…”) can considerably improve zero-shot performance on unseen tasks. The rationale behind instruction tuning is that the format of pre-training data of a LM is not similar to the format of prompts, making zero-shot inference hard. To bridge this gap, they introduce <b>FLAN</b>, a LaMDA-PT (137B parameters) fine-tuned on a mixture of NLP datasets expressed under natural language instructions. FLAN zero-shot(ly) outperforms others LLMs of similar number of parameters (LaMDA-PT 137B , GPT-3 173B) on wide range of NLP tasks.</p> <p>Importantly, ablation studies shows that fine-tuning with instruction is a key factor for zero-shot performance on unseen tasks. For example, while fine-tuning LMs with translation task, instead of using input-output pair <em>(“how are you ?”, “comment vas tu ?”)</em>, it’s better using <em>(“translate this sentence to french: how are you ?”, “comment vas tu ?”)</em>.</p> <p><img src="/assets/img/cheatsheet/flan.png" alt="" style="width: 70%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="multitask-prompted-training-enables-zero-shot-task-generalization-sanh-et-al-iclr-2022"><a href="https://arxiv.org/pdf/2110.08207.pdf">Multitask Prompted Training Enables Zero-Shot Task Generalization</a> (Sanh et al., ICLR 2022)</h6> <p>Similar to FLAN, Sanh et al. introduces <b>T0</b>, a LM-adapted T5 3B (Lester et al. 2021) fine-tuned on mixture of NLP datasets via natural language instructions, to improve zero-shot performance on unseen tasks. T0 and its variants achieved similar performance w.r.t FLAN despite being much smaller.</p> <p><img src="/assets/img/cheatsheet/to.png" alt="" style="width: 70%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="chain-of-thought-prompting-elicits-reasoning-in-large-language-models-wei-et-al-neurips-2022"><a href="https://arxiv.org/pdf/2201.11903.pdf">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a> (Wei et al., Neurips 2022)</h6> <p>While scaling up a LM is not sufficient for improve the performance of LM on reasoning tasks, the paper presents <b>Chain-of-Thought</b> prompting to unlock the reasoning ability of large language models (yes, only large LMs, mentionned by the author) by decomposing the initial task into intermediate steps and solving each steps before outputing the final answer, just emulate the way human processes a complicated reasoning problem. Instead of finetuning or rationale-augmented training a LM which requires a larget dataset of {question, intermediate step, answer}, <b>Chain-of-Thought Prompting</b> is only performed on large language models (e.g GPT3, PALM) via in-context few-shot learning learning.</p> <p>An example:</p> <p><img src="/assets/img/cheatsheet/chain_of_thought.png" alt="" style="width: 70%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="do-prompt-based-models-really-understand-the-meaning-of-their-prompts-webson-et-al-naacl-2022"><a href="https://arxiv.org/pdf/2109.01247.pdf">Do Prompt-Based Models Really Understand the Meaning of Their Prompts?</a> (Webson et al., NAACL 2022)</h6> <p>Under the \(k\)-shot scenerios (\(k=0..256\)) for NLI task, the paper finds that LMs learn irrelevant prompts, misleading prompts as fast as instructive prompts, and this is consistent across various models (GPT, BERT, T0, T5). This questions whether the models understand the semantics of the prompts or they are too robust to prompt semantics, making them distinguish proper instructions from pathological ones.</p> <p>They also shows that LMs are more sensitive to the semantics of prediction labels. Learning to predict arbitrary labels (e.g. 1 for Yes, 2 for No) or reversed labels (e.g. No for Yes, Yes for No) is much slower than predicting directly the original labels (Yes/No). The choice of prediction labels can contaminate the semantics of prompt template. Proper prompt associated with arbitrary labels (e.g. 1 for Yes, 2 for No) underperformed irrelevant prompts associated with direct label (Yes/No). Intuitively, given a few samples, human can easily learn the mapping Yes \(\rightarrow\) 1, No \(\rightarrow\) 2.</p> </li> </ul> <p><b>2021</b></p> <ul> <li> <h6 id="prefix-tuning-optimizing-continuous-prompts-for-generation-li-et-al-acl-2021"><a href="https://aclanthology.org/2021.acl-long.353.pdf">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a> (Li et al., ACL 2021)</h6> <p>Traditional fine-tuning of a LM model for a downstream task involves modifying all the model parameters, consequently, a single set of parameters can just work best for a single task. Inspired by prompting, <b>prefix-tuning</b> freezes the LM parameters and instead prepend to it a sequence of task-specific vectors \(P_{\theta}\) (aka. <em>prefix</em>): \([P_{\theta}; LM_{\phi}]\) that represent the downstream task, we optimize solely the <em>prefix</em> \(P_{\theta}\) using the task’s data to steer the LM to the task.</p> <p>Prefix-tuning brings some advantages:</p> <ul> <li>A single LM is reused across different downstream tasks since its parameters are kept intact \(\rightarrow\) efficient storage.</li> <li>Only the prefix vector corresponding to the downstream task need to be optimized \(\rightarrow\) lightweight fine-tuning: much fewer parameters w.r.t. LM.</li> <li><b>Prefix-tuning can outperform full fine-tuning in low-data setting and have better generalization.</b></li> </ul> <p><img src="/assets/img/cheatsheet/prefix_tuning.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="the-power-of-scale-for-parameter-efficient-prompt-tuning-lester-et-al-emnlp-2021"><a href="https://aclanthology.org/2021.emnlp-main.243.pdf">The Power of Scale for Parameter-Efficient Prompt Tuning</a> (Lester et al., EMNLP 2021)</h6> <p>Similarly to Prefix-Tuning, <b>prompt-tuning</b> learns task-specific “soft-prompts” (embedding) prepended to task-input (prefix) to steer the LM to perform the task without changing its parameters. While Prefix-Tuning prepends prefix activations to every layers in the encoder, <b>prompt-tuning</b> simplifies this by only adding <em>k</em> tunable tokens per downstream task to the input text at the input layer (without further interventions in intermediate layers) \(\rightarrow\) <b>prompt-tuning</b> has less parameters than Prefix-Tuning.</p> <p>In addition, <b>prompt-tuning</b> is based on T5 that they found that prompt-tuning with T5 off-the-shelf as the frozen model is inefficient. T5 is pre-trained exclusively on span corruption marked with unique sentinel tokens. As prompt-tuning does not modify the model parameters, it risks to produce unnaturally sentinel tokens in the output. This issue is easily overcome by full fine-tuning. For this reason, before performing prompt-tuning, they continue to pre-train T5 with LM objective in order for the model to produce natural text output.</p> <p>Other features of prompt-tuning:</p> <ul> <li>Performance scales with model size: the larger, the better.</li> <li>May improve the robustness to domain shifts: outperform in-domain fine-tuning on out-of-domain datasets.</li> <li>Efficient prompt ensemble: better than single prompt and parameter-efficient as the core LM is freezed and shared.</li> </ul> </li> </ul> <h4 id="4-tools-augmented-language-model-"><b>4. Tools-Augmented Language Model </b></h4> <p><b>2023</b></p> <ul> <li> <h6 id="toolformer-language-models-can-teach-themselves-to-use-tools-schick-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2302.04761.pdf">Toolformer: Language Models Can Teach Themselves to Use Tools</a> (Schick et al., arxiv 2023)</h6> <p><b>Toolformer</b>, a fine-tuned LM, is capable of deciding when to call APIS, which APIs to call, what arguments to pass, how to leverage API’s results in next token prediction.</p> <p><img src="/assets/img/cheatsheet/toolformer_0.png" alt="" style="width: 30%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>The key idea lies at self-supervised training where LM is first used to generate a dataset augmented with API calls via promting (Figure a) and filtering (Figure b). It is then fine-tuned on this dataset with regular LM objective.</p> <p>(a) <img src="/assets/img/cheatsheet/toolformer_1.png" alt="" style="width: 30%"/> (b) <img src="/assets/img/cheatsheet/toolformer_2.png" alt="" style="width: 65%"/></p> <p>(source: copied from the paper)</p> <p>The generation of API-augmented dataset is illustrated in three steps:</p> <ul> <li>Sampling API calls (Fig. a): write prompt (with a few demonstrations) to ask the model to select <em>k</em> positions in the input text and add up to <em>m</em> API calls (e.g. Q/A API, calculator API, calendar API) at each position that are possible relevant for the prediction of next tokens.</li> <li>Executing API calls (Fig. b): independently execute API calls and get the results.</li> <li>Filtering API calls (Fig .c): as not all API calls generated by prompting are useful, this step filters out API calls whose results that do not improve the perplexity of future predicted tokens.</li> </ul> <p>After filtering, remaining API calls (together with its results) are merged into the original dataset, resulting tool-augmented dataset, used to teach LM how and when to call external APIs.</p> </li> <li> <h6 id="react-synergizing-reasoning-and-acting-in-language-models-yao-et-al-iclr-2023"><a href="https://arxiv.org/pdf/2210.03629.pdf">React: Synergizing Reasoning and Acting in Language Models</a> (Yao et al., ICLR 2023)</h6> <p><img src="/assets/img/cheatsheet/react.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p><b>React</b> is a prompting method that combine Chain-of-thought (<b>Re</b> in React stands for Reasoning) and interfacing with external-API (<b>act</b> in React stands for action) to reduce the hallucination as well as to improve interpretability and trustworthiness of LLMs. The external API considered in this work is Wikipedia API, supporting 3 actions: (i) <b>search[entity]</b> return the first 5 sentences of the entity’s wiki page; (ii) <b>lookup[string]</b> return the sentence containg the string; (iii) <b>finish[entity]</b> finishes the task and return the answer. It should be noted that the first two actions are based on simple exact matching.</p> <p><b>React</b> outperforms CoT on Fever task and slightly lags behind CoT on HotpotQA task. With much fewer demonstrations (3-5 samples), <b>React</b> + CoT + Self-consistency (SC) performs best across tasks and reach the CoT-SC (with 21 samples) performance.</p> <p>In addition, on HotpotQA task, <b>React</b> does not work well with smaller pretrained-model (i.e. PaLM-8/62B). However, it gives the best performance with the fine-tuned PaLM-8/62B, even compared with the larger freezed PaLM-540b.</p> </li> <li> <h6 id="binding-language-models-in-symbolic-languages-cheng-et-al-iclr-2023"><a href="https://arxiv.org/pdf/2210.02875.pdf">Binding Language Models in Symbolic Languages</a> (Cheng et al., ICLR 2023)</h6> </li> </ul> <h4 id="5-misc-"><b>5. Misc </b></h4> <p><b>2023</b></p> <ul> <li> <h6 id="task-specific-skill-localization-in-fine-tuned-language-models-panigrahi-et-al-icml-2023"><a href="https://openreview.net/pdf?id=Rgnaj43Pk0">Task-Specific Skill Localization in Fine-tuned Language Models</a> (Panigrahi et al., ICML 2023)</h6> </li> <li> <h6 id="same-pre-training-loss-better-downstream-implicit-bias-matters-for-language-models-liu-et-al-icml-2023"><a href="https://proceedings.mlr.press/v202/liu23ao/liu23ao.pdf">Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models</a> (Liu et al., ICML 2023)</h6> </li> <li> <h6 id="doremi-optimizing-data-mixtures-speeds-up-language-model-pretraining-xie-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.10429.pdf">DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining</a> (Xie et al., arxiv 2023)</h6> </li> <li> <h6 id="data-selection-for-language-models-via-importance-resampling-xie-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2302.03169.pdf">Data Selection for Language Models via Importance Resampling</a> (Xie et al., arxiv 2023)</h6> <p>The paper presents <b>DSIR</b>, an efficient and scalable for selecting additional relevant data from a large raw unlabeled dataset (e.g. the Pile) that match</p> <p><img src="/assets/img/cheatsheet/dsir.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="language-models-represent-space-and-time-gurnee-et-al-arxiv-2023"><a href="2310.02207">Language Models represent Space and Time</a> (Gurnee et al., arxiv 2023)</h6> <p>In line with <a href="https://huynhvp.github.io/blog/2023/representation-probe/">Emergent World Representations - Exploring a Sequence Model Trained on a Synthetic Task</a>, this paper discovers that LLMs learn latent representations that are relevant to the input fed to it. Specifically, when feeding the names of locations in the world (e.g. cities, countries) into LLM, the spatial information (i.e. longitude and lattitude) of the locations are encoded in internal neurons. In other words, the coordianates of locations can be linearly recovered from the activations of the mid-to-late layers of LLMs.</p> </li> <li> <h6 id="can-foundation-models-wrangle-your-data-narayan-et-al-vldb-2023"><a href="https://arxiv.org/pdf/2205.09911.pdf">Can Foundation Models Wrangle Your Data?</a> (Narayan et al., VLDB 2023)</h6> <p>The paper investigates the capability of generative LLMs (i.e. GPT3) on data wrangling tasks: entity matching, data imputation, error detection:</p> <ul> <li>Generative LLM benefits unifed framework for multi-task learning (task-agnostic architecture).</li> <li>Select a subset of entity’s attribute for entity matching is non-trivial.</li> <li>Performance is sensible to prompt formatting (even with minor modification in prompt) and demonstrations.</li> </ul> <p><br/></p> </li> <li> <h6 id="ranking-and-tuning-pre-trained-models-a-new-paradigm-for-exploiting-model-hubs-you-et-al-jmlr-2023--logme-practical-assessment-of-pre-trained-models-for-transfer-learning-you-et-al-icml-2021"><a href="https://www.jmlr.org/papers/volume23/21-1251/21-1251.pdf">Ranking and Tuning Pre-trained Models: A New Paradigm for Exploiting Model Hubs</a> (You et al., JMLR 2023) + <a href="http://proceedings.mlr.press/v139/you21b.html">LogME: Practical Assessment of Pre-trained Models for Transfer Learning</a> (You et al., ICML 2021)</h6> <p>Given the deluge of available pre-tranined models \(\{\phi_m\}_{m=1}^{M}\), it is challenging to pick the model that can yeild the best transfer learning on target down-stream dataset \(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n\).</p> <p><img src="/assets/img/cheatsheet/logme_1.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Formally, each pretrain-model \(\{\phi_m\}\) fine-tuned on \(\mathcal{D}\) has ground-truth transfer performance \(T_m\) (e.g. accuracy, MAP, MSE, etc). As computing all \(T_m\) for all models is prohibitively expensive as \(M\) grows, it is more relevant to have a score \(S_m\) for model \(\{\phi_m\}\) without fine-tuning it on \(\mathcal{D}\) in such a way that \(S_m\) should well correlate with \(T_m\). Thereby, the ranking of pre-trained model w.r.t. \(\mathcal{D}\) can be based on \(S_m\), instead of \(T_m\). The correlation between \(S_m\) and \(T_m\) is measured by Kendall’s \(\tau\) coefficient:</p> \[\tau = \frac{2}{M(M-1)} \sum_{1 &lt;=i &lt; j &lt;= M} sign(T_i - T_j) sign(S_i - S_j)\] <p>The larger \(\tau\), the better the ranking of \(\{\phi_m\}_{m=1}^{M}\) models.</p> <p>\(S_m\) is computed as the probability \(p(y \vert F)\) where \(y \in R^n\) is the label vector of \(n\) (i.e. scalar label) samples, \(F = \{ f_i = \phi_m(x_i) \}_{i=1}^n \in R^{n \times D}\) is feature vectors extracted by \(\phi_m\). Common solution to estimate \(p(y \vert F\) is to train a regression model \(w\) (similar to apply a linear layer on top of neural model for transfer learning) on \((F, y)\) maximizing the likelihood \(p(y \vert F, w)\). However, this approach has shown to be prone to over-fitting. Alternatively, these papers propose <b>LogME</b> which marginalizes \(p(y \vert F, w)\) over all values of \(w\): \(p(y \vert F) = \int p(w) \times p (y \vert F, w)\). To make it tractable, both prior \(p(w)\) and likelihod \(p(y \vert F, w)\) are assumed to have normal distribution parameterized by \(\alpha\) and \(\beta\): \(p(w) = \mathcal{N} (0, \alpha^{-1}I)\), \(p (y_i \vert f_i, w) = \mathcal{N}(y_i \vert w^Tf_i, \beta^-1)\).</p> <p>\(\alpha\) and \(\beta\) are estimated by an iterative algorithm (see section 4.2)</p> <p><img src="/assets/img/cheatsheet/logme_2.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Experimented on GLUE benchmark with 8 popular pre-trained LMs, the result shows that \(S_m\) represented by <b>LogME</b> well correlates with ground-truth fine-tuned accuracy \(T_m\).</p> <p><img src="/assets/img/cheatsheet/logme_3.png" alt="" style="width: 80%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="on-exploring-the-reasoning-capability-of-large-language-models-with-knowledge-graphs-lo-et-al-genirsigir-2023"><a href="https://coda.io/@sigir/gen-ir/accepted-papers-17">On Exploring the Reasoning Capability of Large Language Models with Knowledge Graphs</a> (Lo et al., GenIR@SIGIR 2023)</h6> <p>The paper investigates the zero-shot performance of LLMs (particularly text-davinci-003, ChatGPT and GPT4) in infering missing entities/relations in KG or predicting a predicate path between two given entities. Specifically, they seek to see whether LLMs are capable of recalling their internal knowledge graph that supposed to be learnt during the pre-training and reason with it to solve the tasks.</p> <p>Results: while text-davinci-003, ChatGPT struggles, GPT-4 shows pretty impressive accuracy for 1-hop entity/relation prediction, and especially for multi-hop relation (predicate path) prediction given the context document without instruction/task decomposition (see below).</p> <p><img src="/assets/img/cheatsheet/chilo.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="towards-robust-and-efficient-continual-language-learning-fisch-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2307.05741.pdf">Towards Robust and Efficient Continual Language Learning</a> (Fisch et al., arxiv 2023)</h6> <p>Given the availability of numerous model checkpoints fine-tuned on different previous task \(\{t_1,.., t_n\}\), this paper introduces an approach to learn a checkpoint selector that help to pick the most relevant checkpoint of a previous task \(t_i\) as base model to fine-tune the new task \(t_{n+1}\) if exist, otherwise, it’s better to start off with the pretrained model rather than with a random checkpoint that could yield negative impact.</p> <p><img src="/assets/img/cheatsheet/cont_learning.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>The checkpoint selector is a simple binary gradient boosted decision tree (GBDT) applied on the features \(\phi(t_i, t_{n+1})\) between previous task \(t_i\) and target task \(t_{n+1}\) to determine whether model’s parameters fine-tuned for \(t_i\) is a good initialization for \(t_{n+1}\). Specifically, features include:</p> <ul> <li>task metadata: 1 if \(t_i\) and \(t_{n+1}\) belongs to the same pre-defined task family, 0 otherwise.</li> <li>relative performance: relative 0-shot and 5-shot performance of model fine-tuned on \(t_i\), then \(t_{n+1}\) w.r.t. model fine-tuned uniquely on \(t_{n+1}\).</li> <li>gradient-update similarity: similarity between average magnitude of weight change of model fine-tuned on \(t_i\), then \(t_{n+1}\) and model fine-tuned uniquely on \(t_{n+1}\).</li> </ul> <p><br/></p> </li> <li> <h6 id="beyond-scale-the-diversity-coefficient-as-a-data-quality-metric-demonstrates-llms-are-pre-trained-on-formally-diverse-data-lee-et-al-icml-2023"><a href="https://arxiv.org/pdf/2306.13840.pdf">Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data</a> (Lee et al., ICML 2023)</h6> </li> <li> <h6 id="textbooks-are-all-you-need-gunsasekar-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.13062.pdf">Textbooks Are All You Need</a> (Gunsasekar et al., arxiv 2023)</h6> <p>The paper introduces <b>pi-1</b>, a 1B decoder-only LLM trained of high-quality code book that competes with many much larger sized models. The recipe for success comes from the careful selection of data for training and fine-tuning. Author inspects the popular datasets used to train sota Code-LLMs (e.g. The Stack) and discovers several drawbacks that may hindle model from effective learning:</p> <ul> <li>Many codes are not self-contained. They depend on extenal moduls or files.</li> <li>Many codes do not contain meaningful semantics, but trivial texts.</li> <li>Many codes are not well documents, making them difficult to learn from.</li> <li>Skewed distribution of topics/concepts in the dataset.</li> </ul> <p>From those intuitions, authors propose to select and generate a much smaller but higher quality corpus:</p> <ul> <li>A <em>filter code-language</em> dataset (6B tokens) filterd from The Stack and StackOverflow. How the filterd is built ? First, GPT-4 is used to annotae ~100K code-examples with two labels: high education value and low educational value. Then, a random forest classifier is trained on the embeddings retrieved from a pretrained codegen model of those 100K examples to predict the label.</li> <li>A <em>synthetic textbook</em> dataset consists of &lt;1B tokens, generated by GPT-3.5. The diversity of generated text book is controlled by constraining the topics and target audiences in the prompt.</li> <li>A smal <em>synthetic Python exercises</em> dataset of ~180M tokens is used to further fine-tune the model, playing the same role as instruction-tuning, making model better align with natural language instructions.</li> </ul> <p>The last fine-tuning step is proved to be important, that leads to subtaintial improvement in generalizing to new tasks. Indeed, the fine-tuned model is capable of distilling easier seen tasks from pretraining (e.g. calling external libraries more logically). An example is illustrated below (phi-1 and phi-1-small are models fine-tuned with <em>synthetic Python exercises</em> while phi-1-base is not.)</p> <p><img src="/assets/img/cheatsheet/phi-1.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> </li> <li> <h6 id="evaluating-and-enhancing-structural-understanding-capabilities-of-large-language-models-on-tables-via-input-designs-sui-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.13062.pdf">Evaluating and Enhancing Structural Understanding Capabilities of Large Language Models on Tables via Input Designs</a> (Sui et al., arxiv 2023)</h6> <p>This work introduces Structural Understanding Capabilities (<b>SUC</b>) benchmark to assess whether LLMs can truly understand structured tabular data. The benchmark includes 5 tasks: table partition (detect the location of table in the context), table size detection, merged cell detection, cell lookup (return the mention given its row/column index), column&amp;row retrieval (return the column values given its index). They discovered that LLMs have some basic understading of structural tabular data, but are still far from being good.</p> <p>Experimented with GPT-3 family, results demonstrate that:</p> <ul> <li>Use markup language such as HTML rather than NL to represent table gives significant gain.</li> <li>In-context learning with one demonstration outperforms zero-shot, suggestings that model needs examplars to understand the structural information.</li> <li>Additional information (e.g. table formation explaination) shoud be placed ahead the table in the context</li> <li>Adding table format explaination to the context (e.g. “Each table cell is defined by a &lt;td&gt; and a &lt;/td&gt; tag” ) is generally helpful.</li> <li>Self-augmented prompting (see figure below): similarly to CoT, before tackling the target task, several intermediate steps are performed to elicit the structural knowledge learned by LLMs during pretraining, such as asking LLM to identify critical elements in the table.</li> </ul> <p><img src="/assets/img/cheatsheet/tab_prompt.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> </li> <li> <h6 id="benchmarking-large-language-model-capabilities-for-conditional-generation-joshua-et-al-acl-2023"><a href="https://arxiv.org/pdf/2306.16793.pdf">Benchmarking Large Language Model Capabilities for Conditional Generation</a> (Joshua et al., ACL 2023)</h6> <p>Similar to <a href="https://crfm.stanford.edu/helm/latest/">HELM</a> benchmark, this paper introduces a holistic benchmark to evaluate the generation quality of autoregressive LLMs via automatic metrics. The benchmark collects data-to-text and text-to-text datasets (27 in total).</p> <p>Observations:</p> <ul> <li>Few-shot learning falls behind full finetuning. However, multiple tasks have finetuning performance saturated, suggesting there is no clear trend when scaling the models.</li> <li>Finetuned decoder-only LLM can match encoder-decoder LLM when scaling to large size.</li> <li>Overlap-based metrics is not suitable for evaluating few-shot learning as it is sensitive to generation length and LLMs struggle to predict output length properly given the demonstrations in the context.</li> <li>The model ranking can still be reliable when considering a small random subset of the test set (to mitigate the computational cost while performing inference with LLMs). Specifically, (1) randomly sampling <em>n</em> samples and recording the scores of models on those samples, then ranking them based on the scores; (2) perform Wilcoxon Rank Sum test on every pair of models to assess if two models yeilds the same ranking according to a <em>p</em>-value; (3) repeate (1) and (2) <em>k</em> times and count number of times that any pair of models results inditinguisable ranking (according to step (2)).</li> </ul> <p><br/></p> </li> <li> <h6 id="lost-in-the-middle-how-language-models-use-long-contexts-nelson-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2307.03172.pdf">Lost in the Middle: How Language Models Use Long Contexts</a> (Nelson et al., arxiv 2023)</h6> <p>This work empirically analyzes how well LLMs use longer context through two tasks: open-domain QA and key-value retrieval (i.e. the context contains a dictionary of {UUID_key: UUID_value} and model is asked to return the value of a specific key).</p> <p>They observe an U-shaped performance curve as a function of the position of relevant information in the context. In other words, the models perform best when relevant information is located at the beginning or the end of the context. Even for key-value retrieval task, if requested key is in the middle of the dictionary, several models still struggle to get the correct value. Additionally, model performance substaintially decreases as input contexts grow longer.</p> <p><img src="/assets/img/cheatsheet/lost_in_mid.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>Ablation study suggests that encoder-decoder models may exploit better the longer context due to their bidirectional encoders. Placing the query ahead of the context or using instruction-tuning still exhibits the U-trend performance. Finally, for open-domain QA, it is still questioning whether using more context leads to sigificant improvement. Indeed, the performance undergoes a statureation zone as the context grows.</p> </li> <li> <h6 id="faith-and-fate-limits-of-transformers-on-compositionality-dziri-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.18654.pdf">Faith and Fate: Limits of Transformers on Compositionality</a> (Dziri et al., arxiv 2023)</h6> <p>Transformers, on the one hand, can perform impressively on complex task. On the other hand, it can fail suprisingly on trivial tasks. This paper attemps to understand whether this paradox is incidental or substantial limitations of transformer. They investigates three <em>compositional tasks</em> including multi-digit multiplication and dynamic programming. A <em>compositional task</em> can be decomposed into multiple sub-tasks which can be representad as a computation graph and requires cohenrent step-by-step reasoning to arrive at the correct answer. An example of multi-digit multiplication is illustrated as below: <img src="/assets/img/cheatsheet/faith_1.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>Author quantifies the performance of transformer as a function of:</p> <ul> <li><b>reasoning depth</b>: the length of the longest path from answer node to input node.</li> <li><b>reasoning width</b>: (TBD: <em>unclear for me</em> at the time of writing)</li> <li><b>relative Information Gain</b>: quantify the (normalized) gain of input nodes contributed to output nodes.</li> </ul> <p>Considering multi-digit multiplication tasks, the paper shows several empirical evidences on the limit of Transformer:</p> <ul> <li>All settings (zero-shot, in-contexts with and without scratchpad, full finetuning) yield poor OOD generalization (i.e. model trained on {1,2,3}-digit multiplication and tested on {4,5}-digit multiplication which requires wider and deeper computation graph. Full fine-tuning is better than other settings.</li> </ul> <p><img src="/assets/img/cheatsheet/faith_2.png" alt="" style="width: 70%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <ul> <li> <p>Relative information gains reveal that several (spurious) correlations between input and output (e.g. first digit of output highly correlates with first digit of input) are learned. Hence, to some extent, the input is mapped directly to the output without actually executing the multi-hop reasoning over the computation graph.</p> </li> <li> <p>Even though a full computation graph in test set is unseen in training set, its subgraphs do appear in the training set. Consequently, the model can memorize or matches patterns, helping it make correct predictions. However, this does not imply that model has learned a generalized reasoning capabilitities.</p> </li> <li> <p>A large propotion of error is propagation error and restoration error, suggesting: (i) model can perform correct single-step intermediate reasoning, but fail to compose the whole reasoning pipeline, (ii) due to memorization, output can have precise value but the computation steps are incorrect.</p> </li> </ul> </li> <li> <h6 id="improving-representational-continuity-with-supervised-continued-pretraining-sun-et-al-arxiv-2023--fine-tuning-can-distort-pretrained-features-and-underperform-out-of-distribution-kumar-et-al-iclr-2022"><a href="https://arxiv.org/pdf/2302.13289.pdf">Improving Representational Continuity with Supervised Continued Pretraining</a> (Sun et al., arxiv 2023) + <a href="https://openreview.net/pdf?id=UYneFzXSJWh">Fine-tuning can distort pretrained features and underperform out-of-distribution</a> (Kumar et al., ICLR 2022)</h6> <p>The paper title says it all. In the pretraining-then-finetuning paradigm, if the pre-trained features are good and the distribution shift between the fine-tuning data (in-domain) and the testing data (out-domain OOD) for downstream task is large, then fine-tuning outperforms (resp. underperforms) linear probing (only update the last linear layer) on in-domain test data (resp. OOD test data).</p> <p><img src="/assets/img/cheatsheet/lp_ft.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>Author employs a toy example (\(output = w_{*} x\)) to illustrate this issue.</p> <p><img src="/assets/img/cheatsheet/lp_ft_2.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>\(B_0\) is the pretrained feature. \(w\) is learned weights mapping input \(x\) to output \(y\). For fine-tuning, both \(B_0\) and head layer \(v\) needs to be updated. Assuming the subspace spanning in-domain data (horizontal axis \(x_1\)) is orthogonal to the subspace spanning OOD data (vertical axis \(x_2\)), then fine-tuning with in-domain data only modifies \(B_{x_1}\) while \(B_{x_2}\) keeps unchanged (vector in red color). They say pretrained features are distorted. Consequently, fine-tuning pushes the learned \(w_{ft}\) far away from the true \(w_{*}\) despite that \(w_{ft}\) still yields good performance on in-domain data, but worse performance on out-domain data.</p> <p>To mitigate this distribution shift, author proposes a simple strategy: linear probing then fine-tuning. Linear probing is performed first to get a better initialization of the head layer, then, the whole model parameters are updated with fine-tuning.</p> <p>This paradigm is also beneficial for continual learning to help the model forget less the old tasks. Specifically, for each new task, the head layer is updated first by linear probing while others layers are freezed. Then, the fine-tuning updates all layer’s parameters.</p> </li> <li> <h6 id="can-language-models-solve-graph-problems-in-natural-language-wang-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.10037.pdf">Can Language Models Solve Graph Problems in Natural Language?</a> (Wang et al., arxiv 2023)</h6> <p>LLMs are more and more adopted for tasks involving graphical structures. <em>can LLMs reason with graphs?</em>. The paper introduces the Natural Language Graph (NLGraph) benchmark including 29370 problems encompassing 8 graph reasoning tasks of different complexity. The graph in each problem is generated with controlled complexity level: number of edges, nodes, paths.</p> <p><img src="/assets/img/cheatsheet/nlgraph.png" alt="" style="width: 65%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Observations:</p> <ul> <li>LLms have prilimiary ability to handle simple graph reasoning task like connectivity, cycle and shorted path task. However, they tend to rely on, to some extent, the spurious correlation (e.g. node frequency vs. node connectivity) to make prediction.</li> <li>In-Context Learning with demonstrations underperforms zero-shot learning for complex structured reasoning tasks: hamilton path, bipartie graph matching.</li> <li>Two prompting methods are proposed to improve the graph reasoning: (i) build-a-Graph prompting: append the instructionn: “Let’s construct a graph with the nodes and edges first” to the task description; (ii) algorithmic prompting: append the description of an algorithm that could be employed to reason the graph, such as depth-first-search for the shortest path task.</li> </ul> <p>Above all, the performance of LLM for complex graph structured task remains unsolved.</p> </li> <li> <h6 id="selection-inference-exploiting-large-language-models-for-interpretable-logical-reasoning-creswell-et-al-iclr-2023"><a href="https://openreview.net/pdf?id=3Pf3Wg6o-A4">Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning</a> (Creswell et al., ICLR 2023)</h6> </li> <li> <h6 id="trusting-your-evidence-hallucinate-less-with-context-aware-decoding-shi-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.14739.pdf">Trusting Your Evidence: Hallucinate Less with Context-aware Decoding</a> (Shi et al., arxiv 2023)</h6> <p>LMs sometiems do not pay enough attention to the context given to it and over-rely on prior knowledge it learned in the pretraining. This could be an issue in case there is factual contradict between the prior knowledge and the context.</p> <p><img src="/assets/img/cheatsheet/trust.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>This paper proposes <b>context-aware decoding</b> that integrates into the original output distribution with the pointwise mutual information (PMI) between the context \(c\) and the output \(y\) factoring out the contribution of prior knowledge:</p> \[y_t \sim softmax[(1+\alpha) \, logit_{\theta} (y | c, x) - \alpha \, logit_{\theta} (y | x)]\] <p>where large \(\alpha\) means more attention to the context \(c\). (\(\alpha\) is empirically determined at 0.5)</p> <p><b>context-aware decoding</b> consistenly improve the performance of various LLMs (e.g. LLaMa, FLAN, OPT) on summarization tasks and knowledge conflict related task such as MemoTrap.</p> </li> <li> <h6 id="codet5-open-code-large-language-models-for-code-understanding-and-generation-wang-et-al-arxiv-2023"><a href="https://arxiv.org/pdf/2305.07922.pdf">CodeT5+: Open Code Large Language Models for Code Understanding and Generation</a> (Wang et al., arxiv 2023)</h6> <p><img src="/assets/img/cheatsheet/codet5p.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p><b>CodeT5+</b>, an enhanced version of code language model CodeT5. Through a mixture of pre-training objectives as well as using both unimodal (only code) and bimodal (code-text) corpora, the encoder and decoder have strong representation capability that mitigate the pretrain-finetune discrepancy. Specifically, the pre-training objectives are:</p> <ul> <li>Span Denoising: similar to T5</li> <li>Causal language modeling: (i) generate the second part of a code function given the first part, (ii) generate the whole code function given a special token [CLM]</li> <li>Text-Code Contrastive Learning: to align the representation space of code and text, the encoder is trained with contrastive learning where positive code-text pairs are pulled together and negative ones are pulled apart. [CLS] is appended to the input sequence (code or text) and regarded as the representation of the input.</li> <li>Text-Code Matching: the encoder takes in a text, the decoder takes in a code, a special token [EOS] is appended to the end of the code and its embedding is used to train a binary classifier, predicting whether the text matches (or unmatches) the code</li> <li>Text-Code Causal LM: the encoder takes a text (resp. a code) and the decoder generates the corresponding the code (reps. text).</li> <li>Instruction tuning: to align the model with natural language instructions.</li> </ul> <p>With the intuition that the generation in the decoder may have a higher degree of complexity than the encoding in the encoder, <b>CodeT5+</b> employs “shallow encoder and deep decoder” architecture. Furthermore, for an efficient pretraining, only the encoder’s layers and cross-attention layers are trainable, while the decoder is freezed.</p> </li> <li> <h6 id="emergent-world-representations-exploring-a-sequence-model-trained-on-a-synthetic-task-li-et-al-iclr-2023"><a href="https://openreview.net/pdf?id=DeG07_TcZvT">Emergent World Representations: Exploring a Sequence Model Trained On a Synthetic Task</a> (Li et al., ICLR 2023).</h6> <p>refer to <a href="/blog/2023/representation-probe">blog</a></p> </li> <li> <h6 id="quantifying-memorization-across-neural-language-models-carlini-et-al-iclr-2023"><a href="https://arxiv.org/pdf/2202.07646.pdf">Quantifying Memorization Across Neural Language Models</a> (Carlini et al., ICLR 2023)</h6> <p>Definition of memorization in this paper:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  A training sample s is extractable with k tokens of context from a model f if the model can produce 
  exactly s[k:] using greedy decoding when prompted with s[:k].

</code></pre></div> </div> <p>Several key observations:</p> <ul> <li>Bigger models memorize more. By comparing with a baseline model which has not seen the test data before, they conclude that the model actually memorizes data.</li> <li>It is easier to memorize repeated data.</li> <li>Longer prompt (large k) invoke more memorized data.</li> </ul> </li> </ul> <p><b>2022</b></p> <ul> <li> <h6 id="i2d2-inductive-knowledge-distillation-with-neurologic-and-self-imitation-bhagavatula-et-al"><a href="https://arxiv.org/pdf/2212.09246.pdf">I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation</a> (Bhagavatula et al.):</h6> <p>while common sense statements are simple, clear and short, text generated by small LM can be often trivially long or repetitive. To improve the generation quality, NeuroLogic Decoding enforces logical constraints at decoding time (e.g. limiting number of function words such as “in”, “on”, or excluding connective words such as “although”, “since”, or a given word must be generated).</p> <p><img src="/assets/img/vldb/i2d2.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/> <em>(source: copied from the paper).</em> <em>(source: copied from the paper).</em></p> </li> <li> <h6 id="symbolic-knowledge-distillation-from-general-language-models-to-commonsense-models-west-et-al-naacl-2022"><a href="https://aclanthology.org/2022.naacl-main.341.pdf">Symbolic Knowledge Distillation: from General Language Models to Commonsense Models</a> (West et al., NAACL 2022)</h6> <p>Manually crafting a high-quality common sense knowledge graph to teach a common sense model is expensive, hard to scale, hence, resulting a knowledge repository with limited coverage. Based on the intuition that a LLM (e.g. GPT-3) contains a vast amount of knowledge, but may be noisy and not fully exploitable, authors propose to distill high-quality commonsense knowledge from this LLM (referred to as the teacher model) through prompting (e.g. “X goes jogging. Prerequisites: For this to happen,<em>__</em>”) and filtering the prompt’s results by a small critic model. The critic model is fine-tuned on a set of <em>correct vs. incorrect</em> human judgements on a randomly sampled set of knowledge extracted (but unfiltered) from GPT-3. As a result, for the first time, they obtain <b>ATOMIC</b>, a commonsense KG, automatically distilled from GPT-3, outperforms human-curated KG in three criteria: quantity, quality and diversity.</p> <p><img src="/assets/img/vldb/distill.png" alt="" style="width: 30%; display:block; margin-left:auto; margin-right:auto"/> <em>(source: copied from the paper).</em></p> <p>Subsequently, the distilled commensense KG is employed to train a much smaller model (GPT-2 XL), resulting a knowledge model, <b>COMET_DISTIL</b>, surpassing the commonsense of GPT-3.</p> </li> <li> <h6 id="efficient-training-of-language-models-to-fill-in-the-middle-bavarian-et-al-arxiv-2022"><a href="https://arxiv.org/pdf/2207.14255.pdf">Efficient Training of Language Models to Fill in the Middle</a> (Bavarian et al., arxiv 2022).</h6> <p>Casual decoder-only LLMs (AR) are overwhelming thanks to their superiority in open-ended text generation, in-context learning and pre-training computational efficiency. Unlike encoder-only or encoder-decoder models, AR models are pre-tranined in left-to-right fashion, hindering them from infilling tasks that needs to condition on both prefix and suffix. This paper proposes <b>Fill in the Midle (FIM)</b> pretraining for AR models that improve its infilling capability without compromising its left-to-right generative capability. It is performed by simply breaking the training sample into three pieces, seperated by sentinel token, then concatenate them and feed into the model:</p> <p>\(\textsf{document} \rightarrow \textsf{[PRE] Enc(prefix) [SUF] Enc(suffix)} [MID] Enc(middle) [EOT]\) (PSM style) or \(\textsf{document} \rightarrow \textsf{[SUF] Enc(suffix) [PRE] Enc(prefix)} [MID] Enc(middle) [EOT]\) (SPM style). Token [EOT] is important as it marks a sucessful join of middle span to the suffix/prefix. <b>Fill in the Midle (FIM)</b> is particularly useful for code domain, in applications such as docstring or function argument generation, where the model need to look before and after the point of generation.</p> <p><img src="/assets/img/cheatsheet/fim.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="language-models-of-code-are-few-shot-commonsense-learners-madaan-et-al-emnlp-2022"><a href="https://aclanthology.org/2022.emnlp-main.90.pdf">Language Models of Code are Few-Shot Commonsense Learners</a> (Madaan et al., EMNLP 2022).</h6> <p><b>CoCoGen</b> shows that Code-LLMs outperforms natural-LLMs for structured data-related tasks or structured commonsense reasoning tasks such as graph generation, graph reasoning.</p> <p><img src="/assets/img/cheatsheet/cocogen.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>The intuition is that it is easier and more informative to convert structured data into code rather than serializing it into plain-text. Consequently, this helps to narrow the gap between fine-tuning data and pre-training data in Code-LLMs.</p> <p>Through experiments on script generation task (PROSCRIPT: generate a graph [nodes, edges] given a goal or predict the edge set given the goal and the node set) and entity state tracking task (PROPARA: predict the state of an entity after an action), <b>CoCoGen</b> achieve several remarks:</p> <ul> <li>Few-shot Code-LLMs outperform few-shot NL-LLMs of similar size or fine-tuned-LLMs in all semantic and structural metrics.</li> <li>Impressive performance of Code-LLms in edge-genration task suggest that Code-LLMs are highly capable of capturing structure.</li> <li>Code-LLMs reasons better than NL-LLMs, via tracking better the state of entitiy after a series of actions.</li> <li>Both code-like prompts and Code-LLMs are important for the performance improvement of structured-related task.</li> <li>Prompts that are more similar to the conventions of typical code may benefit more gains.</li> <li>Automatic metrics proposed in CoCoGen (i.e. semantic and structural metrics correlates) with human evaluation.</li> </ul> <p><br/></p> </li> <li> <h6 id="fast-model-editing-at-scale-mitchell-et-al-iclr-2022"><a href="https://arxiv.org/pdf/2110.11309.pdf">Fast Model Editing at Scale</a> (Mitchell et al., ICLR 2022).</h6> </li> <li> <h6 id="locating-and-editing-factual-associations-in-gpt-meng-et-al-neurips-2022"><a href="https://arxiv.org/pdf/2202.05262.pdf">Locating and Editing Factual Associations in GPT</a> (Meng et al., Neurips 2022).</h6> </li> <li> <h6 id="understanding-dataset-difficulty-with-v-usable-information-ethayarajh-et-al-icml-2022"><a href="https://proceedings.mlr.press/v162/ethayarajh22a/ethayarajh22a.pdf">Understanding Dataset Difficulty with V-Usable Information</a> (Ethayarajh et al., ICML 2022).</h6> <p>refer to <a href="/blog/2022/dataset-difficulty">blog</a></p> </li> <li> <h6 id="a-contrastive-framework-for-neural-text-generation-su-et-al-neurips-2022"><a href="https://arxiv.org/pdf/2202.06417.pdf">A Contrastive Framework for Neural Text Generation</a> (Su et al., NeurIPS 2022).</h6> <p>Aiming at avoiding repetition patterns while maintaining semantic coherence in generated text, <b>constrastive search</b> introduces a <em>degeneration penalty</em> into the decoding objective. This <em>degeneration penalty</em> compares the cosine similarity between a token at current decoding step and all generated tokens at previous decoding steps. The closer the token is to precedent decoded text (more likely leading to repetition), the larger the penalty it receives.</p> </li> <li> <h6 id="the-trade-offs-of-domain-adaptation-for-neural-language-models-grangier-et-al-acl-2022"><a href="https://aclanthology.org/2022.acl-long.264.pdf">The Trade-offs of Domain Adaptation for Neural Language Models</a> (Grangier et al., ACL 2022)</h6> <p>This paper provides some evidences using concepts of machine learning theory to support past prevailing empirical practices/observations for domain adaption of LM.</p> <p><b>1. In-domain training</b></p> <p>The loss of fitting a LM to a domain \(\mathcal{D}\) is decomposed as 3 components: \(\mathcal{L} (\theta_D, \mathcal{D}) = \mathcal{L}_H(\mathcal{D}) + \mathcal{L}_{app} (\mathcal{D}, \Theta) + \mathcal{L}_{est} (\mathcal{D}, \Theta, D)\) where</p> <ul> <li>\(\mathcal{L}_H(\mathcal{D})\) is the intrinsic uncertainty of the domain \(\mathcal{D}\) itself.</li> <li>\(\mathcal{L}_{app} (\mathcal{D}, \Theta)\) is the approximation error of using a LM parameterized by \(\theta \in \Theta\) to approximate the true distribution \(P( \bullet \mid \mathcal{D})\) over domain \(\mathcal{D}\): \(\mathcal{L}_{app} (\mathcal{D}, \Theta) = \min_{\theta \in \Theta} \mathcal{L}(\theta; \mathcal{D}) - H(P( \bullet \mid \mathcal{D}))\) where \(\mathcal{L}(\theta; \mathcal{D})\) is the expectation of risk of using LM \(P(\bullet \mid \theta)\) to approximate true distribution \(P(\bullet \mid \mathcal{D})\): \(\mathcal{L}(\theta; \mathcal{D}) = -\sum_{y \in \mathcal{D}} log \; P(y \mid \theta) P(y \mid \mathcal{D})\). Larger model with deeper, wider layers has more capacity, consequently, can reduce this error.</li> <li>\(\mathcal{L}_{est} (\mathcal{D}, \Theta, D)\) is the error of using the LM parameters empirically estimated from a subset \(D \subset \mathcal{D}\) to represent the true distribution \(P(\bullet \mid \mathcal{D})\): \(\mathcal{L}_{est} (\mathcal{D}, \Theta, D) = \mathcal{L} (\theta_D, \mathcal{D}) - \min_{\theta} \mathcal{L}(\theta; \mathcal{D})\) where \(\theta_D = arg \min_{\theta \in \Theta} \mathcal{L} (\theta, D)\).</li> </ul> <p>For a given training set size, increasing the size of the model can decrease \(\mathcal{L}_{app} (\mathcal{D}, \Theta)\) but can increase \(\mathcal{L}_{est} (\mathcal{D}, \Theta, D)\) due to overfiting \(\rightarrow\) \(\mathcal{L}_{app} \; vs. \; \mathcal{L}_{est}\) trade-off or VC-dimension trade-off.</p> <p><b>2. Out-of-domain training</b></p> <p>Given two LMs pretrained on two generic domaines \(\mathcal{D}\) and \(\mathcal{D'}\), which one we should choose to adapt it to a target domain \(\mathcal{T}\) ?. Intuitively, we choose the one whose distribution is closer to the target distribution \(\mathcal{T}\) or KL divergence between two distributions is smaller as the generalization loss of adapting LM parameters \(\theta_D\) estimated from generic domain \(\mathcal{D}\) for \(\mathcal{T}\) is upper-bounded by the KL divergence \(KL(\mathcal{D}, \mathcal{T})\)</p> \[\forall \epsilon, \exists D \subset \mathcal{D}, \mathcal{L}(\theta_D; \mathcal{T}) \leqslant H(\mathcal{T}) + KL(\mathcal{D}, \mathcal{T}) + \epsilon\] <p><b>3. Fine-Tuning &amp; Multitask Learning</b></p> <p>Pre-training a LM on a large out-of-domain corpus \(D\) then fine-tuning it on a small in-domain corpus \(T\) implicitly involves the trade-off between empirical losses over \(T\) and \(D\). This trade-off is controlled by the number of fine-tuning steps \(n_{ft}\): \(\parallel \theta_{ft} - \theta_D \parallel_2 \;\leqslant \lambda n_{ft} g_{max}\) where \(\lambda\) is maximum learning rate, \(g_{max}\) is upper bound of update norm. More fine-tuning steps \(n_{ft}\), larger possible distance between \(\theta_{ft}\) and \(\theta_D\) is, meaning that \(\theta_{ft}\) could be no longer optimal for \(D\) where \(\mathcal{L}(\theta_{ft}; D)\) may be far from the optimum \(\mathcal{L}(\theta_{D}; D)\). For this reason, fine-tuning is also considered as a regularization technique.</p> </li> <li> <h6 id="memorization-without-overfitting-analyzing-the-training-dynamics-of-large-language-models-tirumala-et-al-neurips-2022"><a href="https://arxiv.org/pdf/2205.10770.pdf">Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models</a> (Tirumala et al., Neurips 2022)</h6> <p>The paper presents a large-scale study of the dynamics of memorization over LM training. The metric <em>exact memorization</em> \(M(f)\) of a LM \(f\) is defined as the proportion of times the LM \(f\) predicts the gold token for the masked token in training dataset. Given a threshold \(\tau\), \(T(f, \tau)\) is the minimal number of times (i.e. training epoches) the model \(f\) needs to see each training sample in order to satisfy \(M(f) \geq \tau\).</p> <p>Some empirical findings about \(M(f)\) and \(T(f, \tau)\) are:</p> <ul> <li>Larger causal LMs memorize faster. Smaller masked LMs memorize quicker initially (lower \(\tau\)) and slower in the long run (larger \(\tau\)).</li> <li>The studied memorization occurs before overfitting \(\rightarrow\) overfitting cannot explain why larger models memorize faster.</li> <li>Learning ability of large LMs are less sensitive to learning rate.</li> <li>Prepending a unique identifer to every traing samples leads to faster memorization.</li> <li>LMs memorize nouns, proper nouns, numeral values earlier than adjectives, verbs.</li> <li>The forgetting curve has a lower bound and this value increases as the model become bigger \(\rightarrow\) large models forget less.</li> </ul> <p><br/></p> </li> <li> <h6 id="from-zero-shot-to-few-shot-text-classification-with-setfit-tunstall-et-al-enlsp-at-neurips-2022"><a href="https://arxiv.org/pdf/2209.11055.pdf">From zero-shot to few-shot Text Classification with SetFit</a> (Tunstall et al., ENLSP at Neurips 2022)</h6> <p>SetFit is a few-shot text classifier (e.g. sentiment analysis) based on <a href="https://arxiv.org/abs/1908.10084">Sentence Transformer</a>. Speaking of its performance,</p> <blockquote> <p>With only 8 labeled examples per class on the Customer Reviews (CR) sentiment dataset, SetFit\(_{MPNET}\) (110M parameters) is competitive with fine-tuning RoBERTa Large (355M parameters) on the full training set of 3k examples 🤯. (Source: https://huggingface.co/blog/setfit)</p> </blockquote> <p>In zero-shot setting, we can generate some very simple samples for each classification label (e.g. 8 samples per label) to make it a few-shot learning problem. For example, in the sentiment analysis task, using template “This sentence is about {}”, a positive sample for label “joy” can be “This sentence is about joy”, for label “sadness” can be “This sentence is about sadness”, etc.</p> </li> <li> <h6 id="improving-language-models-by-retrieving-from-trillions-of-token-borgeaud-et-al-icml-2022"><a href="https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf">Improving Language Models by Retrieving from Trillions of Token</a> (Borgeaud et al., ICML 2022)</h6> </li> </ul> <p><b>2021</b></p> <ul> <li> <h6 id="adapt-and-distill-developing-small-fast-and-effective-pretrained-language-models-for-domains-yao-et-al-acl-findings-2021"><a href="https://aclanthology.org/2021.findings-acl.40.pdf">Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains</a> (Yao et al., ACL Findings 2021)</h6> <p>To adapt a general domain LM to a specific domain, it is necessary to augment the original vocabulary with domain-specific subwords or terms (original vocabulary is kept intact). The paper proposes a simple method to determine domain-specific tokens to add to the vocabulary.</p> <p>It assumes that each subword \(x_i\) is independent of another and it is assigned a probability \(p(x_i)\) equal to its frequency in the corpus:</p> <p>\(\forall i \; x_i \in \mathcal{V}, \; \sum_{x_i \in \mathcal{V}} p(x_i) = 1\) where \(\mathcal{V}\) is the vocabulary.</p> <p>and the log probability of a sentence \(x\) consisting of a subword sequence \(x = (x_1,...,x_M)\) is given by: \(P(x) = log \prod_{i=1}^{M} p(x_i) = \sum_{i=1}^{M} log \; p(x_i)\)</p> <p>Given a domain-specific corpus D consisting of \(\mid\)D\(\mid\) sentences, the likelihood of D is calculated as: \(P(D) = \sum_{x \in D} log \; P(x)\).</p> <p>The original vocabulary is iteratively enriched with subwords taken from domain corpus D. At the time step \(i\), a subset of subwords with highest frequency in D is added to the vocabulary, which helps to improve the likelihood \(P(D)\). The procedure continues if the likelihood gain w.r.t. previous time step \(i-1\) is higher than a threshold \(\delta\): \(\frac{P_{i} (D) - P_{i-1} (D)}{P_{i-1} (D)} &gt; \delta\)</p> </li> <li> <h6 id="udalm-unsupervised-domain-adaptation-through-language-modeling-karouzos-et-al-naacl-2021"><a href="https://aclanthology.org/2021.naacl-main.203.pdf">UDALM: Unsupervised Domain Adaptation through Language Modeling</a> (Karouzos et al., NAACL 2021)</h6> <p>This method adapts a general pretrained LM to the target domain distribution in a simple strategy consisting of three steps:</p> <ul> <li>Pre-training LM on general corpus using MLM objective.</li> <li>Continue the pre-trainining on target domain corpus using MLM objective</li> <li> <p>Perform simultaneously/interleavely two supervised fine-tuning task: (i) a supervised task on labelled source domain data (e.g. classification) (ii) MLM task on target domain data. The idea is to avoid the <b>catastrophic forgetting</b> while adapting the general LM to target domain:</p> <p>\(Loss = \lambda Loss_{classification \; task} + (1-\lambda) Loss_{MLM \; task}\).</p> <p>During this process, the samples from two tasks are interleaved in a batch and are fed to the BERT encoder. The value of \(\lambda\) is determined by the proportion of samples of the classification task (i) in the batch.</p> </li> </ul> </li> <li> <h6 id="mauve-measuring-the-gap-between-neural-text-and-human-text-using-divergence-frontiers-pillutla-et-al-neurips-2021"><a href="https://arxiv.org/pdf/2102.01454.pdf">MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers</a> (Pillutla et al., NeurIPS 2021).</h6> <p>Measuring the “true” closeness between the distribution of text generated by a LM and the “true” distribution of human-written text is computationally intractable. Instead, it is approximated by samples from each distribution.</p> <p>\(\textsf{MAUVE}\) measurement metric, based on the KL divergences, quantifies two types of errors (as illustrated in the figure below):</p> <ul> <li><b>Type I error (False Positive)</b>: the model (Q) assigns high probability to texts that are unlikely written by human (P)</li> <li><b>Type II error (False Negative)</b>: the model (Q) can not generate texts (assign low probability) that are likely under human-written text distribution (P).</li> </ul> <p><img src="/assets/img/cheatsheet/mauve.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Theoretically, the <b>Type I error</b> can be represented by the KL divergence between Q and P: <b>KL</b>(Q | P). A text \(x\) receives large penalty if Q(\(x\)) is large but P(\(x\)) is small. Similarly, the <b>Type II error</b> can be represented by the KL divergence between P and Q: <b>KL</b>(P | Q). A text \(x\) receives large penalty if P(\(x\)) is large but Q(\(x\)) is small. However, these two quantity risk to be infinite if the support of P and Q are not identical which is often the case in practice. To overcome this issue, \(\textsf{MAUVE}\) introduces soft measures for the two errors using the mixture distribution: R\(_{\lambda}\) = \(\lambda\) P + \((1-\lambda)\) Q, \(\lambda = (0..1)\), leading to <b>KL</b>(Q | R\(_{\lambda}\)) as <b>soft Type I error</b> at level \(\lambda\) and <b>KL</b>(P | R\(_{\lambda}\)) as <b>soft Type II error</b> at level \(\lambda\). By varying \(\lambda\), we obtain a <em>divergence curve</em> \(\mathcal{C}\)(P, Q) which amounts to the trade-off between <b>Type I error</b> and <b>Type II error</b>:</p> <p>\(\mathcal{C}(P, Q) = \{(exp(-cKL(Q \mid R_{\lambda})), exp(-cKL(P \mid R_{\lambda}))): R_{\lambda} = \lambda P + (1-\lambda) Q, \lambda \in (0,1) \}\) where \(c\) is scaling factor.</p> <p>Likewise the AUROC (area under the receiver operating characteristic) concept in classification problem, \(\textsf{MAUVE}\) metric is the area under the divergence curve.</p> <p><b>How to tractably compute \(KL(Q \mid R_{\lambda})\) and \(KL(P \mid R_{\lambda})\)</b></p> <p>\(N\) samples \(\{x_i\}_{i=1}^N\) are sampled from LM’s distribution Q and \(M\) samples \(\{x'_i\}_{i=1}^M\) are sampled from human text P. Each sample \(x_i\) is encoded by an external LM, yielding its embedding \(LM(x_i)\). Then, \(M+N\) embeddings are jointly quantized into \(k\) histogram bins using \(k\)-mean clustering algorithm, for example. The two distribution P and Q are merely approximated by multinomial distribution of k constant probabilities \(p_1,..,p_k\) where \(p_k (Q) = \frac{\sum_1^{N} \mathbb{I} (\phi(x_i) = k)}{N}\) and \(p_k (P) = \frac{\sum_1^{M} \mathbb{I} (\phi(x'_i) = k)}{M}\), \(\phi(x_i)\) is the bin assignment of the sample \(x_i\).</p> <p>Through thorough experimentations, \(\textsf{MAUVE}\) proves to meet expected behavior of a good measure for open-ended text generation:</p> <ul> <li>Generation length: as the generation length increases, the quality of generated text decreases.</li> <li>Model size: larger model has higher generation quality.</li> <li>Decoding algorithm: consistent with prevail conclusion: greedy \(\prec\) ancestral \(\prec\) nucleus.</li> <li>Embedding scheme and Quantization scheme: robust to different embedding models and quantization algorithms, yielding consistent results.</li> <li>High correlation with human evaluation.</li> </ul> <p><br/></p> </li> <li> <h6 id="simcse-simple-contrastive-learning-of-sentence-embeddings-gao-et-al-emnlp-2021"><a href="https://aclanthology.org/2021.emnlp-main.552">SimCSE: Simple Contrastive Learning of Sentence Embeddings</a> (Gao et al., EMNLP 2021).</h6> <p>Contrastive learning is employed to learn the sentence embedding with a single encoder in unsupervised manner. They use dropout for the generation of positive samples. Specifically, an input sentence is fed to the LM <em>twice</em> with two different dropout masks that will generate a positive pair of sentence representations for the training.</p> <p><img src="/assets/img/cheatsheet/sim_cse.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> <p>Two take-away messages:</p> <ul> <li>dropout as data augmentation for text. Defaut dropout (0.1) from Transformer works best.</li> <li>contrastive learning + dropout helps to evenly distribute learned representations in the embedding space (<em>isotropy</em> or <em>uniformity</em>) and align better embeddings of positive sentence pairs (<em>alignment</em>).</li> </ul> <p><img src="/assets/img/cheatsheet/sim_cse_2.png" alt="" style="width: 30%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="surface-form-competition-why-the-highest-probability-answer-isnt-always-right-holtzman-et-al-emnlp-2021"><a href="https://arxiv.org/pdf/2104.08315.pdf">Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right</a> (Holtzman et al., EMNLP 2021)</h6> <p>The likelihood of a text \(y=y_1, y_2,...,y_n\) (where \(y_i\) is a token in the vocabulary) of length \(n\) given an input text \(x\), is given by a LM:</p> \[p(y \mid x) = \prod_{i=1}^{n} p(y_i \mid x, y_{i-1}...y_1)\] <p>However, in the context of scoring function, the likelihood \(p(y \mid x)\) is not widely used to compare the text \(y\) with other texts \(y'\) given \(x\). Instead, the <em>length-normalized</em> log-likelihood has been standard for this end.</p> \[score \; (y \mid x) = \frac{log \; p(y \mid x)}{n} = \frac{\sum_{i=1}^{n} log \; p(y_i \mid x, y_{i-1}...y_1) }{n}\] <p>This paper investigates an very interesting problem of text scoring function used to determine a prediction \(y\) for an input \(x\) with LM: <b> surface form competition </b>. Specifically, given \(x\), there could be many relevant \(y\)(s) that differ from their surface forms but share the same underlying concept in the context of \(x\). For example, if \(x\) is “Which is the richest country in the world”, then \(y\) could be “USA”, “United States”, “U.S.A” or even “U.S of A”. All those answers should receive high score, however, since they come from the same finite probability mass function \(p(y \mid x)\), they compete each other for how much probability they could get. Due to the different level of popularity of each answer \(y\) in the training corpus, the model tends to allocate much more probability mass to popular “United States” or “USA”, which consequently decrease the amount for rare “U.S of A”.</p> <p><b>Solution</b> Rather than calculating the ranking score \(score \; (y \mid x)\) via \(p(y \mid x)\) which make solutions \(y\) compete each other, the <b>Pointwise Mutual Information (PMI)</b> is leveraged to evaluate the relevance between the input \(x\) and the output \(y\):</p> \[score \; (y \mid x) = \text{PMI}(x, y) = log \frac{p(x,y)}{p(x) \times p(y)} = log \frac{p (x \mid y)}{p(x)}\] <p>While \(p (x)\) is constant w.r.t \(y\) and the probability of surface form \(p (y)\) is factored out in \(\text{PMI}(x, y)\), the ranking of a solution \(y\) relies solely on \(p (x \mid y)\) that does not cause the competition between different \(y\).</p> </li> <li> <h6 id="prefix-tuning-optimizing-continuous-prompts-for-generation-li-et-al-acl-2021-1"><a href="https://aclanthology.org/2021.acl-long.353.pdf">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a> (Li et al., ACL 2021)</h6> <p>Traditional fine-tuning of a LM model for a downstream task involves modifying all the model parameters, consequently, a single set of parameters can just work best for a single task. Inspired by prompting, <b>prefix-tuning</b> freezes the LM parameters and instead prepend to it a sequence of task-specific vectors \(P_{\theta}\) (aka. <em>prefix</em>): \([P_{\theta}; LM_{\phi}]\) that represent the downstream task, we optimize solely the <em>prefix</em> \(P_{\theta}\) using the task’s data to steer the LM to the task.</p> <p>Prefix-tuning brings some advantages:</p> <ul> <li>A single LM is reused across different downstream tasks since its parameters are kept intact \(\rightarrow\) efficient storage.</li> <li>Only the prefix vector corresponding to the downstream task need to be optimized \(\rightarrow\) lightweight fine-tuning: much fewer parameters w.r.t. LM.</li> <li><b>Prefix-tuning can outperform full fine-tuning in low-data setting and have better generalization.</b></li> </ul> <p><img src="/assets/img/cheatsheet/prefix_tuning.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper)</p> </li> <li> <h6 id="the-power-of-scale-for-parameter-efficient-prompt-tuning-lester-et-al-emnlp-2021-1"><a href="https://aclanthology.org/2021.emnlp-main.243.pdf">The Power of Scale for Parameter-Efficient Prompt Tuning</a> (Lester et al., EMNLP 2021)</h6> <p>Similarly to Prefix-Tuning, <b>prompt-tuning</b> learns task-specific “soft-prompts” (embedding) prepended to task-input (prefix) to steer the LM to perform the task without changing its parameters. While Prefix-Tuning prepends prefix activations to every layers in the encoder, <b>prompt-tuning</b> simplifies this by only adding <em>k</em> tunable tokens per downstream task to the input text at the input layer (without further interventions in intermediate layers) \(\rightarrow\) <b>prompt-tuning</b> has less parameters than Prefix-Tuning.</p> <p>In addition, <b>prompt-tuning</b> is based on T5 that they found that prompt-tuning with T5 off-the-shelf as the frozen model is inefficient. T5 is pre-trained exclusively on span corruption marked with unique sentinel tokens. As prompt-tuning does not modify the model parameters, it risks to produce unnaturally sentinel tokens in the output. This issue is easily overcomed by full fine-tuning. For this reason, before performing prompt-tuning, they continue to pre-train T5 with LM objective in order for the model to produce natural text output.</p> <p>Other features of prompt-tuning:</p> <ul> <li>Performance scales with model size: the larger, the better.</li> <li>May improve the roburstness to domain shifts: outperform in-domain fine-tuning on out-of-domain datasets.</li> <li>Efficient prompt ensemble: better than single prompt and parameter-efficient as the core LM is freezed and shared.</li> </ul> </li> </ul> <p><b>2020</b></p> <ul> <li> <h6 id="biomegatron-larger-biomedical-domain-language-model-shin-et-al-emnlp-2020"><a href="https://aclanthology.org/2020.emnlp-main.379.pdf">BioMegatron: Larger Biomedical Domain Language Model</a> (Shin et al., EMNLP 2020)</h6> <p>BioMegatron is a Megatron-LM pretrained on PubMed dataset and/or others general corpus for Biomedical domain.</p> <p>The paper studies the impact of several factors on the performance of both general LM and domain-adapted LM on 3 applications: NER, RE and Q/A in Biomedical domain.</p> <ul> <li>Domain-specific vocabulary is important for NER and RE task as general-vocabulary breaks domain named-entities into sub-words.</li> <li>Q/A: (i) BioMegatron with <b>Bio-vocab</b> finetuned on general SQUAD then on BioASQ results poor results on BioASQ. (ii) larger models tend to perform better.</li> <li>Domain Transfer &amp; Generalization: (i) NER: general LLM with general vocabulary if pre-trained sufficiently on domain-specific corpus can be as good as a LM pre-trained only domain corpus only with general vocabulary. (ii) Q/A: large general LM fine-tuned on BioASQ does not mean better performance. (iii) General-domain Q/A: large BioMegatron performs better than small general LM on general-domain Q/A.</li> </ul> </li> <li> <h6 id="dont-stop-pretraining-adapt-language-models-to-domains-and-tasks-gururangan-et-al-acl-2020"><a href="https://aclanthology.org/2020.acl-main.740">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</a> (Gururangan et al., ACL 2020):</h6> <p>Before fine-tuning, continue pre-training a general pretrained language model (PLM) on in-domain unlabeled data (<b>domain-adaptive pretraining DAPT</b>) or task-specific unlabeled data (<b>task-adaptive pretraining TAPT</b>) can improve the performance of downstream tasks.</p> <p>Some findings from a thorough analysis of domain- and task- adaptive pretraining across 4 domains and 8 downstream task involving both high- and low- resource settings:</p> <ul> <li>Target domain which is more dissimilar to the source domain benefits more the <b>DAPT</b>. The domain dissimilarity can be quantified by the vocabulary overlap.</li> <li>Combined <b>DAPT, then TAPT</b> setting achieves the best performance on all tasks.</li> <li><b>TAPT</b> could be harmful when applied across tasks (i.e. pretrain the LM with unlabeled data of a task, then fine-tune it with data of another task within the same given domain can degrade the performance of later task).</li> <li>In low-resource scenario, augmenting the unlabeled data that aligns with the task distribution is beneficial. One data augmentation approach is to employ an external LM to encode task’s data and in-domain corpus into a shared embedding space, then for each sample in the task’s data, \(k\) candidate samples are selected from the in-domain corpus using k-nearest neighbor search.</li> </ul> <p><br/></p> </li> <li> <h6 id="generalization-through-memorization-nearest-neighbor-language-models-khandelwal-et-al-iclr-2020-1"><a href="https://arxiv.org/pdf/1911.00172.pdf">Generalization through Memorization: Nearest Neighbor Language Models</a> (Khandelwal et al., ICLR 2020):</h6> <p>The paper hypothesizes that the representation learning problem may be easier than the prediction problem. For example, two sentences <em>Dickens is the author of</em> and <em>Dickens wrote</em> will essentially have the same distribution over the next word, even if they do not know what that distribution is. Given a sequence of tokens \(x = (w_1,...,w_{t-1})\), \(k\) nearest neighbors \(\mathcal{N}\) of \(x\) is retrieved from a pre-built catalog \(\mathcal{C}\) by comparing the sentence embedding of each sequence in Eclidean space. Each nearest neighbor \(x_i\) of \(x\) has a next token \(y_i\): \((x_i, y_i) \in \mathcal{N}\). The distribution of the next token \(y\) of \(x\) can be estimated via a simple linear regression: \(p_{kNN} (y \mid x) = \sum_{(x_i, y_i) \in \mathcal{N}} softmax (\mathbb{1}_{y=y_i} exp (-d (\textsf{Emb}(x), \textsf{Emb}(x_i))))\).</p> <p>The LM distribution of a token \(y\) \(p_{LM} (y \mid x)\) given \(x\) is then updated by the nearest neighbor distribution \(p_{kNN} (y \mid x)\): \(p (y \mid x) = \lambda p_{kNN} (y \mid x) + (1-\lambda) p_{LM} (y \mid x)\).</p> <p>Several advantages of nearest neighbor LM:</p> <ul> <li>No additional training required.</li> <li>Long-tail patterns can be explicitly memorized in the pre-built catalog \(\mathcal{C}\) instead of encoded implicitly in model parameters. New domain can be adapted to LM by creating a new catalog for the target domain dataset.</li> <li>\(k\) nearest neighbor search in the embedding space of word sequences can be efficiently done using FAISS index.</li> </ul> </li> </ul> <p><b>2019</b></p> <ul> <li> <h6 id="when-does-label-smoothing-help-müller-et-al-neurips-2019"><a href="https://arxiv.org/abs/1906.02629">When does label smoothing help?.</a> (Müller et al., NeurIPS 2019).</h6> <p>Optimizing cross entropy loss with hard targets (i.e. one-hot encoding labels) can make the model predict a training sample too confidently where the logit predicted for true label is very large comparing with ones predicted for other labels, as a consequence, the softmax function will generate probabilities with huge gap (e.g. 0.99 for target label and ~0.0 for other labels). To alleviate this issue, one solution is to increase the <em>temperature T</em> to smooth out soft-max probabilities. Another solution is: instead of training with one-hot encoded label (e.g. [1, 0, 0]), we use soft label (e.g. [0.9, 0.05, 0.05]) by re-weighing labels with a small added value playing as noise. <b>Note:</b> we shoud not distill knowledge from a teacher model which is trained with label smoothing since it cause accuracy degradation.</p> </li> </ul> <p><b>2016</b></p> <ul> <li> <h6 id="back-translation-improving-neural-machine-translation-models-with-monolingual-data-sennrich-et-al-acl-2016">Back Translation <a href="https://aclanthology.org/P16-1009">Improving Neural Machine Translation Models with Monolingual Data</a> (Sennrich et al., ACL 2016)</h6> <p>Given a text in a known language, we translate it into some other languages and then translate it back to the original language. This will generate synthetic texts that syntactically differ from the input text but have similar semantics. For example, the English sentence “I love watching move” is translated into French: “J’aime regarder un film” then mapped back to English: “I like to watch a movie”.</p> </li> </ul>]]></content><author><name></name></author><category term="NLP,"/><category term="AI"/><category term="research"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Recap of The Very Large Database (VLDB) Conference 2023</title><link href="https://huynhvp.github.io/blog/2023/vldb-report/" rel="alternate" type="text/html" title="Recap of The Very Large Database (VLDB) Conference 2023"/><published>2023-09-01T10:09:00+00:00</published><updated>2023-09-01T10:09:00+00:00</updated><id>https://huynhvp.github.io/blog/2023/vldb-report</id><content type="html" xml:base="https://huynhvp.github.io/blog/2023/vldb-report/"><![CDATA[<hr/> <p><img src="/assets/img/vldb/IMG_1013.jpg" alt="" style="width: 40%; height: 40%; display:block; margin-left:auto; margin-right:auto"/> <em>(photo taken from Capilano Regional Park, North Vancouver, a highly recommended destination for nature lover)</em></p> <p>Last week, I had the opportunity to attend in-person The Very Large Data Base (VLDB) conference, held in Vancouver, Canada, from 28 August to 1 September. This year, the conference brought together over 1000 researchers, practitioners from the Database and NLP communities. With the rapid adoption of Large Language Model (LLM) in multiple fields, discussions on LLM, applications and impacts of LLM on data management drew a substantial audience. This post is a recap of presentation/discussion sessions that I was able to attend, focusing on topics that align with my research interests.</p> <hr/> <p><b>Table of Contents</b></p> <ul id="markdown-toc"> <li><a href="#vldb-by-numbers" id="markdown-toc-vldb-by-numbers"><b>VLDB by numbers</b></a></li> <li><a href="#nlp-community-meets-database-community" id="markdown-toc-nlp-community-meets-database-community"><b>NLP community meets Database community</b></a> <ul> <li><a href="#common-sense-the-dark-matter-of-language-and-intelligence---yejin-chois-keynote" id="markdown-toc-common-sense-the-dark-matter-of-language-and-intelligence---yejin-chois-keynote">Common Sense: the Dark Matter of Language and Intelligence - Yejin Choi’s keynote.</a></li> <li><a href="#language-model-agents-for-building-natural-language-interfaces-to-data---tao-yus-keynote-xlang-nlp-lab-university-of-hong-kong-in-databases-and-large-language-models-llmdb-workshop" id="markdown-toc-language-model-agents-for-building-natural-language-interfaces-to-data---tao-yus-keynote-xlang-nlp-lab-university-of-hong-kong-in-databases-and-large-language-models-llmdb-workshop">Language Model Agents for Building Natural Language Interfaces to Data - Tao Yu’s keynote, XLANG NLP Lab, University of Hong Kong, in Databases and Large Language Models (LLMDB) workshop.</a></li> </ul> </li> <li><a href="#database-community-in-the-era-of-llm" id="markdown-toc-database-community-in-the-era-of-llm"><b>Database Community in the era of LLM</b></a> <ul> <li><a href="#panel-will-llms-reshape-supercharge-or-kill-data-science-" id="markdown-toc-panel-will-llms-reshape-supercharge-or-kill-data-science-">Panel: Will LLMs reshape, supercharge, or kill data science ?</a></li> <li><a href="#the-first-edition-of-databases-and-large-language-models-workshop" id="markdown-toc-the-first-edition-of-databases-and-large-language-models-workshop">The first edition of Databases and Large Language Models workshop</a></li> <li><a href="#knowledge-graph-in-the-era-of-llm" id="markdown-toc-knowledge-graph-in-the-era-of-llm">Knowledge Graph in the era of LLM</a></li> <li><a href="#several-paper-sessions" id="markdown-toc-several-paper-sessions">Several paper sessions</a></li> </ul> </li> <li><a href="#tabular-data-table-is-gaining-attention" id="markdown-toc-tabular-data-table-is-gaining-attention"><b>Tabular Data (table) is gaining attention</b></a></li> </ul> <h3 id="vldb-by-numbers"><b>VLDB by numbers</b></h3> <p><img src="/assets/img/vldb/IMG_0981.jpg" alt="" style="width: 30%;"/> <img src="/assets/img/vldb/IMG_0982.jpg" alt="" style="width: 30%;"/> <img src="/assets/img/vldb/IMG_0983.jpg" alt="" style="width: 30%;"/></p> <ul> <li><b>1066</b> attendees, half of them are from USA/Canada.</li> <li><b>1074</b> papers submitted to the main research track. <b>266</b> accepted paper.</li> <li><b>16.2%</b> of papers are on Machine Learning, AI. (one of two hottest topics, together with Database Engines)</li> <li><b>3</b> keynotes, one from a NLP rockstar <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a> from AI2 (personally, I refresh her scholar profile once per month and read many of her papers).</li> <li><b>15</b> workshops, one focused on <a href="https://haixun.github.io/llmdb/">LLM + Database</a>, one focused on <a href="https://tabular-data-analysis.github.io/tada2023/">Tabular data</a>.</li> <li>Google, Microsoft, Amazon, Salesforce, Huawei, etc have stands at the conference, to showcase their products as well as present job opportunities.</li> </ul> <h3 id="nlp-community-meets-database-community"><b>NLP community meets Database community</b></h3> <h4 id="common-sense-the-dark-matter-of-language-and-intelligence---yejin-chois-keynote">Common Sense: the Dark Matter of Language and Intelligence - Yejin Choi’s keynote.</h4> <p>She gave an insightful talk on the subject: <b>LLMs (e.g. ChatGPT, GPT4) are incredibly powerful, yet surprisingly brittle</b>, makes nonsensical errors or inconsistent answers (e.g. question + negated question get the same answer). She argues this is due to the lack of common sense knowledge (examples of common sense: bird can fly or it’s not ok to keep the fridge door open). While scale people believes this issue could be (easily) fixed when model gets bigger and bigger and consumes more similar training data, she questions why we even need to take that approach when children can naturally acquire such common sense knowledge without reading trillion words. <b>During the keynote, she presented methods/algorithms (as described below) that help build smaller but competitive models, powered with knowledge (“knowledge model”), compared to extreme-scale language models</b>:</p> <ul> <li> <p><a href="https://aclanthology.org/2022.naacl-main.341.pdf">Symbolic Knowledge Distillation</a>: manually crafting a high-quality common sense knowledge graph to teach a common sense model is expensive, hard to scale, hence, resulting a knowledge repository with limited coverage. Based on the intuition that a LLM (e.g. GPT-3) contains a vast amount of knowledge, but may be noisy and not fully exploitable, <a href="https://aclanthology.org/2022.naacl-main.341.pdf">West et al</a> propose to distill high-quality commonsense knowledge from this LLM (referred to as the teacher model) through prompting (e.g. “X goes jogging. Prerequisites: For this to happen,<em>__</em>”) and filtering the prompt’s results by a small critic model. The critic model is fine-tuned on a set of <em>correct vs. incorrect</em> human judgements on a randomly sampled set of knowledge extracted (but unfiltered) from GPT-3. As a result, for the first time, they obtain <b>ATOMIC</b>, a commonsense KG, automatically distilled from GPT-3, outperforms human-curated KG in three criteria: quantity, quality and diversity.</p> <p><img src="/assets/img/vldb/distill.png" alt="" style="width: 30%;"/> <em>(source: copied from the paper).</em></p> <p>Subsequently, the distilled commensense KG is employed to train a much smaller model (GPT-2 XL), resulting a knowledge model, <b>COMET_DISTIL</b>, surpassing the commonsense of GPT-3.</p> </li> <li> <p>Inference-time algorithms: enhancing the common sense capability of LM at inference-time by improving prompting technique or decoding technique, requiring no further fine-tuning.</p> <ul> <li> <p><a href="https://arxiv.org/pdf/2212.09246.pdf">Constrained NeuroLogic decoding</a>: while common sense statements are simple, clear and short, text generated by small LM can be often trivially long or repetitive. To improve the generation quality, NeuroLogic Decoding enforces logical constraints at decoding time (e.g. limiting number of function words such as “in”, “on”, or excluding connective words such as “although”, “since”, or a given word must be generated).</p> <p><img src="/assets/img/vldb/i2d2.png" alt="" style="width: 60%;"/> <em>(source: copied from the paper).</em></p> </li> <li> <p><a href="https://aclanthology.org/2022.emnlp-main.82.pdf">Maieutic Prompting</a>: LLM can generate inconsistent and unreliable explanation when a question and its negated version get the same answer (e.g. “One is a number that comes before zero ? … True” vs. “One is a number that comes after zero ? … True”). They introduce a novel prompting technique, <b> Maieutic Prompting</b> to improve the consistency in LLM’s generation. Inspired by Socratic style of conversation, the inference process exploits the depth of reasoning by asking recursively if a newly generated explanation is logically consistent with its parent (previous) explanation, as illustrated in the figure below:</p> <p><img src="/assets/img/vldb/maieutic.png" alt="" style="width: 50%;"/> <em>(source: copied from the paper).</em></p> </li> </ul> </li> </ul> <h4 id="language-model-agents-for-building-natural-language-interfaces-to-data---tao-yus-keynote-xlang-nlp-lab-university-of-hong-kong-in-databases-and-large-language-models-llmdb-workshop">Language Model Agents for Building Natural Language Interfaces to Data - <a href="https://taoyds.github.io/">Tao Yu</a>’s keynote, XLANG NLP Lab, University of Hong Kong, in Databases and Large Language Models (LLMDB) workshop.</h4> <p>Teaching LLM to use tools (<b>Tools-Augmented LLM</b>) is probably one of the most exciting capabilities of LLM, enabling it to interact with the real world and address some of limitations:</p> <ul> <li>Math capabilities (by calling a calculator).</li> <li>Keep LLM up-to-date with the latest information of real world (by coupling with a search engine).</li> <li> <p>Enhance Interoperability and Trustworthiness (by tracing tool’s operations, or citing sources).</p> <p><img src="/assets/img/vldb/lm_agent.png" alt="" style="width: 50%;"/> <em>(source: copied from author’s slides).</em></p> </li> </ul> <p>XLANG Lab is building such LM Agent, supporting a wide range of data-related tools (Python, SQL, Plot tools, Kaggle tools…). Additionally, as tools involve both text and code, they introduces <b>Lemur-70B</b>, built on top of LLaMa-2, balancing text and code capabilities.</p> <p><img src="/assets/img/vldb/xlang.png" alt="" style="width: 40%;"/> <img src="/assets/img/vldb/lemur.png" alt="" style="width: 30%;"/> <em>(source: copied from author’s slides).</em></p> <h3 id="database-community-in-the-era-of-llm"><b>Database Community in the era of LLM</b></h3> <h4 id="panel-will-llms-reshape-supercharge-or-kill-data-science-">Panel: Will LLMs reshape, supercharge, or kill data science ?</h4> <p>The panel, moderated by Alon Halevey (Meta), featured Yejin Choi (University of Washington and AI2), Avrilia Floratou (Microsoft), Michael Franklin (University of Chicago), Natasha Noy (Google) and Haixun Wang (Instacart).</p> <p>To kick off the discussion, Alon Halevey showcased two examples that GPT-4 performs very well:</p> <ul> <li>He asks GPT-4 to read an uploaded .csv file containing the books that he has ever read and perform several tasks such as: count number of books, deduplicate entries, plot a histogram. He then asks if GPT-4 can suggest some creative new tasks based on that .csv file. GPT-4 did well (although I don’t recall the details)</li> <li>He extracted posts from his friend on facebook over a span of 2 months. He asks GPT-4 to summarize these posts in a structured way by defining a schema and generating attribute values. The results provided by GPT-4 were awesome.</li> </ul> <p>Given the fact that LLMs has been significantly advancing several long-standing challenges that the database community has ben tackling for decades, Alon asks the panel following questions:</p> <p><img src="/assets/img/vldb/panel_1.jpeg" alt="" style="width: 40%;"/> (from Halevy) <img src="/assets/img/vldb/panel_2.jpeg" alt="" style="width: 40%;"/> (from Floratou)</p> <p>Several thoughts from the panel list:</p> <ul> <li>Writing manually SQL stuffs will go way.</li> <li>LLM changes remarkably the search engine (understand the intent and give good answer). Past trends go from unstructured data to structured data via ETL, Wrangling. Present trends: structured data –&gt; un-structured data with LLM.</li> <li>LLM has issue with latency, data freshness.</li> <li>LLM has no exact durable memory, while database does.</li> <li>Replacing .csv, .yaml by .txt is challenging.</li> <li>Add whole database on-the-fly is expensive –&gt; Tools-Augmented LLMs.</li> </ul> <h4 id="the-first-edition-of-databases-and-large-language-models-workshop">The first edition of <a href="Databases and Large Language Models">Databases and Large Language Models workshop</a></h4> <p>This workshop has emerged to meet the “urgent and exiting” need of integrating the impressive capabilities of LLMs into the real-world data management applications.</p> <ul> <li> <p><b>Wang-Chiew Tan (Meta)’s keynote</b>: Using LLMs to Query Unstructured and Structured Data (similar to Alon Halevey (Meta)’s keynote at <a href="https://tabular-data-analysis.github.io">Tabular Data Analysis Workshop</a>)</p> <p>Personal data records valuable information (e.g. health, activities, hobby, etc) throughout one’s life.</p> <p><img src="/assets/img/vldb/meta_1.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p><em>(source: copied from the slides)</em>.</p> <p>This talk presents interesting opportunities of leveraging LLMs to interact with our personal timeline data (whether structured or not), thereby providing new user experience. As LLMs know nothing about us, there are two ways to query timeline data in natural language using LLMs:</p> <ul> <li> <p>Retrieval Augmented LM (RAG) (Figure a): LM consults an external repository that stores personal data to answer the question. <b>The speaker argues that this scheme does not work well on complex/multi-hop questions</b>.</p> <p><img src="/assets/img/vldb/meta_2.png" alt="" style="width: 30%"/> <em>(source: copied from the slides)</em>.</p> </li> <li> <p>Tool Augmented LM (Figure b): LM converts the question into SQL and call a SQl-engine to execute the SQL over database of personal data.</p> <p><img src="/assets/img/vldb/meta_3.png" alt="" style="width: 40%"/> <img src="/assets/img/vldb/meta_4.png" alt="" style="width: 40%"/> <em>(source: copied from the slides)</em>.</p> </li> </ul> </li> <li> <p><b>Laurel Orr (Numbers Station)’s keynote</b>: Deploying LLMs on Structured Data Tasks: Lessons from the Trenches</p> <p>She talked about her experiences when it comes to deploying LLMs applications for Structured data Wrangling in production, at <a href="https://www.numbersstation.ai/">Numbers Station</a>.</p> <p><img src="/assets/img/vldb/number_3.jpeg" alt="" style="width: 30%"/> <em>(source: copied from the slides)</em>.</p> <p>In a nutshell, she frames “LLMs for Data Wrangling in Production” as: “Possible, Exciting, Not Easy”, illustrated by three challenges:</p> <ul> <li>(a) - High Cost: real relational database is huge, put it all in the context is tricky. Solution: Tools-augmented LLMs use tools to interact with database.</li> <li>(b) - Lacking Personalization: General LLMs lack required enterprise knowledge. Solution: fine-tune LLMs with enterprise data.</li> <li> <p>(c) - Not enough context.</p> <p>(a) <img src="/assets/img/vldb/number_1.jpeg" alt="" style="width: 30%"/> (b) <img src="/assets/img/vldb/number_2.jpeg" alt="" style="width: 30%"/> (c) <img src="/assets/img/vldb/IMG_1030.jpeg" alt="" style="width: 30%"/></p> <p><em>(source: copied from the slides)</em>.</p> </li> </ul> </li> </ul> <h4 id="knowledge-graph-in-the-era-of-llm">Knowledge Graph in the era of LLM</h4> <p>Xin Luna Dong (Meta) was awarded the 2023 VLDB Women in Database Research Award for her significant contributions to knowledge graph construction and data integration. You can refer to her <a href="https://arxiv.org/pdf/2308.14217.pdf">vision</a> on the next generations of KG, leveraging the recent big success of LLMs.</p> <h4 id="several-paper-sessions">Several paper sessions</h4> <ul> <li> <p><a href="https://www.vldb.org/pvldb/vol16/p3302-fernandez.pdf">How Large Language Models Will Disrupt Data Management</a> (University of Chicago):</p> <p><img src="/assets/img/vldb/IMG_0993.jpg" alt="" style="width: 30%"/> <em>(source: copied from the slides)</em>.</p> </li> <li><a href="https://www.vldb.org/pvldb/vol16/p738-narayan.pdf">Can Foundation Models Wrangle Your Data?</a> (Stanford): LLMs show strong zero-shot and few-shot capability in data wrangling tasks such as entity matching, error detection and data imputation. A benchmark for this topic will soon be integrated into the famous <a href="https://crfm.stanford.edu/helm/latest/">LLM HELM benchmark</a>.</li> <li><a href="https://www.vldb.org/pvldb/vol16/p1534-fu.pdf">CatSQL: Towards Real World Natural Language to SQL Applications</a> (Alibaba Group): new state-of-the-art natural language to SQL converter on Spider benchmark.</li> </ul> <h3 id="tabular-data-table-is-gaining-attention"><b>Tabular Data (table) is gaining attention</b></h3> <ul> <li> <p>Together with <a href="https://table-representation-learning.github.io">Table Representation Learning Workshop</a>, co-located with Neurips 2023 and <a href="https://sem-tab-challenge.github.io/2023/">Semantic Web Challenge on Tabular data to Knowledge KG Matching</a>, co-located with ISWC 2023, this year’s VLDB conference organized <a href="https://tabular-data-analysis.github.io">Tabular Data Analysis Workshop</a>. The workshop featured two keynote speakers: (i) Renée Miller (Northeastern University) on Table Discovery and Integration from Data Lake of tables and (ii) Alon Halevey (Meta) on the potential of LLM of interfacing a database with natural language query.</p> </li> <li>As part of Renée Miller’s keynote, she presented two papers accepted at VLDB: <ul> <li><a href="https://arxiv.org/pdf/2210.01922.pdf">Semantics-aware Dataset Discovery from Data Lakes with Contextualized Column-based Representation Learning</a>: <b>table union search</b> involves searching for tables in a data lake that are semantically close to a given table by evaluating the similarity between column embeddings in two tables. They employ the popular contrastive self-supervised training to learn the embeddings of table columns. The key point lies in data augmentation for the training: given a input table \(x\), applying transformation operators \(f\) such as cell dropping, cell swapping, row shuffling to \(x\) yields a positive training sample (\(x\), \(f(x)\)).</li> </ul> <p><img src="/assets/img/vldb/alite_1.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p><em>(source: copied from the paper)</em>.</p> <ul> <li><a href="http://vldb.org/pvldb/vol16/p932-khatiwada.pdf">Integrating Data Lake Tables</a>: presents different techniques for integrating relevant tables discovered from data lake into a single table.</li> </ul> <p><img src="/assets/img/vldb/alite_2.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p><em>(source: copied from the paper)</em>.</p> <p>Her talks attracted a lot of discussion among researchers in the Semantic Web community. They agreed that adding a semantic layer on top of table (via Semantic Table Interpretation) would facilitate and improve the discovery and integration of data lake tables. Several related papers have been presented during the conference:</p> <ul> <li><a href="https://www.vldb.org/pvldb/vol16/p1319-sun.pdf">RECA: Related Tables Enhanced Column Semantic Type Annotation Framework</a> (The University of Hong Kong).</li> <li><a href="https://ceur-ws.org/Vol-3462/TADA1.pdf">Column Type Annotation using ChatGPT</a> (University of Mannheim).</li> <li><a href="https://ceur-ws.org/Vol-3462/TADA7.pdf">Towards Generative Semantic Table Interpretation</a> (Orange + EURECOM).</li> <li></li> </ul> </li> <li> <p><b>VLDB Best paper award</b>: <a href="https://www.vldb.org/pvldb/vol16/p3391-he.pdf">Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples</a> (Georgia Tech &amp; Microsoft):</p> <p>Non-relational tables, despite of being overwhelming on the wild web, are not easy to be queried using SQL-based tools. Transforming non-relational tables into standard relational table (Figure b) is a non-trivial task and has been a longstanding challenge in the database community (Figure a).</p> <p>(a) <img src="/assets/img/vldb/auto_table_1.png" alt="" style="width: 30%"/> (b) <img src="/assets/img/vldb/auto_table_2.png" alt="" style="width: 60%"/></p> <p><em>(source: copied from the paper)</em>.</p> <p>This work proposes <b>Auto-Tables</b>, a pipeline for transforming non-relational tables into standard relational table automatically. The process involves the use table transformation operators such as transpose, stacking, splitting, etc to generate a self-supervised training dataset which will be used to train a deep neural network.</p> </li> </ul>]]></content><author><name></name></author><category term="NLP,"/><category term="Database"/><category term="research"/><summary type="html"><![CDATA[(photo taken from Capilano Regional Park, North Vancouver, a highly recommended destination for nature lover) Last week, I had the opportunity to attend in-person The Very Large Data Base (VLDB) conference, held in Vancouver, Canada, from 28 August to 1 September. This year, the conference brought together over 1000 researchers, practitioners from the Database and NLP communities. With the rapid adoption of Large Language Model (LLM) in multiple fields, discussions on LLM, applications and impacts of LLM on data management drew a substantial audience. This post is a recap of presentation/discussion sessions that I was able to attend, focusing on topics that align with my research interests. Table of Contents VLDB by numbers NLP community meets Database community Common Sense: the Dark Matter of Language and Intelligence - Yejin Choi’s keynote. Language Model Agents for Building Natural Language Interfaces to Data - Tao Yu’s keynote, XLANG NLP Lab, University of Hong Kong, in Databases and Large Language Models (LLMDB) workshop. Database Community in the era of LLM Panel: Will LLMs reshape, supercharge, or kill data science ? The first edition of Databases and Large Language Models workshop Knowledge Graph in the era of LLM Several paper sessions Tabular Data (table) is gaining attention VLDB by numbers 1066 attendees, half of them are from USA/Canada. 1074 papers submitted to the main research track. 266 accepted paper. 16.2% of papers are on Machine Learning, AI. (one of two hottest topics, together with Database Engines) 3 keynotes, one from a NLP rockstar Yejin Choi from AI2 (personally, I refresh her scholar profile once per month and read many of her papers). 15 workshops, one focused on LLM + Database, one focused on Tabular data. Google, Microsoft, Amazon, Salesforce, Huawei, etc have stands at the conference, to showcase their products as well as present job opportunities. NLP community meets Database community Common Sense: the Dark Matter of Language and Intelligence - Yejin Choi’s keynote. She gave an insightful talk on the subject: LLMs (e.g. ChatGPT, GPT4) are incredibly powerful, yet surprisingly brittle, makes nonsensical errors or inconsistent answers (e.g. question + negated question get the same answer). She argues this is due to the lack of common sense knowledge (examples of common sense: bird can fly or it’s not ok to keep the fridge door open). While scale people believes this issue could be (easily) fixed when model gets bigger and bigger and consumes more similar training data, she questions why we even need to take that approach when children can naturally acquire such common sense knowledge without reading trillion words. During the keynote, she presented methods/algorithms (as described below) that help build smaller but competitive models, powered with knowledge (“knowledge model”), compared to extreme-scale language models: Symbolic Knowledge Distillation: manually crafting a high-quality common sense knowledge graph to teach a common sense model is expensive, hard to scale, hence, resulting a knowledge repository with limited coverage. Based on the intuition that a LLM (e.g. GPT-3) contains a vast amount of knowledge, but may be noisy and not fully exploitable, West et al propose to distill high-quality commonsense knowledge from this LLM (referred to as the teacher model) through prompting (e.g. “X goes jogging. Prerequisites: For this to happen,__”) and filtering the prompt’s results by a small critic model. The critic model is fine-tuned on a set of correct vs. incorrect human judgements on a randomly sampled set of knowledge extracted (but unfiltered) from GPT-3. As a result, for the first time, they obtain ATOMIC, a commonsense KG, automatically distilled from GPT-3, outperforms human-curated KG in three criteria: quantity, quality and diversity. (source: copied from the paper). Subsequently, the distilled commensense KG is employed to train a much smaller model (GPT-2 XL), resulting a knowledge model, COMET_DISTIL, surpassing the commonsense of GPT-3. Inference-time algorithms: enhancing the common sense capability of LM at inference-time by improving prompting technique or decoding technique, requiring no further fine-tuning. Constrained NeuroLogic decoding: while common sense statements are simple, clear and short, text generated by small LM can be often trivially long or repetitive. To improve the generation quality, NeuroLogic Decoding enforces logical constraints at decoding time (e.g. limiting number of function words such as “in”, “on”, or excluding connective words such as “although”, “since”, or a given word must be generated). (source: copied from the paper). Maieutic Prompting: LLM can generate inconsistent and unreliable explanation when a question and its negated version get the same answer (e.g. “One is a number that comes before zero ? … True” vs. “One is a number that comes after zero ? … True”). They introduce a novel prompting technique, Maieutic Prompting to improve the consistency in LLM’s generation. Inspired by Socratic style of conversation, the inference process exploits the depth of reasoning by asking recursively if a newly generated explanation is logically consistent with its parent (previous) explanation, as illustrated in the figure below: (source: copied from the paper). Language Model Agents for Building Natural Language Interfaces to Data - Tao Yu’s keynote, XLANG NLP Lab, University of Hong Kong, in Databases and Large Language Models (LLMDB) workshop. Teaching LLM to use tools (Tools-Augmented LLM) is probably one of the most exciting capabilities of LLM, enabling it to interact with the real world and address some of limitations: Math capabilities (by calling a calculator). Keep LLM up-to-date with the latest information of real world (by coupling with a search engine). Enhance Interoperability and Trustworthiness (by tracing tool’s operations, or citing sources). (source: copied from author’s slides). XLANG Lab is building such LM Agent, supporting a wide range of data-related tools (Python, SQL, Plot tools, Kaggle tools…). Additionally, as tools involve both text and code, they introduces Lemur-70B, built on top of LLaMa-2, balancing text and code capabilities. (source: copied from author’s slides). Database Community in the era of LLM Panel: Will LLMs reshape, supercharge, or kill data science ? The panel, moderated by Alon Halevey (Meta), featured Yejin Choi (University of Washington and AI2), Avrilia Floratou (Microsoft), Michael Franklin (University of Chicago), Natasha Noy (Google) and Haixun Wang (Instacart). To kick off the discussion, Alon Halevey showcased two examples that GPT-4 performs very well: He asks GPT-4 to read an uploaded .csv file containing the books that he has ever read and perform several tasks such as: count number of books, deduplicate entries, plot a histogram. He then asks if GPT-4 can suggest some creative new tasks based on that .csv file. GPT-4 did well (although I don’t recall the details) He extracted posts from his friend on facebook over a span of 2 months. He asks GPT-4 to summarize these posts in a structured way by defining a schema and generating attribute values. The results provided by GPT-4 were awesome. Given the fact that LLMs has been significantly advancing several long-standing challenges that the database community has ben tackling for decades, Alon asks the panel following questions: (from Halevy) (from Floratou) Several thoughts from the panel list: Writing manually SQL stuffs will go way. LLM changes remarkably the search engine (understand the intent and give good answer). Past trends go from unstructured data to structured data via ETL, Wrangling. Present trends: structured data –&gt; un-structured data with LLM. LLM has issue with latency, data freshness. LLM has no exact durable memory, while database does. Replacing .csv, .yaml by .txt is challenging. Add whole database on-the-fly is expensive –&gt; Tools-Augmented LLMs. The first edition of Databases and Large Language Models workshop This workshop has emerged to meet the “urgent and exiting” need of integrating the impressive capabilities of LLMs into the real-world data management applications. Wang-Chiew Tan (Meta)’s keynote: Using LLMs to Query Unstructured and Structured Data (similar to Alon Halevey (Meta)’s keynote at Tabular Data Analysis Workshop) Personal data records valuable information (e.g. health, activities, hobby, etc) throughout one’s life. (source: copied from the slides). This talk presents interesting opportunities of leveraging LLMs to interact with our personal timeline data (whether structured or not), thereby providing new user experience. As LLMs know nothing about us, there are two ways to query timeline data in natural language using LLMs: Retrieval Augmented LM (RAG) (Figure a): LM consults an external repository that stores personal data to answer the question. The speaker argues that this scheme does not work well on complex/multi-hop questions. (source: copied from the slides). Tool Augmented LM (Figure b): LM converts the question into SQL and call a SQl-engine to execute the SQL over database of personal data. (source: copied from the slides). Laurel Orr (Numbers Station)’s keynote: Deploying LLMs on Structured Data Tasks: Lessons from the Trenches She talked about her experiences when it comes to deploying LLMs applications for Structured data Wrangling in production, at Numbers Station. (source: copied from the slides). In a nutshell, she frames “LLMs for Data Wrangling in Production” as: “Possible, Exciting, Not Easy”, illustrated by three challenges: (a) - High Cost: real relational database is huge, put it all in the context is tricky. Solution: Tools-augmented LLMs use tools to interact with database. (b) - Lacking Personalization: General LLMs lack required enterprise knowledge. Solution: fine-tune LLMs with enterprise data. (c) - Not enough context. (a) (b) (c) (source: copied from the slides). Knowledge Graph in the era of LLM Xin Luna Dong (Meta) was awarded the 2023 VLDB Women in Database Research Award for her significant contributions to knowledge graph construction and data integration. You can refer to her vision on the next generations of KG, leveraging the recent big success of LLMs. Several paper sessions How Large Language Models Will Disrupt Data Management (University of Chicago): (source: copied from the slides). Can Foundation Models Wrangle Your Data? (Stanford): LLMs show strong zero-shot and few-shot capability in data wrangling tasks such as entity matching, error detection and data imputation. A benchmark for this topic will soon be integrated into the famous LLM HELM benchmark. CatSQL: Towards Real World Natural Language to SQL Applications (Alibaba Group): new state-of-the-art natural language to SQL converter on Spider benchmark. Tabular Data (table) is gaining attention Together with Table Representation Learning Workshop, co-located with Neurips 2023 and Semantic Web Challenge on Tabular data to Knowledge KG Matching, co-located with ISWC 2023, this year’s VLDB conference organized Tabular Data Analysis Workshop. The workshop featured two keynote speakers: (i) Renée Miller (Northeastern University) on Table Discovery and Integration from Data Lake of tables and (ii) Alon Halevey (Meta) on the potential of LLM of interfacing a database with natural language query. As part of Renée Miller’s keynote, she presented two papers accepted at VLDB: Semantics-aware Dataset Discovery from Data Lakes with Contextualized Column-based Representation Learning: table union search involves searching for tables in a data lake that are semantically close to a given table by evaluating the similarity between column embeddings in two tables. They employ the popular contrastive self-supervised training to learn the embeddings of table columns. The key point lies in data augmentation for the training: given a input table \(x\), applying transformation operators \(f\) such as cell dropping, cell swapping, row shuffling to \(x\) yields a positive training sample (\(x\), \(f(x)\)). (source: copied from the paper). Integrating Data Lake Tables: presents different techniques for integrating relevant tables discovered from data lake into a single table. (source: copied from the paper). Her talks attracted a lot of discussion among researchers in the Semantic Web community. They agreed that adding a semantic layer on top of table (via Semantic Table Interpretation) would facilitate and improve the discovery and integration of data lake tables. Several related papers have been presented during the conference: RECA: Related Tables Enhanced Column Semantic Type Annotation Framework (The University of Hong Kong). Column Type Annotation using ChatGPT (University of Mannheim). Towards Generative Semantic Table Interpretation (Orange + EURECOM). VLDB Best paper award: Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples (Georgia Tech &amp; Microsoft): Non-relational tables, despite of being overwhelming on the wild web, are not easy to be queried using SQL-based tools. Transforming non-relational tables into standard relational table (Figure b) is a non-trivial task and has been a longstanding challenge in the database community (Figure a). (a) (b) (source: copied from the paper). This work proposes Auto-Tables, a pipeline for transforming non-relational tables into standard relational table automatically. The process involves the use table transformation operators such as transpose, stacking, splitting, etc to generate a self-supervised training dataset which will be used to train a deep neural network.]]></summary></entry><entry><title type="html">Table Representation Learning with Transformer</title><link href="https://huynhvp.github.io/blog/2023/trl/" rel="alternate" type="text/html" title="Table Representation Learning with Transformer"/><published>2023-08-05T00:09:00+00:00</published><updated>2023-08-05T00:09:00+00:00</updated><id>https://huynhvp.github.io/blog/2023/trl</id><content type="html" xml:base="https://huynhvp.github.io/blog/2023/trl/"><![CDATA[<hr/> <p>Tabular data (aka. table) is one of the most prevalent data structures used to store and present information on the web or in industry. Table contains rich and meaningful structural and semantc information. Making table machine-understandable is necessary to facilitate a wide range of applications related to table such as table-based question answering (an example shown below), fact-checking, table indexing/search/retrieval or table content imputation.</p> <p><img src="/assets/img/trl/tapas_1.png" alt="" style="width: 80%; display:block; margin-left:auto; margin-right:auto"/></p> <p><em>(source: <a href="https://arxiv.org/pdf/2004.02349.pdf">TAPAS (ACL 2020)</a>)</em>.</p> <p>The emergence of Transformer-based Large Language Models (LLM) has revolutionized various natural language understanding tasks. As the massive corpora used to pre-train LLMs contains most of free-form text, or programming code, many works investigate and extents the applicability of LLMs to structured data like table. This post gathers prominent transformer models for learning the neural representation of tabular data.</p> <p><em>A survey on this emerging topic can be found at <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00544/115239/Transformers-for-Tabular-Data-Representation-A">Transformers for Tabular Data Representation: A Survey of Models and Applications, TACL 2022</a>.</em></p> <hr/> <p><b>Table of Contents</b></p> <ul id="markdown-toc"> <li><a href="#large-language-models-are-versatile-decomposers-decomposing-evidence-and-questions-for-table-based-reasoning-ye-sigir-2023" id="markdown-toc-large-language-models-are-versatile-decomposers-decomposing-evidence-and-questions-for-table-based-reasoning-ye-sigir-2023">Large Language Models are Versatile Decomposers: Decomposing Evidence and Questions for Table-based Reasoning (Ye, SIGIR 2023)</a></li> <li><a href="#large-language-models-are-few1-shot-table-reasoners-chen-findings-of-eacl-2023" id="markdown-toc-large-language-models-are-few1-shot-table-reasoners-chen-findings-of-eacl-2023">Large Language Models are few(1)-shot Table Reasoners (Chen, Findings of EACL 2023)</a></li> <li><a href="#tableformer-robust-transformer-modeling-for-table-text-encoding-yang-acl-2022" id="markdown-toc-tableformer-robust-transformer-modeling-for-table-text-encoding-yang-acl-2022">TABLEFORMER: Robust Transformer Modeling for Table-Text Encoding (Yang, ACL 2022)</a></li> <li><a href="#reastap-injecting-table-reasoning-skills-during-pre-training-via-synthetic-reasoning-examples-zhao-emnlp-2022" id="markdown-toc-reastap-injecting-table-reasoning-skills-during-pre-training-via-synthetic-reasoning-examples-zhao-emnlp-2022">REASTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples (Zhao, EMNLP 2022)</a></li> <li><a href="#tapex-table-pre-training-via-learning-a-neural-sql-executor-liu-iclr-2022" id="markdown-toc-tapex-table-pre-training-via-learning-a-neural-sql-executor-liu-iclr-2022">TAPEX: Table Pre-training via Learning a Neural SQL Executor (Liu, ICLR 2022)</a></li> <li><a href="#tabbie-pretrained-representations-of-tabular-data-lida-naacl-2021" id="markdown-toc-tabbie-pretrained-representations-of-tabular-data-lida-naacl-2021">TABBIE: Pretrained Representations of Tabular Data (Lida, NAACL 2021)</a></li> <li><a href="#grappa-grammar-augmented-pre-training-for-table-semantic-parsing-yu-iclr-2021" id="markdown-toc-grappa-grammar-augmented-pre-training-for-table-semantic-parsing-yu-iclr-2021">GRAPPA: Grammar-Augmented Pre-Training For Table Semantic Parsing (Yu, ICLR 2021)</a></li> <li><a href="#tabert-pretraining-for-joint-understanding-of-textual-and-tabular-data-yin-acl-2020" id="markdown-toc-tabert-pretraining-for-joint-understanding-of-textual-and-tabular-data-yin-acl-2020">TABERT: Pretraining for Joint Understanding of Textual and Tabular Data (Yin, ACL 2020)</a></li> <li><a href="#tapas-weakly-supervised-table-parsing-via-pre-training-herzig-acl-2020" id="markdown-toc-tapas-weakly-supervised-table-parsing-via-pre-training-herzig-acl-2020">TAPAS: Weakly Supervised Table Parsing via Pre-training (Herzig, ACL 2020)</a></li> <li><a href="#turl-table-understanding-through-representation-learning-deng-vldb-2020" id="markdown-toc-turl-table-understanding-through-representation-learning-deng-vldb-2020">TURL: Table Understanding through Representation Learning (Deng, VLDB 2020)</a></li> </ul> <h6 id="large-language-models-are-versatile-decomposers-decomposing-evidence-and-questions-for-table-based-reasoning-ye-sigir-2023"><a href="https://arxiv.org/abs/2301.13808">Large Language Models are Versatile Decomposers: Decomposing Evidence and Questions for Table-based Reasoning (Ye, SIGIR 2023)</a></h6> <p>DATER employs LLM to tackle table-related tasks with in-context learning in two steps:</p> <ul> <li>Evidence decomposition (sub-table extraction): use prompts to ask LLM to extract a small part in the table (set of rows/columns) that is relevant to the question.</li> <li>Question decomposition: decompose the input question into sub-questions in chain-of-though style. First, mask numerical values in question, next convert into SQL queries. The SQL queries are then executed using any SQL engine, the output will be backfilled into placeholders of mask tokens, yeilding sub-questions for the initial question.</li> </ul> <p><img src="/assets/img/trl/dater.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <h6 id="large-language-models-are-few1-shot-table-reasoners-chen-findings-of-eacl-2023"><a href="https://aclanthology.org/2023.findings-eacl.83/">Large Language Models are few(1)-shot Table Reasoners (Chen, Findings of EACL 2023)</a></h6> <p>Although not explicitly pre-trained to encode tabular data, LLM may have seen many tables in the pre-training corpus. The paper demonstrates that LLM (i.e. GPT-3) has strong 1-shot performance (comparable to smaller fine-tuned models) on several table-related tasks (i.e. table-based QA, table-based fact checking).</p> <p><img src="/assets/img/trl/TableCoT.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <h6 id="tableformer-robust-transformer-modeling-for-table-text-encoding-yang-acl-2022"><a href="https://aclanthology.org/2022.acl-long.40.pdf">TABLEFORMER: Robust Transformer Modeling for Table-Text Encoding (Yang, ACL 2022)</a></h6> <p>Many previous LMs for table representation learning encodes row/column indices via sentinel tokens appended to the linearized input. This causes unwanted attention bias. For example, the model may favor the last row to make prediction as it observed frequently the spurious correlation between the last row and the label during the pretraining. Consequently, when the table rows are shuffled, models make wrong prediction although this operation does not change the overall semantics of the table. TableFormer alleviates this issue by removing absolute row_id and column_id embeddings, instead, it relies on relative positional embeddings to model intra/inter-cell, header-cell, utterance-cell interaction in text-table pairs such as: whether two cells are in same row, or in same column, whether a header and a cell are in same column, etc.</p> <p><img src="/assets/img/trl/table_former.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <h6 id="reastap-injecting-table-reasoning-skills-during-pre-training-via-synthetic-reasoning-examples-zhao-emnlp-2022"><a href="https://aclanthology.org/2022.emnlp-main.615.pdf">REASTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples (Zhao, EMNLP 2022)</a></h6> <p>Similar to TAPEX, REASTAP demonstrates that LM can perform reasoning skills over tables without the need of a complex table-specific architecture design (e.g. no need to add a bunch of sentinel tokens to identify table’s components, or modify the original attention mechanism). To this end, REASTAP pre-trains an autoregressive model (i.e. BART) on 7 basic table pre-train tasks. All the tasks are framed flexibly as text-2-text problems. For each task, several templates are predefined and used to genetate synthetic training samples:</p> <p><img src="/assets/img/trl/reastap.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <h6 id="tapex-table-pre-training-via-learning-a-neural-sql-executor-liu-iclr-2022"><a href="https://arxiv.org/abs/2107.07653">TAPEX: Table Pre-training via Learning a Neural SQL Executor (Liu, ICLR 2022)</a></h6> <p>The intuition of TAPEX is that if a LM can perform reasoning over a table via SQL queries, then it should have deep understanding of the table. To this end, TAPEX continue to pre-train a encoder-decoder LM (i.e. BART) on synthetic SQL-table queries with controlled quality/diversity, following the prevalent text-to-text framework:</p> <ul> <li>input: linearized SQL queries + linearized table (\([HEAD], header_1,...,header_N, [ROW], 1, cell_{11},..., cell_{1N}\))</li> <li>output: answer for the input query.</li> </ul> <p><img src="/assets/img/trl/tapex_1.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>With the versatile encoder-decoder architecture, TAPEX is easily fine-tuned to adapt to numerous downstream tasks by framing the tasks as text-to-text problems.</p> <h6 id="tabbie-pretrained-representations-of-tabular-data-lida-naacl-2021"><a href="https://aclanthology.org/2021.naacl-main.270/">TABBIE: Pretrained Representations of Tabular Data (Lida, NAACL 2021)</a></h6> <p>TABBIE hypothizes that if a LM can detect a corrupted cell text, it may have the capacity to understand table structure: row/column seperators or cell boundaries. Without assuming the existence of other information than the table itself, TABBIE corrupts several cells in the table and train the LM to classifier whether every cell has been corrupted or not (similar to ELECTRA’s discriminator). TABBIE employs two seperate Transformer to encode rows (<em>row</em> transformer) and columns (<em>column</em> transformer). The representation of cell \(c_{i,j}\) is an average of the row \(i\) embedding and column \(j\) embedding. The cell embedding is initialized by the embedding of the cell text produced by a freezed BERT, added to two learnable positional embeddings (row and column).</p> <p><img src="/assets/img/trl/tabbie.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <h6 id="grappa-grammar-augmented-pre-training-for-table-semantic-parsing-yu-iclr-2021"><a href="https://arxiv.org/abs/2009.13845">GRAPPA: Grammar-Augmented Pre-Training For Table Semantic Parsing (Yu, ICLR 2021)</a></h6> <p>GRAPPA induces context-free templates from annotated text-to SQL examples, such as \(\textsf{Show the COLUMN0 that have the OPO VALUE0 TABLE0, SELECT COLUMN0 FROM TABLE0 GROUPBY... OPO \in \{&gt;, &lt;, &gt;=\}}\), then uses such templates to genetate synthetic SQL query from table lake, such as \(\textsf{Show the studient_id have more than 6 class SELECT student_id...}\). This synthetic query is concatenated with the headers of correponding table, used to pretrain a RoBERTa with SQL semantic loss: predict whether a column appears in the SQL query and involves what operations. Additional, GRAPPA also pre-trains the model on joint text-table datasets with MLM loss.</p> <p><img src="/assets/img/trl/grappa.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <h6 id="tabert-pretraining-for-joint-understanding-of-textual-and-tabular-data-yin-acl-2020"><a href="https://arxiv.org/abs/2005.08314">TABERT: Pretraining for Joint Understanding of Textual and Tabular Data (Yin, ACL 2020)</a></h6> <p>TABERT creates a dataset including tables and surrounding text from Wikipedia pages, then jointly pre-train a LM on this mix of free-from text and structured tabular data. Due to limited input length, the table is cut off, only keep rows that are most relevant to the surrounding text (aka. <em>utterance</em>). Each table row is linearized (i.e. a cell is reprensented by \(Column\_Name \; \vert \; Column\_Type \; \vert \; Cell\_Value\)) and concatenated with the <em>utterance</em>, then is fed into BERT to yeild row-level embeddings of cell tokens and <em>utterance</em> tokens. Cell embeddings in the same column are aligned by vertical self-attention layers to allow for information flow across cell representations of different rows. The column representation is the average of embeddings of belonging cells.</p> <p><img src="/assets/img/trl/tabert.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>TABERT employs two unsupervised learning objectives:</p> <ul> <li>Masked Column Prediction: mask and predict column header and data type of the belonging cells.</li> <li>Cell Value Recovery: mask and predict the cell text.</li> </ul> <h6 id="tapas-weakly-supervised-table-parsing-via-pre-training-herzig-acl-2020"><a href="https://aclanthology.org/2020.acl-main.398/">TAPAS: Weakly Supervised Table Parsing via Pre-training (Herzig, ACL 2020)</a></h6> <p>TAPAS linearizes the concatenation of the utterance (e.g. query/question) and the table, then feeds it into a BERT model, performing unsupervised pre-training with typical masked LM objective. Each token embedding in the input is appended with additional special embedding: position emb, segment emb (whether token is within the utterance or table), column/row emb, rank emb (whether the cell text is string or float, if float, this emb encodes its order).</p> <p><img src="/assets/img/trl/tapas_2.png" alt="" style="width: 70%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <h6 id="turl-table-understanding-through-representation-learning-deng-vldb-2020"><a href="https://arxiv.org/abs/2006.14806">TURL: Table Understanding through Representation Learning (Deng, VLDB 2020)</a></h6> <p>TURL continues to pre-train a Tiny-BERT on the linearization of [table caption, table topic, table headers, table cells]. Tabel cells are represented by a fusion of token embeddings of cell text and the learnable embedding of the associated entity. Positional embeddings are added to tokens in the caption and the header. Additional type embeddings are used to indicate whether a token is within the caption, the header or is a subject cell or object cell. The attention meachnism is modified in the way that not all tokens in the input are visible to each other (e.g. two tokens residing in different rows and different columns can not see each other). This design models the row-column interaction in the table.</p> <p><img src="/assets/img/trl/turl_1.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p><img src="/assets/img/trl/turl_2.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/></p> <p>(source: copied from the paper).</p> <p>Two unsupervised learning objectives in TURL:</p> <ul> <li>MLM: mask and predict a token in caption or header.</li> <li>Masked Entity Recovery: mask a cell and predict the entity representing it (i.e. entity classification).</li> </ul>]]></content><author><name></name></author><category term="NLP,"/><category term="Table_Representation_Learning,"/><category term="AI"/><category term="research"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Experimental benchmarks on the GPU requirements for the training/fine-tuning of LLMs.</title><link href="https://huynhvp.github.io/blog/2023/gpu/" rel="alternate" type="text/html" title="Experimental benchmarks on the GPU requirements for the training/fine-tuning of LLMs."/><published>2023-06-12T10:09:00+00:00</published><updated>2023-06-12T10:09:00+00:00</updated><id>https://huynhvp.github.io/blog/2023/gpu</id><content type="html" xml:base="https://huynhvp.github.io/blog/2023/gpu/"><![CDATA[<hr/> <p>This post gathers experimental setups on the GPU usage for the training/fine-tuning of LLMs in the wild.</p> <hr/> <p><b>Table of Contents</b></p> <ul id="markdown-toc"> <li><a href="#fine-tuning-with-deepspeed" id="markdown-toc-fine-tuning-with-deepspeed"><b>Fine-tuning with DeepSpeed</b></a></li> <li><a href="#fine-tuning-with-fsdp" id="markdown-toc-fine-tuning-with-fsdp"><b>Fine-tuning with FSDP</b></a></li> <li><a href="#fine-tuning-with-lora" id="markdown-toc-fine-tuning-with-lora"><b>Fine-tuning with LoRA</b></a></li> </ul> <h6 id="fine-tuning-with-deepspeed"><b>Fine-tuning with DeepSpeed</b></h6> <ul> <li> <p>My own experience:</p> <table> <thead> <tr> <th>Model</th> <th>Context length</th> <th>DS Zero offload</th> <th>GPU</th> <th>precision</th> <th>batch_size_per_GPU</th> <th>gradient_accumulation</th> </tr> </thead> <tbody> <tr> <td>CodeT5+ 2B</td> <td>1024</td> <td>Yes (50G CPU)</td> <td>2x A100 40G</td> <td>bf16</td> <td>1</td> <td>4</td> </tr> <tr> <td>FLANT5-XL 3B</td> <td>1024</td> <td>Yes (50G CPU)</td> <td>2x RTX 4090 24G</td> <td>bf16</td> <td>1</td> <td>4</td> </tr> </tbody> </table> </li> </ul> <p><br/></p> <ul> <li> <p><b>FLAN-XL (3B) + FLAN-XL (11B)</b></p> <p><img src="/assets/img/gpu/ft_deepspeed_flan.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/> <em>(Source: https://www.philschmid.de/fine-tune-flan-t5-deepspeed)</em></p> </li> </ul> <h6 id="fine-tuning-with-fsdp"><b>Fine-tuning with FSDP</b></h6> <ul> <li> <p>My own experience:</p> <table> <thead> <tr> <th>Model</th> <th>Context length</th> <th>Full_shard</th> <th>GPU</th> <th>precision</th> <th>batch_size_per_GPU</th> <th>gradient_accumulation</th> </tr> </thead> <tbody> <tr> <td>Llama-Alpaca 6B</td> <td>512</td> <td>Yes</td> <td>6x A100 40G</td> <td>bf16</td> <td>1</td> <td>4</td> </tr> </tbody> </table> </li> <li> <p><b>GPT-2 XL (1.5B)</b></p> <p>2x24GB NVIDIA RTX</p> <p><img src="/assets/img/gpu/fsdp_gpt.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/> <em>(Source: https://huggingface.co/blog/pytorch-fsdp)</em></p> </li> </ul> <h6 id="fine-tuning-with-lora"><b>Fine-tuning with LoRA</b></h6> <ul> <li> <p>My own experience:</p> <table> <thead> <tr> <th>Model</th> <th>Context length</th> <th>GPU</th> <th>precision</th> <th>batch_size_per_GPU</th> <th>gradient_accumulation</th> </tr> </thead> <tbody> <tr> <td>CodeT5+ 2B</td> <td>1024</td> <td>1x RTX 4090 24G</td> <td>fp16</td> <td>1</td> <td>8</td> </tr> </tbody> </table> </li> </ul> <p><br/></p> <ul> <li> <p><b>TO (3B) + mTO (12B) + BLOOMZ (7B) (7B)</b></p> <p><img src="/assets/img/gpu/a100.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/> <em>(Source: https://github.com/huggingface/peft</em></p> </li> </ul>]]></content><author><name></name></author><category term="language_model,"/><category term="gpu"/><category term="dev"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">PoTM - Emergent World Representations - Exploring a Sequence Model Trained on a Synthetic Task</title><link href="https://huynhvp.github.io/blog/2023/representation-probe/" rel="alternate" type="text/html" title="PoTM - Emergent World Representations - Exploring a Sequence Model Trained on a Synthetic Task"/><published>2023-05-01T10:09:00+00:00</published><updated>2023-05-01T10:09:00+00:00</updated><id>https://huynhvp.github.io/blog/2023/representation-probe</id><content type="html" xml:base="https://huynhvp.github.io/blog/2023/representation-probe/"><![CDATA[<hr/> <p>I recently came across an interesting paper that got accepted at ICLR 2023: <a href="https://arxiv.org/abs/2210.13382">Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task (Li et al.)</a>. It provides valuable insights toward the understanding of black-box language model. Specifically, by training a LM to play a chess-like game, Othello, without feeding it any knowledge of the game rules, they discover that LM does learn meaningful latent representations that help it uncover the game and make legal disc moves on the board.</p> <hr/> <p><b>Table of Contents</b></p> <ul id="markdown-toc"> <li><a href="#the-othello-game-" id="markdown-toc-the-othello-game-"><b>The Othello game </b></a></li> <li><a href="#teach-lm-to-play-othello" id="markdown-toc-teach-lm-to-play-othello"><b>Teach LM to play Othello</b></a> <ul> <li><a href="#observation-1-lm-respects-the-games-rule" id="markdown-toc-observation-1-lm-respects-the-games-rule">Observation 1: LM respects the game’s rule.</a></li> <li><a href="#observation-2-hidden-representations-encoded-in-lms-layers-represent-the-boards-states" id="markdown-toc-observation-2-hidden-representations-encoded-in-lms-layers-represent-the-boards-states">Observation 2: Hidden representations encoded in LM’s layers represent the board’s states</a></li> <li><a href="#observation-3-the-relationship-between-models-internal-representation-or-board-state-thanks-to-observation-2-and-models-prediction-ie-next-legal-move-is-causal" id="markdown-toc-observation-3-the-relationship-between-models-internal-representation-or-board-state-thanks-to-observation-2-and-models-prediction-ie-next-legal-move-is-causal">Observation 3: The relationship between model’s internal representation (or board state, thanks to observation 2) and model’s prediction (i.e next legal move) is causal.</a></li> </ul> </li> </ul> <h3 id="the-othello-game-"><b>The Othello game </b></h3> <p><img src="/assets/img/probe/othello.gif" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/> <em>(Source: https://github.com/SiyanH/othello-game)</em></p> <p>Two players, one holding black discs, one holding white discs, take turns making a move (aka. placing their colored disc) on the 8x8 board. The goal is to cover the board with the majority of their color. When a player makes a move, any opponent’s disc found in between (horizontally, vertically or diagonally) the disc his just placed and any of existing discs of his color will be flipped over to become their own color. A legal move is a move that ensures at least one such flip happens. Otherwise, the game ends.</p> <p>A casual LM is trained to play Othello and its internal representation will be analyzed. Author chooses this game as it is simple enough while still has a sufficiently large solution space (aka. where to move given the board’s current state) to avoid memorization.</p> <h3 id="teach-lm-to-play-othello"><b>Teach LM to play Othello</b></h3> <p>The LM is fed a naive transcript recording interleaving moves of two players without adding the game rules or additional analysis of game state (e.g. [F5, C5, …] where F, C are vertical indices, and 5 is horizontal index of the board)). It has to figure out who play next and identify which tiles on the board are legal to move to.</p> <h4 id="observation-1-lm-respects-the-games-rule">Observation 1: LM respects the game’s rule.</h4> <p>The trained LM respects the game rule, it can predict the next legal move with very low error rate according to the current state of the board. (Note: a legal move is not necessary an optimized move as the model is not trained to win the game). This is not due to the memorization as the test set is ensured not to be seen during the training.</p> <h4 id="observation-2-hidden-representations-encoded-in-lms-layers-represent-the-boards-states">Observation 2: Hidden representations encoded in LM’s layers represent the board’s states</h4> <p>Board state involves whether each tile holds a black disc or a while disc or is empty. Author trains a non-linear MLP classifier \(p_{\theta}(x)\), taking in the internal activations \(x\) of a specific layer of LM, at a given game step, as features and yielding one of three labels: {black, white, empty}. In this way, they seek to investigate whether there is a mapping between LM’s internal representations and board’s states. Indeed, the results show that the classifier achieves high accuracy, implying such mapping exists.</p> <h4 id="observation-3-the-relationship-between-models-internal-representation-or-board-state-thanks-to-observation-2-and-models-prediction-ie-next-legal-move-is-causal">Observation 3: The relationship between model’s internal representation (or board state, thanks to observation 2) and model’s prediction (i.e next legal move) is causal.</h4> <p>In other words, changes in the network’s activations, leading to changes in board’s states according to observation 2 (e.g. a tile is switched from black to white), will causally cause the model to predict a move from a new set of possible legal moves in compliance with new board state.</p> <p>For example, in the figure below, from lower left board to lower right board, network’s activations has been intervened to switch the tile E6 from black to white. Consequently, the set of next possible legal moves has to be changed from {B4, C6, D3, E7, F4, F6} (upper left) to {B3,B4,C6,D3,F4} (upper right). The model predicts correctly this new set.</p> <p><img src="/assets/img/probe/board_state.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/> <em>(Source: copied from the paper)</em></p> <p>Question arises: how to modify the network’s activations \(x\) such that the tile E6 switches from black to white while others keep intact ? (we denote current board state as \(B\), new board state as \(B'\) and \(B'\) differs from \(B\) only at tile E6)</p> <ul> <li> <p>Pre-define a layer index \(l_s\), all activations from the layer \(l_s\) until the final layer, at the last game step, will be modified, as in figure below:</p> <p><img src="/assets/img/probe/interven.png" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/> <em>(Source: copied from the paper)</em></p> <p>Author argues that the intervention at only one layer \(l_s\) is not effective, as the change made at layer \(l_s\) will be diluted when it reaches the last layer, making the output being not affected by the change.</p> </li> <li> <p>The network’s activations \(x\) is updated in gradient descent manner such that new \(x'\) is mapped to \(B'\) via \(p_{\theta}(x')\) (see Observation 2). This resorts to:</p> </li> </ul> \[x' = x - \alpha \frac{\partial \mathcal{L}_{CE} (p_{\theta}(x), B')}{\partial x}\]]]></content><author><name></name></author><category term="paper_of_the_month,"/><category term="language_model,"/><category term="explainable_ai"/><category term="research"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">PoTM - Understanding Dataset Difficulty with V-Usable Information</title><link href="https://huynhvp.github.io/blog/2022/dataset-difficulty/" rel="alternate" type="text/html" title="PoTM - Understanding Dataset Difficulty with V-Usable Information"/><published>2022-12-06T10:09:00+00:00</published><updated>2022-12-06T10:09:00+00:00</updated><id>https://huynhvp.github.io/blog/2022/dataset-difficulty</id><content type="html" xml:base="https://huynhvp.github.io/blog/2022/dataset-difficulty/"><![CDATA[<hr/> <p>In this post, I would like to summarize an interesting paper that received the Outstanding Paper Award at ICML 2022: <a href="https://proceedings.mlr.press/v162/ethayarajh22a/ethayarajh22a.pdf">Understanding Dataset Difficulty with V-Usable Information (Ethayarajh et al.)</a>. The paper introduces a novel method for estimating the difficulty of a dataset w.r.t. a model using information theory. Specifically, it proposes \(\mathcal{V}\) - <em>usable information</em> and <em>pointwise</em> \(\mathcal{V}\) - <em>usable information</em> extended from Shannon’s mutual information to measure how much information contained in a dataset \((X, Y)\) (\(X\): input, \(Y\): label for example) or in an instance of \((X, Y)\) is <em>usable</em> by a model \(\mathcal{V}\). Lower value \(\mathcal{V}\) - <em>usable information</em> indicates that the dataset (or the instance) is more difficult for the model \(\mathcal{V}\).</p> <hr/> <p><b>Table of Contents</b></p> <ul id="markdown-toc"> <li><a href="#the-lack-of-interpretability-for-estimating-dataset-difficulty-of-related-works-" id="markdown-toc-the-lack-of-interpretability-for-estimating-dataset-difficulty-of-related-works-"><b>The lack of interpretability for estimating dataset difficulty of related works </b></a></li> <li><a href="#shannon-mutual-information-" id="markdown-toc-shannon-mutual-information-"><b>Shannon Mutual Information </b></a></li> <li><a href="#mathcalv-usable-information-or-mathcalv-information-" id="markdown-toc-mathcalv-usable-information-or-mathcalv-information-"><b>\(\mathcal{V}-usable\) information (or \(\mathcal{V}\) information) </b></a></li> <li><a href="#pointwise-mathcalv-information-" id="markdown-toc-pointwise-mathcalv-information-"><b>Pointwise \(\mathcal{V}\) information </b></a></li> <li><a href="#usage-of-mathcalv-information-" id="markdown-toc-usage-of-mathcalv-information-"><b>Usage of \(\mathcal{V}\) information </b></a></li> </ul> <h3 id="the-lack-of-interpretability-for-estimating-dataset-difficulty-of-related-works-"><b>The lack of interpretability for estimating dataset difficulty of related works </b></h3> <ul> <li> <p>Typical strategy of assessing whether a dataset is hard is to benchmark state-of-the-art models on this dataset and compare their performances to human. The bigger gap, the harder the data is considered to be. Since such evaluation is generally done at dataset-scale, it is limited in the capacity of understanding the different difficulty of individual sample in the dataset (which sample is harder than other). Furthermore, classic performance metrics, such as accuracy or F1 score for classification problem, are not suitable for standardized comparison across models and datasets. For example, considering 2 datasets \((X_1, Y_1)\) and \((X_2, Y_2)\) where \(X_1\) (resp. \(X_2\)) is independent of \(Y_1\) (resp. \(Y_2\)), we should expect that they have the same highest level difficulty (\(\mathcal{V}\) - <em>usable information</em> \(\approx\) zero), however, a model can obtain different accuracy on two datasets depending on the frequency of the majority class \(y\) in the dataset.</p> </li> <li> <p>Model-agnostic approaches to estimate the difficulty of a dataset are not able to explain why the dataset is easy for some models and hard for other models.</p> </li> <li> <p>Some approaches consider text-based heuristics such as word identity, input length or learning-based metrics such as training loss, prediction variance as proxies for dataset difficulty. However, they are not as readable as \(\mathcal{V}\) - <em>usable information</em>.</p> </li> </ul> <h3 id="shannon-mutual-information-"><b>Shannon Mutual Information </b></h3> <p>Shannon mutual information between two random variables measures the the amount of information obtained about one random variable (or the change in entropy of one random variable) by observing the other random variable (in the context of dataset difficulty, \(X\) is the input variable and \(Y\) is the label variable):</p> \[I(X, Y) = H(Y) - H(Y \mid X)\] <p>However, because this quantity is calculated with the assumption of infinite computation capacity, it is not suitable in practice as computational constraint is an important aspect to be considered. For example, considering three datasets \((X, Y)\), \((f(X), Y)\) and \((g(X), Y)\) where \(f\), \(g\) is an encrypting function and an useful preprocessing function applied on \(X\), respectively, we should expect that using \(f(X)\) to predict \(Y\): \(f(X) \rightarrow Y\) is harder and using \(g(X)\) to predict \(Y\): \(g(X) \rightarrow Y\) is easier than using \(X\) to predict \(Y\): \(X \rightarrow Y\) as after encrypting \(X\), the information contained in \(X\) becomes less accessible or after pre-processing \(X\), the information contained in \(X\) is exploited more easily. Despite that, the Shannon mutual information \(I(X, Y)\) would not change: \(I(X, Y) = I(f(X), Y) = I(g(x), Y)\) as it allows for unbounded computation, so one can employ arbitrarily complex strategy to decode \(f(X)\) and predict \(Y\) from \(X\).</p> <h3 id="mathcalv-usable-information-or-mathcalv-information-"><b>\(\mathcal{V}-usable\) information (or \(\mathcal{V}\) information) </b></h3> <p>Let \(\mathcal{X}, \mathcal{Y}\) be the sample space of two random variables \(X\), \(Y\) respectively and \(\Omega = \{f: \mathcal{X} \cup \varnothing \rightarrow \mathcal{P}(\mathcal{Y}) \}\) be any mapping function that predicts a distribution over \(\mathcal{Y}\) using the input \(\mathcal{X}\) or no side information \(\varnothing\). The computation-unbounded Shannon mutual information can be rewritten as: \(I(X, Y)=I_{\Omega}(X, Y)\).</p> <p>Under the computational or statistical constraints scenario, only a subset \(\mathcal{V} \subset \Omega\) is allowed to use to predict \(Y\), leading to the definition of \(\mathcal{V}\) information, extended from Shannon mutual information:</p> \[I_{\mathcal{V}}(X, Y) = H_{\mathcal{V}}(Y) - H_{\mathcal{V}}(Y \mid X)\] <p>where \(H_{\mathcal{V}}(Y) = \inf_{f \in \mathcal{V}} \mathbb{E}_{y \sim Y}[-\text{log} \; f[\varnothing](y)]\) and \(H_{\mathcal{V}}(Y \mid X) = \inf_{f \in \mathcal{V}} \mathbb{E}_{y \sim Y, x \sim X}[-\text{log} \; f[x](y)]\).</p> <p>Intuitively, the conditional \(\mathcal{V}-entropy\) \(H_{\mathcal{V}}(Y \mid X )\) (resp. \(\mathcal{V}-entropy\) \(H_{\mathcal{V}}(Y)\)) is smallest expected negative log-likelihood of predicted label \(Y\) given observations \(X\) (resp. no side information \(\varnothing\)).</p> <p>In practice, it is impossible to calculate the true \(\mathcal{V}-information\) as it requires the whole data distribution. Instead, it is empirically estimated on a finite dataset supposed to include samples that i.i.d drawn from the distribution, leading to the gap between the true \(\mathcal{V}-information\) and empirical \(\mathcal{V}-information\). However, if \(\mathcal{V}\) is less complex and the dataset is large, the gap becomes small.</p> <p>In the learning context, \(H_{\mathcal{V}}(Y \mid X )\) is estimated by training (or fine-tuning) a model \(f \in \mathcal{V}\) with cross-entropy loss to minimize the negative log-likelihood of \(Y\) given \(X\): \(\mathbb{E}_{y \sim Y_{train}, x \sim X_{train}}[-\text{log} \; f[x](y)]\), then using the trained model to calculate \(\mathbb{E}_{y \sim Y_{test}, x \sim X_{test}}[-\text{log} \; f[x](y)]\) on test dataset. Similarly, \(H_{\mathcal{V}}(Y)\) is estimated by fitting another model \(f \in \mathcal{V}\) on label distribution.</p> <h3 id="pointwise-mathcalv-information-"><b>Pointwise \(\mathcal{V}\) information </b></h3> <p>\(\mathcal{V}\) information is extended to individual instance \((x, y)\) of random variables \((X, Y)\) under pointwise \(\mathcal{V}\) information \(\textsf{PVI}(x, y)\).</p> \[\textsf{PVI}(x \rightarrow y) = - \text{log} \; g[\varnothing](y) + \text{log} \; g'[x](y)\] <p>where \(g \in \mathcal{V} \; s.t. \mathbb{E}[- \text{log} \; g[\varnothing](Y)] = H_{\mathcal{V}}(Y)\) and \(g' \in \mathcal{V} \; s.t. \mathbb{E}[- \text{log} \; g'[X](Y)] = H_{\mathcal{V}}(Y \mid X)\). Loosely speaking, \(g\) and \(g'\) are models (e.g. BERT) before and after fine-tuning with training samples of \((X, Y)\), and \(\textsf{PVI}(x, y)\) is the difference in log-probability that two models assign to the label \(y\). The higher the \(\textsf{PVI}\), the easier the instance is w.r.t. \(\mathcal{V}\).</p> <p>\(\textsf{PVI}(x, y)\) should only depend on the distribution of \(X\) and \(Y\). Fine-tuning models \(\in \mathcal{V}\) with different size of training set \(\{(x, y)_i\}_{i=1}^k\) should not change this quantity.</p> <p>The estimated \(\mathcal{V}\) information \(\hat{I}_{\mathcal{V}}(X, Y)\) is written as: \(\hat{I}_{\mathcal{V}}(X, Y) = \frac{\sum_i \text{PVI} (x_i, y_i)}{n}.\)</p> <h3 id="usage-of-mathcalv-information-"><b>Usage of \(\mathcal{V}\) information </b></h3> <ol> <li> <p><b>Compare different models \(\mathcal{V}\) for the same dataset \((X, Y)\) by computing \(I_{\mathcal{V}}(X \rightarrow Y)\)</b></p> <p>Following figure shows the test accuracy of 4 models {GPT2-small, BERT-base, BART-base, DistillBERT-base } on SNLI task. Model with higher \(\mathcal{V}-information\) exploits more information from the dataset, leading to better performance (BART-base). <img src="/assets/img/v_information/snli.PNG" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/> <em>(Source: copied from the paper)</em></p> <p>Furthermore, \(\mathcal{V}-information\) can be an early sign of overfitting. At epoch 5, the models start to be less certain about the true label \(\rightarrow\) \(\mathcal{V}-information\) starts to decrease but it can still make correct predictions (test accuracy is stable). Then, at epoch 10, \(\mathcal{V}-information\) reach its lowest value and diverges but it looks like test accuracy is just starting to decline.</p> </li> <li> <p><b>Compare the difficulty of different dataset \((X, Y)\)(s) for the same model \(\mathcal{V}\) by computing \(I_{\mathcal{V}}(X \rightarrow Y)\) </b></p> <p>The dotted lines in figure below show \(BERT-information\)(s) (\(\mathcal{V}\) = BERT) for 3 NLI datasets: CoLA, MultiNLI and SNLI. It is expected that CoLA is the most difficult dataset, then MultiNLI for NLI task addressed by BERT model. <img src="/assets/img/v_information/dataset_diff.PNG" alt="" style="width: 40%; display:block; margin-left:auto; margin-right:auto"/> <em>(Source: copied from the paper)</em></p> </li> <li> <p>\(\textsf{PVI}\) can help to spot mislabelled instances where such instances have negative \(\textsf{PVI}\).</p> </li> <li> <p>The \(\textsf{PVI}\) threshold at which predictions become incorrect is similar across datasets. The gold threshold is 0.5 which is useful for cross-dataset comparison.</p> </li> </ol>]]></content><author><name></name></author><category term="paper_of_the_month,"/><category term="information_theory,"/><category term="machine_learning"/><category term="research"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">What I’ve learned from finding ways to accelerate the inference of a Transformer model.</title><link href="https://huynhvp.github.io/blog/2022/optimization/" rel="alternate" type="text/html" title="What I’ve learned from finding ways to accelerate the inference of a Transformer model."/><published>2022-11-10T10:09:00+00:00</published><updated>2022-11-10T10:09:00+00:00</updated><id>https://huynhvp.github.io/blog/2022/optimization</id><content type="html" xml:base="https://huynhvp.github.io/blog/2022/optimization/"><![CDATA[<p><b>Table of Contents</b></p> <ul id="markdown-toc"> <li><a href="#introduction" id="markdown-toc-introduction"><b>Introduction</b></a></li> <li><a href="#a-solution" id="markdown-toc-a-solution"><b>A solution</b></a> <ul> <li><a href="#-onnx-open-neural-network-exchange-converter" id="markdown-toc--onnx-open-neural-network-exchange-converter"><img src="/assets/img/optimization/question_1.png" alt="" style="width: 3.7%"/> <b>ONNX (Open Neural Network eXchange) Converter</b></a></li> <li><a href="#-adapter-onnx-runtime" id="markdown-toc--adapter-onnx-runtime"><img src="/assets/img/optimization/question_2.png" alt="" style="width: 3.5%"/> <b>Adapter: ONNX Runtime</b></a></li> <li><a href="#inference-strategies-based-on-onnx-model-format" id="markdown-toc-inference-strategies-based-on-onnx-model-format"><b>Inference strategies based on ONNX model format</b></a></li> <li><a href="#huggingfaces-optimum-" id="markdown-toc-huggingfaces-optimum-"><b>Huggingface’s Optimum</b> <img src="/assets/img/optimization/optimum.png" alt="" style="width: 3.5%"/></a></li> </ul> </li> <li><a href="#conclusion" id="markdown-toc-conclusion"><b>Conclusion</b></a></li> </ul> <h3 id="introduction"><b>Introduction</b></h3> <p>When it comes to deploying a machine learning/deep learning model in production environments, there are many factors that need to be worked out. In this post, I would like to outline 2 aspects:</p> <ul> <li> <p><b>(1) The compatibility between the development environment you use to train your model and the production environment.</b></p> <p>As nowadays’s AI ecosystem is considerably fragmented from software-level to hardware-level. A lot of ML/DL training frameworks are at hand for us to build our model such as PyTorch, Tensorflow or scikit-learn. We can get our jobs done on Windows or Linux with Intel GPUs and Python runtime, but later on, we want to deploy the product on cloud or edge devices with another runtime (e.g. C++) and another sort of GPUs (e.g. NVIDIA). Facing this cross-platform deployment challenge, a classic strategy is to build a specific model for a specific platform, as illustrated in the figure below. This means that developer is required to pick developing tools corresponding to targeted environments which may not be the ones they love to use. Also, they need to ensure that there is no accuracy gap between different model versions.</p> <p><img src="/assets/img/optimization/deployment_1.png" alt="" style="width: 100%; display:block; margin-left:auto; margin-right:auto"/></p> </li> <li> <p><b>(2) Optimizing model inference for an efficient user experience.</b></p> <p>In production, together with accuracy, scalability and high performance become crucial concerns. To deal with that, there could be two strategies:</p> <ul> <li><b>Using smaller models or distilled models that still yield the accuracy you need:</b> DL models, especially, Transformer-based LM models, become more and more powerful, at the cost of number of model parameters and environmental responsibility (carbon footprint). However, larger model does not necessarily mean better. The data size, the training/fine-tuning recipe also account for the performance of your model (<a href="https://www.deeplearning.ai/the-batch/finding-the-best-data-to-parameter-ratio-for-nlp-models/">https://www.deeplearning.ai/the-batch/finding-the-best-data-to-parameter-ratio-for-nlp-models/</a>).</li> </ul> <p><img src="/assets/img/optimization/model_size.jpg" alt="" style="width: 100%; display:block; margin-left:auto; margin-right:auto"/> <em>(Source: <a href="https://huggingface.co/blog/large-language-models">https://huggingface.co/blog/large-language-models</a>)</em></p> <ul> <li><b>Optimizing your model:</b> basically, the optimization involves the improvement of running time (latency) and memory throughput and it is done not only at software level (algorithm, training framework, OS, programming language) but also at hardware level (GPU, hardware accelerator). As the AI ecosystem is fragmented, we can have many possible combinations of {OS, framework, runtime, hardware} with different pros/cons for developing a model. This makes the optimization a challenging task.</li> </ul> </li> </ul> <h3 id="a-solution"><b>A solution</b></h3> <p>Up to this point, you might envision how tough the path from conception to production in ML is. To address two deployment issues mentioned above, a research direction has been identified for AI ecosystem in which the development-production workflow becomes modularizable and the frameworks become interoperable. The idea is to standardize the bridge between development environment and production environment, alternatively stated, the bridge between software (OS, framework) and hardware (CPU, GPU). By this way, different frameworks can be combined with different hardwares without modification. Let’s dive a bit deeper into this standardization process, as demonstrated in the figure below:</p> <p><img src="/assets/img/optimization/deployment_2.png" alt="" style="width: 100%; display:block; margin-left:auto; margin-right:auto"/></p> <p>It requires two plugins: (i) a <b>converter</b> to transform the model you are developing in your environment into an universal one that can be loaded, optimized and executed by (ii) an <b>adapter</b> installed on different target platforms.</p> <h5 id="-onnx-open-neural-network-exchange-converter"><img src="/assets/img/optimization/question_1.png" alt="" style="width: 3.7%"/> <b>ONNX (Open Neural Network eXchange) Converter</b></h5> <p>According to <a href="https://onnx.ai/">https://onnx.ai/</a>, <a href="https://onnx.ai/">ONNX</a> (Open Neural Network eXchange) is an open serialized format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers. It is backed by many leading AI companies:</p> <p><img src="/assets/img/optimization/onnx.png" alt="" style="width: 60%; display:block; margin-left:auto; margin-right:auto"/><em>(Source: <a href="https://onnx.ai/">https://onnx.ai/</a>)</em></p> <h5 id="-adapter-onnx-runtime"><img src="/assets/img/optimization/question_2.png" alt="" style="width: 3.5%"/> <b>Adapter: ONNX Runtime</b></h5> <p><a href="https://onnxruntime.ai/">ONNX Runtime</a> is a performant inference engine that can read, optimize the ONNX model format and leverage hardware accelerators to perform inference from ONNX format. It is compatible with various technology stack (frameworks, operating systems and hardware platforms).</p> <p><img src="/assets/img/optimization/onnx_runtime.png" alt="" style="width: 50%; display:block; margin-left:auto; margin-right:auto"/><em>(Source: <a href="https://onnxruntime.ai/">https://onnxruntime.ai</a>)</em></p> <p>For example, assuming your production environment supports <code class="language-plaintext highlighter-rouge">Linux OS x64</code>, <code class="language-plaintext highlighter-rouge">Python runtime</code> and <code class="language-plaintext highlighter-rouge">NVIDIA GPU</code> as in figure below, then you can install ONNX Runtime via <code class="language-plaintext highlighter-rouge">pip install onnxruntime-gpu</code> to be ready for deploying any model created from any technology stack on your dev machine as long as it can be exported to ONNX format.</p> <p><img src="/assets/img/optimization/onnx_runtime_choice.png" alt="" style="width: 80%; display:block; margin-left:auto; margin-right:auto"/></p> <p>With <b>ONNX</b> and <b>ONNX Runtime</b>, we are able to address two of the issues of putting a ML model in production (compatibility/interoperability and inference performance) introduced earlier in the post. For ML developers, they are liberated from the constraint of framework compatibility and have more freedom to choose their preferred tech stacks. For hardware manufacturers, they can rely on a standard and universal software specification (e.g. ONNX model format) to ease their process of hardware optimization for AI.</p> <p>Now, let’s go a bit more details into possible inference strategies with ONNX format.</p> <h5 id="inference-strategies-based-on-onnx-model-format"><b>Inference strategies based on ONNX model format</b></h5> <p><img src="/assets/img/optimization/onnx_strategies.png" alt="" style="width: 100%; display:block; margin-left:auto; margin-right:auto"/></p> <ul> <li> <p><strong><span style="color:red">(1)</span></strong> After your model is converted into ONNX format, it can be used immediately on targeted device using ONNX Runtime.</p> </li> <li> <p><strong><span style="color:#1589FF">(2)</span></strong> After your model is converted into ONNX format, ONNX Runtime can further optimize it before performing inference on targeted device. The optimization techniques can be: graph optimization (node pruning, node fusion…) and quantization (e.g. convert FP32 to INT8).</p> </li> <li> <p><strong><span style="color:green">(3)</span></strong> It is also possible to convert ONNX model to another format that is optimized for a specific hardware. For example, in the figure above, as <strong><span style="color:orange">Cloud</span></strong> is equipped with NVIDIA GPU, we can convert ONNX model to <a href="https://github.com/NVIDIA/TensorRT">TensorRT</a> model which is developed dedicatedly for NVIDIA GPUs. ONNX Runtime is no longer in use in this case of course.</p> </li> </ul> <h5 id="huggingfaces-optimum-"><b>Huggingface’s Optimum</b> <img src="/assets/img/optimization/optimum.png" alt="" style="width: 3.5%"/></h5> <p>If you are working with Transformers and you want to easily accelerate your model performance leveraging your existing hardware powers, you may think of <a href="https://huggingface.co/docs/optimum/index">Optimum</a>. It gathers a set of optimization toolkits (e.g. ONNX Runtime presented above, Intel Neural Compressor, OpenVINO) each of which is designed specifically for each specific hardware. It also provides a high-level API to facilitate the utilization and the optimization in a few lines of code. Typically, an optimization pipeline with Optimum consists of 3 steps:</p> <ul> <li> <p>Convert the trained model into the standard format such as ONNX or OpenVINO.</p> </li> <li> <p>Optimize the converted model (e.g. graph optimization, quantization) using available Runtimes such as ONNX Runtime or OpenVINO Runtime.</p> </li> <li> <p>Run the inference on optimized model.</p> </li> </ul> <p>Now, let’s get our hand a bit dirty by trying to accelerate a Transformer model with ONNX and ONNX Runtime using Optimum. In detail, we optimize the inference of a BERT-based model for text classification task (<a href="https://huggingface.co/tasks/text-classification#sentiment-analysis">Sentiment Analysis</a>).</p> <p><b>Example:</b></p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Input: I am learning how to accelerate the inference of a language model using Huggingface's Optimum. I am thinking if it can bring some performance gain. I hope it does."

Output: [{'label': 'POSITIVE', 'score': 0.7894014716148376}]
</span></code></pre></div></div> <p><b>My machine is equiped with 6GB GPU NVIDIA Quadro RTX 3000</b>. Hence, I’m going to leverage 2 hardware accelerators for NVIDIA via <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu">ONNX Runtime</a> : generic accelerator (<code class="language-plaintext highlighter-rouge">CUDAExecutionProvider</code>) and <a href="https://developer.nvidia.com/tensorrt">TensorRT inference engine</a> (<code class="language-plaintext highlighter-rouge">TensorrtExecutionProvider</code>)</p> <p><b>For comparison</b>, there are two baselines where the inference is done with native pytorch on either CPU (<code class="language-plaintext highlighter-rouge">cpu_torch_model</code>) or GPU (<code class="language-plaintext highlighter-rouge">gpu_torch_model</code>).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">from</span> <span class="n">optimum.onnxruntime</span> <span class="kn">import</span> <span class="n">ORTModelForSequenceClassification</span><span class="p">,</span> <span class="n">ORTOptimizer</span>
<span class="kn">from</span> <span class="n">optimum.onnxruntime.configuration</span> <span class="kn">import</span> <span class="n">OptimizationConfig</span>
<span class="kn">from</span> <span class="n">time</span> <span class="kn">import</span> <span class="n">perf_counter</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="sh">"</span><span class="s">distilbert-base-uncased-finetuned-sst-2-english</span><span class="sh">"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="c1"># baseline: native pytorch CPU
</span><span class="n">cpu_torch_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">cpu_torch_pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">sentiment-analysis</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">cpu_torch_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># baseline: native pytorch GPU
</span><span class="n">gpu_torch_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda:0</span><span class="sh">'</span><span class="p">))</span>
<span class="n">gpu_torch_pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">sentiment-analysis</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">gpu_torch_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <p>To optimize the native pytorch model using ONNX Runtime, we first convert it into ONNX format <code class="language-plaintext highlighter-rouge">onnx_model</code> and apply graph optimization on converted model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># convert native pytorch model to ONNX format and graph-optimize it.
</span><span class="n">onnx_model</span> <span class="o">=</span> <span class="n">ORTModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">from_transformers</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">ORTOptimizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)</span>
<span class="n">optimization_config</span> <span class="o">=</span> <span class="nc">OptimizationConfig</span><span class="p">(</span>
    <span class="n">optimization_level</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">optimize_with_onnxruntime_only</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">optimize_for_gpu</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">optimizer</span><span class="p">.</span><span class="nf">optimize</span><span class="p">(</span><span class="n">save_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">./optimized</span><span class="sh">"</span><span class="p">,</span> <span class="n">optimization_config</span><span class="o">=</span><span class="n">optimization_config</span><span class="p">)</span>
</code></pre></div></div> <p>Next, we define 3 types of inferences for optimized ONNX model:</p> <ul> <li>Inference on CPU:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># optimized ONNX CPU
</span><span class="n">optimized_cpu_onnx_model</span> <span class="o">=</span> <span class="n">ORTModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">./optimized/</span><span class="sh">"</span><span class="p">)</span>
<span class="n">optimized_cpu_onnx_pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">sentiment-analysis</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">optimized_cpu_onnx_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Inference on GPU with NVIDIA generic accelerators (CUDA):</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># optimized ONNX CUDA
</span><span class="n">optimized_cuda_onnx_model</span> <span class="o">=</span> <span class="n">ORTModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">./optimized/</span><span class="sh">"</span><span class="p">,</span> <span class="n">provider</span><span class="o">=</span><span class="sh">"</span><span class="s">CUDAExecutionProvider</span><span class="sh">"</span><span class="p">)</span>
<span class="n">optimized_cuda_onnx_pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">sentiment-analysis</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">optimized_cuda_onnx_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Inference on GPU with NVIDIA TensorRT engine:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># optimized ONNX Tensorrt
## ONNX Runtime graph optimization need to be disabled for the model to be consumed and optimized by TensorRT
</span><span class="n">optimized_tensorrt_onnx_model</span> <span class="o">=</span> <span class="n">ORTModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">from_transformers</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">session_options</span><span class="o">=</span><span class="n">session_options</span><span class="p">,</span> <span class="n">provider</span><span class="o">=</span><span class="sh">"</span><span class="s">TensorrtExecutionProvider</span><span class="sh">"</span><span class="p">)</span>
<span class="n">optimized_tensorrt_onnx_pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">sentiment-analysis</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">optimized_tensorrt_onnx_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div></div> <p>We measure the latency of inference settings described above with a simple benchmark as following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">intput</span> <span class="o">=</span> <span class="sh">"</span><span class="s">I am learning how to accelerate the inference of a language model using Huggingface</span><span class="sh">'</span><span class="s">s Optimum.  </span><span class="se">\
</span><span class="s">             I am wondering if it can bring some performance gain. I hope it does.</span><span class="sh">"</span>

<span class="c1"># ref: https://www.philschmid.de/optimizing-transformers-with-optimum
</span><span class="k">def</span> <span class="nf">measure_latency</span><span class="p">(</span><span class="n">pipe</span><span class="p">):</span>
    <span class="n">latencies</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># warm up
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
        <span class="n">_</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">intput</span><span class="p">)</span>
    <span class="c1"># Timed run
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="nf">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span>  <span class="nf">pipe</span><span class="p">(</span><span class="n">intput</span><span class="p">)</span>
        <span class="n">latency</span> <span class="o">=</span> <span class="nf">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="n">latencies</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">latency</span><span class="p">)</span>
    <span class="c1"># Compute averaged execution time
</span>    <span class="n">avg_time_ms</span> <span class="o">=</span> <span class="mi">1000</span> <span class="o">*</span> <span class="nf">sum</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span>
    <span class="k">return</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Average latency (ms) : </span><span class="si">{</span><span class="n">avg_time_ms</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span>
</code></pre></div></div> <p><b>The results</b> are shown below:</p> <p><img src="/assets/img/optimization/latency.png" alt="" style="width: 70%; display:block; margin-left:auto; margin-right:auto"/></p> <p>Clearly, with ONNX format and ONNX Runtime, the performance has been significantly boosted even on CPU mode:</p> <ul> <li> <p>ONNX Runtime runs inference on CPU accelerates 1.56x w.r.t native pyTorch running on CPU.</p> </li> <li> <p>ONNX Runtime leverages better the GPU hardware than native pyTorch by 2.4x faster on NVIDIA GPU.</p> </li> </ul> <h3 id="conclusion"><b>Conclusion</b></h3> <p>This post walked you through the introduction of two challenges for deploying an AI model (Transformer model in particular) in production: <b>(i)</b> the interoperability between different frameworks and <b>(ii)</b> the performance issue. These two challenges can be addressed by standardizing model formats and employing a powerful cross-platform inference engine for running the inference on this format, such as {ONNX format, ONNX Runtime} or {OpenVINO, OpenVINO Runtime}.</p> <p>If we relax the interoperability aspect and focus on boosting the model performance, then, apart from the method based on converting the original model into standard format as described in this post, <a href="https://github.com/ELS-RD/kernl">kernl</a>, recently released, intervenes directly in GPU kernels that allows to accelerate and optimize your model right on Pytorch with a single line of code, without converting to any standard format. Its benchmark (below) shows impressive improvements over other optimization techniques. Furthermore, they pay more attention to generative models (e.g. Seq2Seq model) which prove to be more difficult to optimize. However, if you don’t have the new generation of NVIDIA GPU (Ampere), you are not able to use this tool yet. (<a href="https://github.com/ELS-RD/kernl/issues/133">follow up here</a>)</p> <p><img src="/assets/img/optimization/kernl.png" alt="" style="width: 90%; display:block; margin-left:auto; margin-right:auto"/> <em>(Source: <a href="https://github.com/ELS-RD/kernl">https://github.com/ELS-RD/kernl</a>)</em></p> <p><b>References</b>:</p> <ul> <li>https://www.philschmid.de/optimizing-transformers-with-optimum</li> <li>https://odsc.medium.com/interoperable-ai-high-performance-inferencing-of-ml-and-dnn-models-using-open-source-tools-6218f5709071</li> <li>https://medium.com/geekculture/onnx-in-a-nutshell-4b584cbae7f5</li> <li>https://huggingface.co/blog/convert-transformers-to-onnx</li> </ul>]]></content><author><name></name></author><category term="Optimization,"/><category term="ONNX,"/><category term="ONNX_Runtime,"/><category term="Huggingface_Optimum,"/><category term="Transformer."/><category term="dev"/><summary type="html"><![CDATA[Table of Contents]]></summary></entry></feed>